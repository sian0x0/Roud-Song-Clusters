{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English-language folk songs have a long tradition and have changed over time. Songs are not easily idenifiable by name alone, and lyrics often have variations. Steve Roud began indexing his own collection in the 1970s, and his Roud Index has become the standard for grouping together different versions of the same song. He is still indexing as of 2023.\n",
    "\n",
    "Could a machine learning algorithm hope to match his skill? Given the lyrics, would it choose the same groupings of songs, where the line between \"same\" and \"different\" is fuzzy? Could it help with future indexing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Roud index is a lyrics-based classification system (rather than tune-based), the officially-hosted index at vwml.com does not contain lyric transcriptions as a standard data field. Some lyrics are accessible online, presented in scanned images of historical collections, others on linked external sites, others not at all. \n",
    "\n",
    "So the first challenge is to get a dataset with enough full lyrics and Roud numbers in combination. The main contenders for the source of this data are Mudcat and The Traditional Ballad Index, both well-established online song databases.\n",
    "\n",
    "### Mudcat \n",
    "- Project focuses on song lyrics and tunes, but also contains Roud numbers for approximately 300 songs.\n",
    "- Data formats:\n",
    "    - Digitrad (DT) database MS-DOS download (last updated in 2002)\n",
    "    - Song web pages\n",
    "    - Forum posts containing songs\n",
    "\n",
    "### The Ballad Index \n",
    "- Project focuses on cataloguing*, but also has supplementary lyrics for approximately 1110 songs.\n",
    "- Data formats:\n",
    "    - The Ballad Index Software Filemaker database download\n",
    "    - Song web pages (without lyrics)\n",
    "    - The Ballad Index (BI) and The Supplemental Tradition (ST) (lyrics) as HTML or TXT lists\n",
    "\n",
    "&ast; This is a similar to approach to Roud, but focused os on the basic unit of a song rather than its individual instances (eg songbook entries or performances), and therefore uses song titles as its main identifiers, with keywords and first line for disambiguation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the Mudcat DT or the Ballad Index (including supplementaty Tradition) downloadable databases will open. \n",
    "\n",
    "I therefore need to work with the `.txt` versions of the Ballad Index and Supplementary Tradition and join them in order to link Roud numbers to lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets (BI, ST, DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on text editor finds I estimate I can extract approximately the following data:\n",
    "- BI: 30445 song record files, of which (in combination):\n",
    "    - 14213 are stubs for variants that only refer to other songs\n",
    "    - 2623 refer to DT files (lyrics)\n",
    "    - 1180 refer to ST files (lyrics)\n",
    "    - 12126 of these contain Roud index numbers\n",
    "- ST: 1229 lyrics referencing 1136 BI files\n",
    "    - Note: 404 of the ST filenames seem to be modified DT filenames, eg 'DTwarovr' in ST and BI is the same as 'WAROVR' in DT\n",
    "- DT: 8932 song record files (lyrics)\n",
    "    - Note: 793 records also contain a 'DT #' but I don't yet know what this is. Contrary to my assumption it does not correspond to the SongID in URLs the Mudcat website, which are formatted like this example http://mudcat.org/@displaysong.cfm?SongID=329\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BI (Ballad Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preview of `balldidx.txt`. The text version of the Ballad Index file is tricky to work with as entries are presented as a list with inconsistent columns and mixed data. \n",
    "\n",
    "I first used a text editor to place colons before Roud numbers and DT filenames, so that they could be more easily matched. (This could have been perhaps better achieved with regex, although to begin I decided to save myself a step as they were formatted inconsistently.)\n",
    "\n",
    "Here it is interesting to note that the BI database also references Mudcat's DT filenames, for example `DT, MASS1913*` above. This means we can also supplement lyrics by cross-referencing this data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "===\n",
    "VERSION 6.5, February 26, 2023\n",
    "===\n",
    "NAME: 10,000 Years Ago: see I Was Born About Ten Thousand Years Ago (Bragging Song) (File: R410)\n",
    "===\n",
    "NAME: 10th MTB Flotilla Song: see Fred Karno's Army (File: NeFrKaAr)\n",
    "===\n",
    "NAME: 13 Highway\n",
    "DESCRIPTION: \"I went down 13 highway, Down in my baby's door Raining and storming, Scarcely see the road.\" \"Clouds dark as night, If my baby don't fail me I'll make every thing all right\" \"Going 60 miles an hour...\" \"Don't the highway look lonesome...\"\n",
    "AUTHOR: unknown\n",
    "EARLIEST_DATE: 1938 (recording, Walter Davis)\n",
    "KEYWORDS: grief love promise nonballad lover technology\n",
    "FOUND_IN: US(SE)\n",
    "REFERENCES: (0 citations)\n",
    "Roud #29487\n",
    "RECORDINGS:\n",
    "Walter Davis, \"13 Highway\" (Bluebird B7693, 1938)\n",
    "Moses Williams, \"13 Highway\" (on USFlorida01)\n",
    "NOTES: Moses Williams sings \"I always wonder why ... That woman didn't treat me right.\" The description follows the Walter Davis recording. - BS\n",
    "Last updated in version 5.0\n",
    "File: Rc13Hwy\n",
    "===\n",
    "NAME: 151 Days: see Hundred and Fifty-One Days (File: Colq060)\n",
    "===\n",
    "NAME: 1861 Anti Confederation Song, An: see Anti-Confederation Song (File: FJ028)\n",
    "===\n",
    "NAME: 1913 Massacre\n",
    "DESCRIPTION: In Calumet, Michigan, striking copper miners and their children are having a Christmas celebration; strike-breakers outside bar the doors then raise a false fire alarm. In the ensuing stampede, seventy-three children are crushed or suffocated\n",
    "AUTHOR: Woody Guthrie\n",
    "EARLIEST_DATE: 1945 (recording by author)\n",
    "KEYWORDS: lie strike death labor-movement mining disaster children\n",
    "FOUND_IN: US\n",
    "REFERENCES: (3 citations)\n",
    "Greenway-AmericanFolksongsOfProtest, pp. 157-158, \"1913 Massacre\"\n",
    "Silber/Silber-FolksingersWordbook, p. 306, \"The 1913 Massacre\" (1 text)\n",
    "DT, MASS1913*\n",
    "Roud #17663\n",
    "RECORDINGS:\n",
    "Woody Guthrie, \"1913 Massacre\" (Asch 360, 1945; on Struggle1, Struggle2)\n",
    "CROSS_REFERENCES:\n",
    "cf. \"One Morning in May (To Hear the Nightingale Sing)\" (tune)\n",
    "NOTES: In the late 19th/early 20th century, the rapid expansion of the electrical industry created great demand for copper, for which the chief source was the mines in the upper peninsula of Michigan. Bitter strikes resulted as the miners, under the leadership of the Western Federation of Miners, demanded decent pay and safer working conditions.\n",
    "Guthrie's description of the events of 1913 is dead-on accurate, according to the residents of Calumet; Italian Hall, where the disaster occurred, was still standing in the early 1980s, but has since been torn down. - PJS\n",
    "There is an historical marker on the site (Italian Hall, 7th Street, Calumet, MI, at its junction with Elm Street, one block south of Highway 203), and the site has not been built over. One of the plaques has a picture of Woody and mentions this song. There are quite a few recent photos of the site on Google Maps. - RBW\n",
    "Last updated in version 6.1\n",
    "File: FSWB306A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a script with regular expressions to import while doing the following:\n",
    "- split song records at the marker '==='\n",
    "- extract only the values for 'name', 'description', 'earliest_date', found_in', 'keywords', 'cross_references', 'roud', 'bi_file', and 'dt_file'\n",
    "- split and store reference song name and filename information in one-line stub records that only serve to reference a main song\n",
    "- make stubs inherit Roud number and file references from their parent entries\n",
    "- extract only the earliest year found in the 'EARLIEST_FOUND:' field which contained mixed data\n",
    "- replace empty fields with NumPy `NaN` to allow for better data manipulation\n",
    "\n",
    "These are stored in `df_bi`.\n",
    "\n",
    "Target: 30445 file records |\n",
    "Output: 30418 file records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>earliest_date</th>\n",
       "      <th>found_in</th>\n",
       "      <th>keywords</th>\n",
       "      <th>cross_references</th>\n",
       "      <th>roud</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10,000 Years Ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>see I Was Born About Ten Thousand Years Ago (B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10th MTB Flotilla Song</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>see Fred Karno's Army (File: NeFrKaAr)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NeFrKaAr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13 Highway</td>\n",
       "      <td>\"I went down 13 highway, Down in my baby's doo...</td>\n",
       "      <td>1938</td>\n",
       "      <td>US(SE)</td>\n",
       "      <td>grief love promise nonballad lover technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29487</td>\n",
       "      <td>Rc13Hwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151 Days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>see Hundred and Fifty-One Days (File: Colq060)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Colq060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1861 Anti Confederation Song, An</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>see Anti-Confederation Song (File: FJ028)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FJ028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30413</th>\n",
       "      <td>Zula</td>\n",
       "      <td>\"Thou lov'st another, Zula, Thou lovest him al...</td>\n",
       "      <td>1952</td>\n",
       "      <td>US(So)</td>\n",
       "      <td>love rejection separation travel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11330</td>\n",
       "      <td>Brne049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30414</th>\n",
       "      <td>Zulu Warrior, The</td>\n",
       "      <td>\"I-kama zimba zimba zayo I-kama zimba zimba ze...</td>\n",
       "      <td>1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonballad nonsense campsong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACFF061A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30415</th>\n",
       "      <td>Zum Gali Gali</td>\n",
       "      <td>Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...</td>\n",
       "      <td>1956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>foreignlanguage campsong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACSF314Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30416</th>\n",
       "      <td>Zutula Dead</td>\n",
       "      <td>A nice girl gave Zutula bitter casava to eat a...</td>\n",
       "      <td>1939</td>\n",
       "      <td>West Indies(Trinidad)</td>\n",
       "      <td>death poison food</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RcALZuDe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30417</th>\n",
       "      <td>Zwei Soldaten, Die</td>\n",
       "      <td>German. \"Es war einmal zwei Bauersohn, Die hat...</td>\n",
       "      <td>1923</td>\n",
       "      <td>US(MW)</td>\n",
       "      <td>foreignlanguage soldier food homicide suicide ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RDL056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30418 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  \\\n",
       "0                      10,000 Years Ago   \n",
       "1                10th MTB Flotilla Song   \n",
       "2                            13 Highway   \n",
       "3                              151 Days   \n",
       "4      1861 Anti Confederation Song, An   \n",
       "...                                 ...   \n",
       "30413                              Zula   \n",
       "30414                 Zulu Warrior, The   \n",
       "30415                     Zum Gali Gali   \n",
       "30416                       Zutula Dead   \n",
       "30417                Zwei Soldaten, Die   \n",
       "\n",
       "                                             description earliest_date  \\\n",
       "0                                                    NaN           NaN   \n",
       "1                                                    NaN           NaN   \n",
       "2      \"I went down 13 highway, Down in my baby's doo...          1938   \n",
       "3                                                    NaN           NaN   \n",
       "4                                                    NaN           NaN   \n",
       "...                                                  ...           ...   \n",
       "30413  \"Thou lov'st another, Zula, Thou lovest him al...          1952   \n",
       "30414  \"I-kama zimba zimba zayo I-kama zimba zimba ze...          1946   \n",
       "30415  Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...          1956   \n",
       "30416  A nice girl gave Zutula bitter casava to eat a...          1939   \n",
       "30417  German. \"Es war einmal zwei Bauersohn, Die hat...          1923   \n",
       "\n",
       "                    found_in  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                     US(SE)   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "...                      ...   \n",
       "30413                 US(So)   \n",
       "30414                    NaN   \n",
       "30415                    NaN   \n",
       "30416  West Indies(Trinidad)   \n",
       "30417                 US(MW)   \n",
       "\n",
       "                                                keywords  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2          grief love promise nonballad lover technology   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "30413                   love rejection separation travel   \n",
       "30414                        nonballad nonsense campsong   \n",
       "30415                           foreignlanguage campsong   \n",
       "30416                                  death poison food   \n",
       "30417  foreignlanguage soldier food homicide suicide ...   \n",
       "\n",
       "                                        cross_references   roud   bi_file  \\\n",
       "0      see I Was Born About Ten Thousand Years Ago (B...    NaN      R410   \n",
       "1                 see Fred Karno's Army (File: NeFrKaAr)    NaN  NeFrKaAr   \n",
       "2                                                    NaN  29487   Rc13Hwy   \n",
       "3         see Hundred and Fifty-One Days (File: Colq060)    NaN   Colq060   \n",
       "4              see Anti-Confederation Song (File: FJ028)    NaN     FJ028   \n",
       "...                                                  ...    ...       ...   \n",
       "30413                                                NaN  11330   Brne049   \n",
       "30414                                                NaN    NaN  ACFF061A   \n",
       "30415                                                NaN    NaN  ACSF314Z   \n",
       "30416                                                NaN    NaN  RcALZuDe   \n",
       "30417                                                NaN    NaN    RDL056   \n",
       "\n",
       "      st_file dt_file  \n",
       "0         NaN     NaN  \n",
       "1         NaN     NaN  \n",
       "2         NaN     NaN  \n",
       "3         NaN     NaN  \n",
       "4         NaN     NaN  \n",
       "...       ...     ...  \n",
       "30413     NaN     NaN  \n",
       "30414     NaN     NaN  \n",
       "30415     NaN     NaN  \n",
       "30416     NaN     NaN  \n",
       "30417     NaN     NaN  \n",
       "\n",
       "[30418 rows x 10 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop to extract information for each record\n",
    "for record in records:\n",
    "    # dict to store key and value for each record\n",
    "    record_data = {}\n",
    "    # loop to extract information for each field in each record\n",
    "    for field, pattern in field_patterns.items():\n",
    "        value = re.search(pattern, record.group(1), re.DOTALL)\n",
    "        # if a value matching a search pattern has been found, replace `value`\n",
    "        # to store the right bit: group(1)\n",
    "        if value:\n",
    "            value = value.group(1).strip()\n",
    "            # extra year handling: find all 4-digit years in field then pick lowest\n",
    "            if field == 'earliest_date':\n",
    "                years = re.findall(r'\\b\\d{4}\\b', value)\n",
    "                if years:\n",
    "                    value = min(map(int, years))\n",
    "        else:\n",
    "            value = \"\"\n",
    "        record_data[field] = value\n",
    "\n",
    "    # stub handling: if 'name' line contains ': see' and/or 'File:',\n",
    "    # store these in 'cross_references' and 'bi_file' fields accordingly\n",
    "    name = record_data['name']\n",
    "    if ': see' in name:\n",
    "        cross_ref_idx = name.find(': see')\n",
    "        record_data['cross_references'] = 'see ' + name[cross_ref_idx + 5:].strip()\n",
    "        record_data['name'] = name[:cross_ref_idx].strip()\n",
    "\n",
    "    bi_file_info = record_data['bi_file']\n",
    "    if '(File:' in bi_file_info:\n",
    "        cross_ref_idx = bi_file_info.find('(File:')\n",
    "        # Update the 'cross_references' field only if there's no existing value\n",
    "        if 'cross_references' not in record_data:\n",
    "            record_data['cross_references'] = bi_file_info[cross_ref_idx + 6:].strip().rstrip(')')\n",
    "        record_data['bi_file'] = bi_file_info[:cross_ref_idx].strip()\n",
    "\n",
    "    # remove any brackets from 'file' field\n",
    "    record_data['bi_file'] = record_data['bi_file'].replace('(', '').replace(')', '')\n",
    "    # remove any * from DT filenames\n",
    "    record_data['dt_file'] = record_data['dt_file'].replace('*', '')\n",
    "\n",
    "    # append the new record_data to the records_data\n",
    "    records_data.append(record_data)\n",
    "\n",
    "# create a DataFrame from the records_data\n",
    "df_bi = pd.DataFrame(records_data)\n",
    "\n",
    "# fill NaNs\n",
    "df_bi = df_bi.replace('', np.nan)\n",
    "\n",
    "# check for empty rows\n",
    "# df_bi[df_bi.isnull().all(axis=1)]\n",
    "# remove empty rows (there was only one at the end)\n",
    "df_bi.dropna(how='all', inplace=True)\n",
    "\n",
    "df_bi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with stub handling... eventually delete and compbine above what works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bi.to_csv('df_bi.csv') #save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: 2623 DT file references | Output: 2602 DT file references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2602"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi.query(\"dt_file.notna()\").dt_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: 1180 ST file references | Output: 1166 ST file references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1166"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi.query(\"st_file.notna()\").st_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query shows I would have 3152 main songs with Roud numbers and lyrics if I were to now join up the data and all the referenced lyrics files can be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>roud</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9721</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200, GYPDAVY GYPLADD GYPLADD2* GYPLADD3 GYPLAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15901</th>\n",
       "      <td>Lord Randal [Child 12]</td>\n",
       "      <td>10</td>\n",
       "      <td>C012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12, LORDRAN1* LORDRNLD* EELHENRY* EELHENR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>Bonny Baby Livingston [Child 222]</td>\n",
       "      <td>100</td>\n",
       "      <td>C222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BABLIVST*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>Bold Privateer, The [Laws O32]</td>\n",
       "      <td>1000</td>\n",
       "      <td>LO32</td>\n",
       "      <td>LO32 (Full)</td>\n",
       "      <td>486, BOLDPRIV BLDPRIV2*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7379</th>\n",
       "      <td>Fair Fanny Moore [Laws O38]</td>\n",
       "      <td>1001</td>\n",
       "      <td>LO38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337, FANMOORE FANMOOR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15852</th>\n",
       "      <td>Lord Cornwallis's Surrender</td>\n",
       "      <td>V50597</td>\n",
       "      <td>SBoA088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LRDCRNWL*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17128</th>\n",
       "      <td>Memory of the Dead, The</td>\n",
       "      <td>V5143</td>\n",
       "      <td>PGa039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEMRYDED*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25278</th>\n",
       "      <td>Star-Spangled Banner, The</td>\n",
       "      <td>V5200</td>\n",
       "      <td>SRW008 the source song</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STARSPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13901</th>\n",
       "      <td>Jolly Good Ale and Old (Back and Sides Go Bare)</td>\n",
       "      <td>V7039</td>\n",
       "      <td>DTbcksid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BACK&amp;SID*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6620</th>\n",
       "      <td>Drive the Cold Winter Away (In Praise of Chris...</td>\n",
       "      <td>V9375</td>\n",
       "      <td>Log293</td>\n",
       "      <td>Log293 (Full)</td>\n",
       "      <td>DRIVCOLD ALLHAIL*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3152 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name    roud  \\\n",
       "9721                       Gypsy Laddie, The [Child 200]       1   \n",
       "15901                             Lord Randal [Child 12]      10   \n",
       "2763                   Bonny Baby Livingston [Child 222]     100   \n",
       "2598                      Bold Privateer, The [Laws O32]    1000   \n",
       "7379                         Fair Fanny Moore [Laws O38]    1001   \n",
       "...                                                  ...     ...   \n",
       "15852                        Lord Cornwallis's Surrender  V50597   \n",
       "17128                            Memory of the Dead, The   V5143   \n",
       "25278                          Star-Spangled Banner, The   V5200   \n",
       "13901    Jolly Good Ale and Old (Back and Sides Go Bare)   V7039   \n",
       "6620   Drive the Cold Winter Away (In Praise of Chris...   V9375   \n",
       "\n",
       "                      bi_file        st_file  \\\n",
       "9721                     C200            NaN   \n",
       "15901                    C012            NaN   \n",
       "2763                     C222            NaN   \n",
       "2598                     LO32    LO32 (Full)   \n",
       "7379                     LO38            NaN   \n",
       "...                       ...            ...   \n",
       "15852                 SBoA088            NaN   \n",
       "17128                  PGa039            NaN   \n",
       "25278  SRW008 the source song            NaN   \n",
       "13901                DTbcksid            NaN   \n",
       "6620                   Log293  Log293 (Full)   \n",
       "\n",
       "                                                 dt_file  \n",
       "9721   200, GYPDAVY GYPLADD GYPLADD2* GYPLADD3 GYPLAD...  \n",
       "15901         12, LORDRAN1* LORDRNLD* EELHENRY* EELHENR2  \n",
       "2763                                           BABLIVST*  \n",
       "2598                             486, BOLDPRIV BLDPRIV2*  \n",
       "7379                              337, FANMOORE FANMOOR2  \n",
       "...                                                  ...  \n",
       "15852                                          LRDCRNWL*  \n",
       "17128                                          MEMRYDED*  \n",
       "25278                                           STARSPAN  \n",
       "13901                                          BACK&SID*  \n",
       "6620                                   DRIVCOLD ALLHAIL*  \n",
       "\n",
       "[3152 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics_available = df_bi.query(\"(st_file.notna() | dt_file.notna()) & roud.notna()\")\n",
    "df_lyrics_available[['name', 'roud', 'bi_file', 'st_file', 'dt_file']].sort_values('roud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number of songs may even increase if:\n",
    "1. I can match variant lyrics from the other data sources to the variant titles and multiple file references listed here, in order to get more song records\n",
    "2. by chance, some backwards file references to BI files are found in the two lyrics data sources which were not found inside the Ballad Index\n",
    "\n",
    "However, this is still unlikely to constitute enough data to cluster lyrics into Roud-sized clusters and compare sytems, as our available data currently only averages 1.03 songs per unique Roud number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3030"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique Roud numbers amongst songs that now have lyrics matched:\n",
    "df_lyrics_available.roud.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2303"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entries with lyrics from DT:\n",
    "df_lyrics_available.dt_file.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4141"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of DT entries on songs with Roud numbers:\n",
    "def word_count(series):\n",
    "    text = ' '.join(series.dropna().astype(str))\n",
    "    files = text.split()\n",
    "    return len(files)\n",
    "\n",
    "word_count(df_lyrics_available.dt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even allowing for the multiple entries per BI row for DT files and assuming we can use all of them, that would leave us with a maximum of 4990 lyrics as things stand, giving a song-to-Roud ratio of only 1.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST (Supplementary Tradition of BI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Supplementary Tradition is the lyrics index of the Ballad index. Again, I must use regular expressions to extract the data, this time from `supptrad.txt`. This has a different format to the BI. \n",
    "\n",
    "The main song title is listed at the head of the records, followed by the type of lyrics [Complete text(s) or Partial text(s)] followed by different versions of the lyrics marked [*** A ***, *** B ***, *** C ***, ...] often preceded by an alternate title and notes about the story and/or provenance of the lyrics."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "===\n",
    "Version 6.5  February 26, 2022\n",
    "===\n",
    "\n",
    "A Robin, Jolly Robin\n",
    "  Complete text(s)\n",
    "\n",
    "          *** A ***\n",
    "\n",
    "A Robyn Jolly Robyn\n",
    "\n",
    "From Percy/Wheatley, I.ii.4, pp. 186-187\n",
    "\n",
    "\"[P]rinted from what appears to be the most ancient of Dr.\n",
    "Harrington's poetical MSS. and which has, therefore, been marked\n",
    "[...]\n",
    "\n",
    "A Robyn,\n",
    "  Jolly Robyn,\n",
    "Tell me how thy leman doeth\n",
    "  And thou shalt know of myn.\n",
    "\n",
    "'My lady is unkynde perde.'\n",
    "  Alack! why is she so?\n",
    "'She loveth an other better than me;\n",
    "  And yet she will say no.'\n",
    "[...]\n",
    "\n",
    "          *** B ***\n",
    "\n",
    "(No title)\n",
    "\n",
    "From Shakespeare, \"Twelfth Night\" Act IV, scene 2. In the scene,\n",
    "the Clown and Malvolio are talking past each other. The text\n",
    "below shows the reconstructed lines of the song, with Malvolio's\n",
    "answers in the margin. Line numbers are in the left margin.\n",
    "\n",
    "71 'Hey, Robin, jolly Robin,\n",
    "72    Tell me how thy lady does.'      Malv: Fool.\n",
    "74 'My lady is unkind, perdie!'        Malv: Fool.\n",
    "76 'Alas, why is she so?'              Malv: Fool, I say.\n",
    "78 'She loves another.'  Who calls, ha?\n",
    "\n",
    "File: Perc1185\n",
    "===\n",
    "\n",
    "A, U, Hinny Bird\n",
    "  Partial text(s)\n",
    "\n",
    "          *** A ***\n",
    "\n",
    "From Stokoe/Reay, Songs and Ballads of Northern England, pp. 160-161.\n",
    "\n",
    "Its O, but aw ken well --\n",
    "    A, U, hinny burd;\n",
    "The bonny lass o' Benwell,\n",
    "    A, U, A.\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the aforementioned song-based classification system of the BI, multiple alternate versions are often linked to one BI record file and key title. Later I may want to split the files into different versions, so I will treat the the main song record as a parent (`key_`...) and treat the versions as children which will stand as individual records but inherit some values from their parents. Some of the alternate versions do not have their own names.\n",
    "\n",
    "I want to extract: `key_name`, `key_full_part`, `version_in_key`, `name`, `provenance` [detected in order to exclude from lyrics], `lyrics`, `bi_file` [this belongs to key/parent but I want to name consistently for later data combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, I.ii.4, pp. 186-187</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>B</td>\n",
       "      <td>From Shakespeare, \"Twelfth Night\" Act IV, scen...</td>\n",
       "      <td>(No title)</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>A</td>\n",
       "      <td>From Stokoe/Reay, Songs and Ballads of Norther...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>A</td>\n",
       "      <td>As found in Gale Huntington, Songs the Whaleme...</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>A</td>\n",
       "      <td>From the Bodleian Library (Cambridge), MS. Sel...</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>Young Strongbow</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>FlNG210</td>\n",
       "      <td>A</td>\n",
       "      <td>From Helen Hartness Flanders, Elizabeth Flande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In olden times there came, A likely youth who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>Young Waters [Child 94]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>C094</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, II.ii.18, pp. 229-231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one sheet 8vo.\", About Yule, quhen the wind bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>Zeb Tourney's Girl [Laws E18]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>LE18</td>\n",
       "      <td>A</td>\n",
       "      <td>As recorded by Vernon Dalhart, 1926. Transcrib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Down in the Tennessee mountains, Away from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>Zek'l Weep</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>San449</td>\n",
       "      <td>A</td>\n",
       "      <td>From Carl Sandburg, The American Songbag, pp. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Zek'l weep, Zek'l moan, Flesh come a-creepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Zion's Sons and Daughters</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>Fus214</td>\n",
       "      <td>A</td>\n",
       "      <td>From Harvey H. Fuson, Ballads of the Kentucky ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>See the fountain opened wide, That from sinnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name     key_full_part   bi_file  \\\n",
       "0              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "1              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "2                  A, U, Hinny Bird   Partial text(s)   StoR160   \n",
       "3      Adieu to Erin (The Emigrant)  Complete text(s)   SWMS255   \n",
       "4              Agincourt Carol, The  Complete text(s)     MEL51   \n",
       "...                             ...               ...       ...   \n",
       "1224                Young Strongbow   Partial text(s)   FlNG210   \n",
       "1225        Young Waters [Child 94]  Complete text(s)      C094   \n",
       "1226  Zeb Tourney's Girl [Laws E18]  Complete text(s)      LE18   \n",
       "1227                     Zek'l Weep  Complete text(s)    San449   \n",
       "1228      Zion's Sons and Daughters   Partial text(s)    Fus214   \n",
       "\n",
       "     version_in_key                                         provenance  \\\n",
       "0                 A           From Percy/Wheatley, I.ii.4, pp. 186-187   \n",
       "1                 B  From Shakespeare, \"Twelfth Night\" Act IV, scen...   \n",
       "2                 A  From Stokoe/Reay, Songs and Ballads of Norther...   \n",
       "3                 A  As found in Gale Huntington, Songs the Whaleme...   \n",
       "4                 A  From the Bodleian Library (Cambridge), MS. Sel...   \n",
       "...             ...                                                ...   \n",
       "1224              A  From Helen Hartness Flanders, Elizabeth Flande...   \n",
       "1225              A         From Percy/Wheatley, II.ii.18, pp. 229-231   \n",
       "1226              A  As recorded by Vernon Dalhart, 1926. Transcrib...   \n",
       "1227              A  From Carl Sandburg, The American Songbag, pp. ...   \n",
       "1228              A  From Harvey H. Fuson, Ballads of the Kentucky ...   \n",
       "\n",
       "                       name                                             lyrics  \n",
       "0       A Robyn Jolly Robyn  \"[F]rom what appears to be the most ancient of...  \n",
       "1                (No title)  71 'Hey, Robin, jolly Robin, 72    Tell me how...  \n",
       "2                       NaN  A, U, hinny burd; The bonny lass o' Benwell, A...  \n",
       "3             Adieu to Erin  Oh, when I breathed a last adieu, To Erin's an...  \n",
       "4     The Song of Agincourt  Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "...                     ...                                                ...  \n",
       "1224                    NaN  In olden times there came, A likely youth who ...  \n",
       "1225                    NaN  one sheet 8vo.\", About Yule, quhen the wind bl...  \n",
       "1226                    NaN  Down in the Tennessee mountains, Away from the...  \n",
       "1227                    NaN  1 Zek'l weep, Zek'l moan, Flesh come a-creepin...  \n",
       "1228                    NaN  See the fountain opened wide, That from sinnin...  \n",
       "\n",
       "[1229 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/BalladIndex/txt/supptradedited.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "def parse_lyric_information(data):\n",
    "    outer_records = data.split(\"\\n===\\n\")  # split into outer records\n",
    "    records_list = []\n",
    "\n",
    "    for record in outer_records:\n",
    "        outer_lines = record.strip().split('\\n')\n",
    "        if len(outer_lines) < 2:\n",
    "            continue  # skip 'records' with insufficient lines\n",
    "\n",
    "        key_name = None\n",
    "        key_full_part = None\n",
    "        bi_file = None\n",
    "\n",
    "        inner_records = record.strip().split('          *** ')[1:]  # split into inner records\n",
    "\n",
    "        for i, line in enumerate(outer_lines):\n",
    "            if line.startswith(\"===\"):\n",
    "                if i > 0:\n",
    "                    break  # Stop looking for key_name and key_full_part after the first record\n",
    "            elif not key_name:\n",
    "                key_name = line.strip()\n",
    "            elif not key_full_part:\n",
    "                key_full_part = line.strip()\n",
    "            elif not bi_file:\n",
    "                bi_file_match = re.search(r\"File: (.+)\", line)\n",
    "                if bi_file_match:\n",
    "                    bi_file = bi_file_match.group(1).strip()\n",
    "\n",
    "        for inner_record in inner_records:\n",
    "            lines = inner_record.strip().split('\\n')\n",
    "            version_in_key = None\n",
    "            name = None\n",
    "            provenance = None\n",
    "            lyrics = None\n",
    "\n",
    "            is_in_provenance = False\n",
    "            provenance_lines = []\n",
    "            is_in_lyrics = False\n",
    "            lyrics_lines = []\n",
    "\n",
    "            for line in lines:\n",
    "                if not version_in_key and line.strip() and line.strip()[0].isupper():\n",
    "                    version_in_key = line.strip()[0]\n",
    "                elif not name and line.strip() and not line.strip().startswith(\"From \") and not line.strip().startswith(\"Text \") and \\\n",
    "                        not line.strip().startswith(\"Derived \") and not line.strip().startswith(\"As printed \") and \\\n",
    "                        not line.strip().startswith(\"Supplied \") and not line.strip().startswith(\"Lyrics \") and \\\n",
    "                        not line.strip().startswith(\"As found in \") and not line.strip().startswith(\"As recorded \") and \\\n",
    "                        not line.strip().startswith(\"Also from \") and not line.strip().startswith(\"Also supplied\") and \\\n",
    "                        not line.strip().startswith(\"Derived from \"):\n",
    "                    if name is None:\n",
    "                        name = line.strip()\n",
    "                elif not provenance and re.match(r\"^(From |Text |Derived |As printed |Supplied |Lyrics |As found in |As recorded |Also from |Also supplied|Derived from )\", line):\n",
    "                    is_in_provenance = True\n",
    "                elif not lyrics and not is_in_provenance and not is_in_lyrics and version_in_key:\n",
    "                    is_in_lyrics = True\n",
    "\n",
    "                if is_in_provenance:\n",
    "                    if line.strip():\n",
    "                        provenance_lines.append(line.strip())\n",
    "                    elif not line.strip() and provenance_lines:\n",
    "                        is_in_provenance = False\n",
    "                        provenance = \"\\n\".join(provenance_lines)\n",
    "                        provenance_lines = []\n",
    "                elif is_in_lyrics:\n",
    "                    if line.strip() and name is not None and name not in line and not line.startswith('File: '):\n",
    "                        if line.strip() == \"===\":\n",
    "                            is_in_lyrics = False  # Stop capturing lyrics at the demarcating line\n",
    "                        else:\n",
    "                            if lyrics_lines and not lyrics_lines[-1].endswith(('.', '?', '!', ',', ';', ':',)):\n",
    "                                lyrics_lines[-1] += ', ' + line.strip()\n",
    "                            else:\n",
    "                                lyrics_lines.append(line.strip())\n",
    "\n",
    "            if provenance_lines:\n",
    "                provenance = \"\\n\".join(provenance_lines)\n",
    "\n",
    "            if name is not None:\n",
    "                if name != \"\" and name in lines:\n",
    "                    name_index = lines.index(name)\n",
    "                    if name_index == 0 or lines[name_index - 1] == \"\" and (name_index == len(lines) - 1 or lines[name_index + 1] == \"\"):\n",
    "                        name = name.strip()\n",
    "                    else:\n",
    "                        name = \"\"\n",
    "\n",
    "            # join the collected lyrics lines from the list into a string\n",
    "            if lyrics_lines:\n",
    "                lyrics = \" \".join(lyrics_lines)\n",
    "\n",
    "            # append the extracted data to the records list\n",
    "            records_list.append([key_name, key_full_part, bi_file, version_in_key, provenance, name, lyrics])\n",
    "\n",
    "    # create a DataFrame from the records list \n",
    "    columns = [\"key_name\", \"key_full_part\", \"bi_file\", \"version_in_key\", \"provenance\", \"name\", \"lyrics\"]\n",
    "    df = pd.DataFrame(records_list, columns=columns)\n",
    "    return df\n",
    "\n",
    "df_st = parse_lyric_information(data)\n",
    "df_st = df_st.replace('', np.nan)\n",
    "df_st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: 1136 records | Output: 1229 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT (Mudcat's Digitrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only Mudcat Digitrad file available to download is an AskSam 32-bit MS-DOS database which I was not able to open. I was able to access a database file in the ZIP where lyrics were visible in plan text.\n",
    "\n",
    "The lack of consistent record delimiters, field labels/delimiters, and the presence of many (often invisibe) unicode control characters made extraction challenging and unreliable. I extracted data using regular expressions, after using a text editor to add some line breaks and spaces in place of some errant unicode characters in the source (itself a marginally more human-readable side-effect of a failed attempt to open the database in a newer version of AskSam for Windows).\n",
    "\n",
    "Due to the aforementioned challanges, there are still some issues with the data:\n",
    "- some titles are incorrect \n",
    "- some lyrics are incomplete due to titles being recognised in the wrong places \n",
    "- some lyrics still include notes on the text which were not easy to separate from the lyrics themselves\n",
    "\n",
    "This data is stored in `df_dt`:\n",
    "\n",
    "Target: 8932 file records |\n",
    "Output: 8249 file records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_file</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARDTAC</td>\n",
       "      <td>'ARD TAC</td>\n",
       "      <td>1.\u0002I'm a shearer, yes I am, and I've shorn 'em...</td>\n",
       "      <td>[Australia, sheep, shearing, drink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FISHFRY</td>\n",
       "      <td>(I'VE GOT) BIGGER FISH TO FRY</td>\n",
       "      <td>Sittin' on the bank of that muddy Mississippi,...</td>\n",
       "      <td>[fishing, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JULY12</td>\n",
       "      <td>THE 12TH OF JULY</td>\n",
       "      <td>Come pledge again your heart and your hand\\n O...</td>\n",
       "      <td>[Irish, peace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVENUE16</td>\n",
       "      <td>16TH AVENUE</td>\n",
       "      <td>From the corners of the country, from the citi...</td>\n",
       "      <td>[country]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASS1913</td>\n",
       "      <td>THE 1913 MASSACRE</td>\n",
       "      <td>Take a trip with me in nineteen thirteen\\nTo C...</td>\n",
       "      <td>[union, work, death, Xmas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>ZEB TOURNEY'S GIRL</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "      <td>[feud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8245</th>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>ZEBRA DUN</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "      <td>[cowboy, animal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8246</th>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>ZEN GOSPEL SINGING</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "      <td>[religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8247</th>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>ZULEIKA</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "      <td>[marriage, infidelity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8248</th>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>THE ZULU KING</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\n\u0002Fell...</td>\n",
       "      <td>[camp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dt_file                           name  \\\n",
       "0      HARDTAC                       'ARD TAC   \n",
       "1      FISHFRY  (I'VE GOT) BIGGER FISH TO FRY   \n",
       "2       JULY12               THE 12TH OF JULY   \n",
       "3     AVENUE16                    16TH AVENUE   \n",
       "4     MASS1913              THE 1913 MASSACRE   \n",
       "...        ...                            ...   \n",
       "8244  ZEBTURNY             ZEB TOURNEY'S GIRL   \n",
       "8245  ZEBRADUN                      ZEBRA DUN   \n",
       "8246  ZENGOSPE             ZEN GOSPEL SINGING   \n",
       "8247    ZULIKA                        ZULEIKA   \n",
       "8248  ZULUKING                  THE ZULU KING   \n",
       "\n",
       "                                                 lyrics  \\\n",
       "0     1.\u0002I'm a shearer, yes I am, and I've shorn 'em...   \n",
       "1     Sittin' on the bank of that muddy Mississippi,...   \n",
       "2     Come pledge again your heart and your hand\\n O...   \n",
       "3     From the corners of the country, from the citi...   \n",
       "4     Take a trip with me in nineteen thirteen\\nTo C...   \n",
       "...                                                 ...   \n",
       "8244  Down in the Tennessee mountains,\\nFar from the...   \n",
       "8245  We was camped on the plains at the head of the...   \n",
       "8246  I once was a Baptist and on each Sunday morn\\n...   \n",
       "8247  Zuleika was fair to see,\\nA fair Persian maide...   \n",
       "8248  Oh the Zulu king with the big nose-ring\\n\u0002Fell...   \n",
       "\n",
       "                                 keywords  \n",
       "0     [Australia, sheep, shearing, drink]  \n",
       "1                         [fishing, food]  \n",
       "2                          [Irish, peace]  \n",
       "3                               [country]  \n",
       "4              [union, work, death, Xmas]  \n",
       "...                                   ...  \n",
       "8244                               [feud]  \n",
       "8245                     [cowboy, animal]  \n",
       "8246                           [religion]  \n",
       "8247               [marriage, infidelity]  \n",
       "8248                               [camp]  \n",
       "\n",
       "[8249 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/Mudcat/Z02cv4edited.txt', 'r', encoding='latin-1') as file:\n",
    "    data = file.read()\n",
    "\n",
    "def extract_records_from_text(text):\n",
    "    # split records based on name detection\n",
    "    records = re.split(r'\\n(?=[A-Z0-9\\s\\'\\\"\\?\\!\\.\\,\\(\\)\\[\\]\\:\\;\\–\\—\\-]+[A-Z0-9][A-Z0-9\\s\\'\\\"\\?\\!\\.\\,\\(\\)\\\\[\\]:\\;\\–\\—\\-]{4,}(?:\\n|$))', text)\n",
    "\n",
    "    # lists to store extracted data\n",
    "    filenames = []\n",
    "    names = []\n",
    "    lyrics = []\n",
    "    keywords = []\n",
    "\n",
    "    # iterate over records to extract data\n",
    "    i = 0\n",
    "    while i < len(records):\n",
    "        record = records[i]\n",
    "\n",
    "        # find and store the name\n",
    "        name_match = re.search(r'^\\s*([A-Z0-9\\s\\'\\\"\\?\\!\\.\\,\\(\\)\\[\\]\\:\\;\\–\\—\\-]+[A-Z0-9][A-Z0-9\\s\\'\\\"\\?\\!.\\,\\(\\)\\\\[\\]:\\;\\–\\—\\-]{4,})\\s*$', record, flags=re.MULTILINE)\n",
    "        if name_match and not re.match(r'^-+$', name_match.group(1)) and '\\n' not in name_match.group(1):\n",
    "            name = name_match.group(1).strip()\n",
    "        else:\n",
    "            name = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # reject name if it's one of the other strings that produces false matches - TODO: cobine this above?\n",
    "        if name == 'OCT98':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # find and store keywords (each staring @ and all on the same line)\n",
    "        keywords_match = re.search(r'@(.+?)\\n', record)\n",
    "        if keywords_match:\n",
    "            keywords_line = keywords_match.group(1)\n",
    "            keywords_list = [keyword.strip('@') for keyword in keywords_line.split() if keyword.strip('@').isalnum()]\n",
    "        else:\n",
    "            keywords_list = []\n",
    "\n",
    "        # find and store the lyrics section (everything between name and keywords or filename)\n",
    "        lyrics_match = re.search(r'(?<=^' + re.escape(name) + r'\\n)(.*?)(?=\\n@|filename:)', record, flags=re.DOTALL)\n",
    "        if lyrics_match:\n",
    "            lyrics_text = lyrics_match.group(1).strip()\n",
    "\n",
    "            # don't store the first line of the section if it's likely a note\n",
    "            first_line_break_idx = lyrics_text.find('\\n')\n",
    "            if first_line_break_idx != -1:\n",
    "                first_line = lyrics_text[:first_line_break_idx].strip()\n",
    "                if first_line.startswith('(') and first_line.endswith(')') or first_line == '-Traditional':\n",
    "                    lyrics_text = lyrics_text[first_line_break_idx+1:].strip()\n",
    "            \n",
    "            # cut off lyrics if there is a line underscores\n",
    "            lyrics_cutoff_idx = lyrics_text.find('_________________________')\n",
    "            if lyrics_cutoff_idx != -1:\n",
    "                lyrics_text = lyrics_text[:lyrics_cutoff_idx].strip()\n",
    "\n",
    "        else:\n",
    "            lyrics_text = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # find and store the filename based on 'filename: '\n",
    "        filename_match = re.search(r'filename:\\s*(.*)', record)\n",
    "        if filename_match:\n",
    "            filename = filename_match.group(1).strip() \n",
    "        else:\n",
    "            filename = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # append extracted data to lists\n",
    "        filenames.append(filename)\n",
    "        names.append(name)\n",
    "        lyrics.append(lyrics_text)\n",
    "        keywords.append(keywords_list)\n",
    "\n",
    "        # Move to the next record\n",
    "        i += 1\n",
    "\n",
    "    # create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'dt_file': filenames,\n",
    "        'name': names,\n",
    "        'lyrics': lyrics,\n",
    "        'keywords': keywords\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "df_dt = extract_records_from_text(data)\n",
    "df_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine BI with ST and DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll add the lyrics to the Ballad Index data by merging the other two dataframes on filenames and storing the result as `df_all_plus_lyrics`.\n",
    "\n",
    "Viewing the header names gives me an overview of columns to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BI: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['name', 'description', 'earliest_date', 'found_in', 'keywords',\n",
       "       'cross_references', 'roud', 'bi_file', 'st_file', 'dt_file'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ST: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['key_name', 'key_full_part', 'bi_file', 'version_in_key', 'provenance',\n",
       "       'name', 'lyrics'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'DT: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['dt_file', 'name', 'lyrics', 'keywords'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('BI: ', df_bi.columns,\n",
    "    'ST: ', df_st.columns,\n",
    "    'DT: ', df_dt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_all_plus_lyrics \u001b[39m=\u001b[39m df_bi\u001b[39m.\u001b[39;49mmerge(df_dt, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m'\u001b[39;49m, on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m df_all_plus_lyrics\n",
      "File \u001b[0;32m~/anaconda3/envs/api_env/lib/python3.8/site-packages/pandas/core/frame.py:9843\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9824\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   9825\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   9826\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9839\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   9840\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   9841\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[0;32m-> 9843\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m   9844\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   9845\u001b[0m         right,\n\u001b[1;32m   9846\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m   9847\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m   9848\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m   9849\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m   9850\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m   9851\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m   9852\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   9853\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m   9854\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   9855\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m   9856\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m   9857\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/api_env/lib/python3.8/site-packages/pandas/core/reshape/merge.py:148\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 148\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    149\u001b[0m         left,\n\u001b[1;32m    150\u001b[0m         right,\n\u001b[1;32m    151\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    152\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    153\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    154\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    155\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    156\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    157\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    158\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    159\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    160\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/envs/api_env/lib/python3.8/site-packages/pandas/core/reshape/merge.py:737\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[1;32m    732\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[1;32m    733\u001b[0m (\n\u001b[1;32m    734\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[1;32m    735\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m--> 737\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[1;32m    739\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[0;32m~/anaconda3/envs/api_env/lib/python3.8/site-packages/pandas/core/reshape/merge.py:1203\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m rk \u001b[39m=\u001b[39m cast(Hashable, rk)\n\u001b[1;32m   1202\u001b[0m \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1203\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/api_env/lib/python3.8/site-packages/pandas/core/generic.py:1778\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1776\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1777\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1780\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dt'"
     ]
    }
   ],
   "source": [
    "df_all_plus_lyrics = df_bi.merge(df_dt, how='outer', on='dt')\n",
    "df_all_plus_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDO THIS SECTION as it excludes variant stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll store only the Ballad Index entries with both Roud numbers and lyrics in `df_roud_lyrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_roud_lyrics = df_bi[(~df_bi.dt.isna()) & (~df_bi.roud.isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I have to check how many lyrics and how many Roud numbers I have, to see if there are enough entries per number to enable comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roud_lyrics.roud.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only records with lyrics and number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
