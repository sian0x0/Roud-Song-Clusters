{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English-language folk songs have a long tradition and have changed over time. Songs are not easily idenifiable by name alone, and lyrics often have variations. Steve Roud began indexing his own collection in the 1970s, and his Roud Index has become the standard for grouping together different versions of the same song. He is still indexing as of 2023.\n",
    "\n",
    "Could a machine learning algorithm hope to match his skill? Given the lyrics, would it choose the same groupings of songs, where the line between \"same\" and \"different\" is fuzzy? Could it help with future indexing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Roud index is a lyrics-based classification system (rather than tune-based), the officially-hosted index at vwml.com does not contain lyric transcriptions as a standard data field. Some lyrics are accessible online, presented in scanned images of historical collections, others on linked external sites, others not at all. \n",
    "\n",
    "So the first challenge is to get a dataset with enough full lyrics and Roud numbers in combination. The main contenders for the source of this data are Mudcat and The Traditional Ballad Index, both well-established online song databases.\n",
    "\n",
    "### Mudcat \n",
    "- Project focuses on song lyrics and tunes, but also contains Roud numbers for approximately 300 songs.\n",
    "- Data and formats:\n",
    "    - Digitrad (DT) download: askSam MS-DOS database (last updated in 2002)\n",
    "    - Song web pages\n",
    "    - Forum posts containing songs\n",
    "\n",
    "### The Traditional Ballad Index \n",
    "- Project focuses on cataloguing*, but also has supplementary lyrics for approximately 1110 songs.\n",
    "- Data and formats:\n",
    "    - The Ballad Index Software download: Claris Filemaker database\n",
    "    - Song web pages (without lyrics)\n",
    "    - The Ballad Index (BI) and The Supplemental Tradition (ST) (lyrics) as HTML or TXT lists\n",
    "\n",
    "&ast; This is a similar to approach to Roud, but focused on the basic unit of a song rather than its individual instances (e.g. variations, songbook entries or performances), and therefore uses song titles as its main identifiers, with keywords and first line for disambiguation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the Ballad Index (which would have included ST lyrics) nor the Mudcat Digitrad downloadable databases will open. \n",
    "\n",
    "In order to link Roud numbers to lyrics, I therefore need to work with the `.txt` version of the Ballad Index (which does not include ST) as my base for a new database, extract the records from it, then join ST and DT's lyrics to these records using the various references provided in each data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking data: Filenames as keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To link the lyrics correctly to the main data of the BI, I need fields that act as idenifiers/keys:\n",
    "\n",
    "#### BI filename\n",
    "Alphanumeric filename serving as an identifier for all BI records, also referenced by ST lyrics where they exist.\n",
    "\n",
    "#### DT filename\n",
    "8.3 filename (all-caps without extension) serving as an identifier for all DT records, also sometimes referenced in BI. \n",
    "* Note: in a minority of cases, modified DT filenames also appear to be used as the main BI filenames ('DT' + first six characters in lower or title case), e.g. 'DToatsbe' is the same as 'OATSBEAN' in DT). However, this occasionally disagrees with the stated DT filename for the BI record.\n",
    "\n",
    "#### Other numbers and references:\n",
    "**DT number:** Many records in DT and BI also contain a 'DT #'. This number is not the same as the DT file, and, contrary to my first assumption, nor does it correspond to the SongID in Mudcat URLs (e.g. http://mudcat.org/@displaysong.cfm?SongID=329). It appears to be another grouping system developed by Mudcat and intended to extend Child numbers (see below): \"*Francis J. Child only went up to 305--since there are ballads he didn't include, you may notice some numbers like DT #510 . Not to worry--it just helps locate variants*\".\n",
    "\n",
    "**Roud number:** Found in BI only (at least as far as downloadable data is concerned - song lyrics on Mudcat's website do often include this).\n",
    "\n",
    "**Child number:** The Child Ballads were the first large collection of songs of English and Scottish origin collected by Francis James Child in the 1800s. Many songs contained multiple versions. Child  numbers (1-305) are often referenced in folk song sources.\n",
    "\n",
    "**Laws number:** George Malcolm Laws and the American Folklore Society published a collection of traditional songs in 1957. Laws numbers contain an initial letter which indicates the song's theme, e.g. 'M: Ballads of Family Opposition to Lovers'. Laws numbers are also commonly referenced.\n",
    "\n",
    "**Other collections:** References to other collections are sometimes found, and some of these also have their own numbers for songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction quantity targets (BI, ST, DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on text editor finds I estimate I can extract approximately the following data [with comparisons for a Google domain search of online versions]:\n",
    "- BI: 30445 song record files, of which (in combination):\n",
    "    - 14213 are stubs for variants that only refer to other songs\n",
    "    - 2623 refer to DT files (lyrics) [compare: Google search: 357]; 356 have BI filenames referring to a DT filename\n",
    "    - 1180 refer to ST files (lyrics) [compare: Google search: 395]\n",
    "    - 12126 of these contain Roud index numbers [compare: Google search: 2700]\n",
    "- ST: 1229 lyrics referencing 1136 BI files [no separate online version]\n",
    "- DT: 8932 song record files (lyrics)\n",
    "    - only 1 contains a Roud number [compare: Google search of newer web version: 435]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI (Ballad Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preview of `balldidx.txt`. The text version of the Ballad Index file is tricky to work with as entries are presented as a list with inconsistent headings and mixed data. \n",
    "\n",
    "I first used a text editor to place colons before Roud numbers and DT filenames, so that they could be more easily matched. (This could have been perhaps better achieved with regex, although to begin I decided to save myself a step as they were formatted inconsistently.)\n",
    "\n",
    "Here it is interesting to note that the BI database also references Mudcat's DT filenames, for example 'DT, MASS1913*' above. This means we can also supplement lyrics by cross-referencing this data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "===\n",
    "VERSION 6.5, February 26, 2023\n",
    "===\n",
    "NAME: 10,000 Years Ago: see I Was Born About Ten Thousand Years Ago (Bragging Song) (File: R410)\n",
    "===\n",
    "NAME: 10th MTB Flotilla Song: see Fred Karno's Army (File: NeFrKaAr)\n",
    "===\n",
    "NAME: 13 Highway\n",
    "DESCRIPTION: \"I went down 13 highway, Down in my baby's door Raining and storming, Scarcely see the road.\" \"Clouds dark as night, If my baby don't fail me I'll make every thing all right\" \"Going 60 miles an hour...\" \"Don't the highway look lonesome...\"\n",
    "AUTHOR: unknown\n",
    "EARLIEST_DATE: 1938 (recording, Walter Davis)\n",
    "KEYWORDS: grief love promise nonballad lover technology\n",
    "FOUND_IN: US(SE)\n",
    "REFERENCES: (0 citations)\n",
    "Roud #29487\n",
    "RECORDINGS:\n",
    "Walter Davis, \"13 Highway\" (Bluebird B7693, 1938)\n",
    "Moses Williams, \"13 Highway\" (on USFlorida01)\n",
    "NOTES: Moses Williams sings \"I always wonder why ... That woman didn't treat me right.\" The description follows the Walter Davis recording. - BS\n",
    "Last updated in version 5.0\n",
    "File: Rc13Hwy\n",
    "===\n",
    "NAME: 151 Days: see Hundred and Fifty-One Days (File: Colq060)\n",
    "===\n",
    "NAME: 1861 Anti Confederation Song, An: see Anti-Confederation Song (File: FJ028)\n",
    "===\n",
    "NAME: 1913 Massacre\n",
    "DESCRIPTION: In Calumet, Michigan, striking copper miners and their children are having a Christmas celebration; strike-breakers outside bar the doors then raise a false fire alarm. In the ensuing stampede, seventy-three children are crushed or suffocated\n",
    "AUTHOR: Woody Guthrie\n",
    "EARLIEST_DATE: 1945 (recording by author)\n",
    "KEYWORDS: lie strike death labor-movement mining disaster children\n",
    "FOUND_IN: US\n",
    "REFERENCES: (3 citations)\n",
    "Greenway-AmericanFolksongsOfProtest, pp. 157-158, \"1913 Massacre\"\n",
    "Silber/Silber-FolksingersWordbook, p. 306, \"The 1913 Massacre\" (1 text)\n",
    "DT, MASS1913*\n",
    "Roud #17663\n",
    "RECORDINGS:\n",
    "Woody Guthrie, \"1913 Massacre\" (Asch 360, 1945; on Struggle1, Struggle2)\n",
    "CROSS_REFERENCES:\n",
    "cf. \"One Morning in May (To Hear the Nightingale Sing)\" (tune)\n",
    "NOTES: In the late 19th/early 20th century, the rapid expansion of the electrical industry created great demand for copper, for which the chief source was the mines in the upper peninsula of Michigan. Bitter strikes resulted as the miners, under the leadership of the Western Federation of Miners, demanded decent pay and safer working conditions.\n",
    "Guthrie's description of the events of 1913 is dead-on accurate, according to the residents of Calumet; Italian Hall, where the disaster occurred, was still standing in the early 1980s, but has since been torn down. - PJS\n",
    "There is an historical marker on the site (Italian Hall, 7th Street, Calumet, MI, at its junction with Elm Street, one block south of Highway 203), and the site has not been built over. One of the plaques has a picture of Woody and mentions this song. There are quite a few recent photos of the site on Google Maps. - RBW\n",
    "Last updated in version 6.1\n",
    "File: FSWB306A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a script with regular expressions to import while doing the following:\n",
    "- split song records at the marker '==='\n",
    "- extract only the values for 'name', 'description', 'earliest_date', found_in', 'keywords', 'cross_references', 'roud', 'bi_file', 'st_file', and 'dt_file'\n",
    "- split and store reference song name and filename information in one-line stub records that only serve to reference a main song\n",
    "- extract only the earliest year found in the 'EARLIEST_FOUND:' field which contained mixed data\n",
    "- replace empty fields with NumPy `NaN` to allow for better data manipulation\n",
    "\n",
    "These are stored in `df_bi`.\n",
    "\n",
    "Target: 30445 file records |\n",
    "Output: 30418 file records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>key_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>description</th>\n",
       "      <th>long_description</th>\n",
       "      <th>found_in</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10,000 Years Ago</td>\n",
       "      <td>I Was Born About Ten Thousand Years Ago (Bragg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10th MTB Flotilla Song</td>\n",
       "      <td>Fred Karno's Army</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NeFrKaAr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13 Highway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grief love promise nonballad lover technology</td>\n",
       "      <td>\"I went down 13 highway, Down in my baby's doo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(SE)</td>\n",
       "      <td>Rc13Hwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151 Days</td>\n",
       "      <td>Hundred and Fifty-One Days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Colq060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1861 Anti Confederation Song, An</td>\n",
       "      <td>Anti-Confederation Song</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FJ028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30412</th>\n",
       "      <td>Zula</td>\n",
       "      <td>NaN</td>\n",
       "      <td>love rejection separation travel</td>\n",
       "      <td>\"Thou lov'st another, Zula, Thou lovest him al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(So)</td>\n",
       "      <td>Brne049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30413</th>\n",
       "      <td>Zulu Warrior, The</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonballad nonsense campsong</td>\n",
       "      <td>\"I-kama zimba zimba zayo I-kama zimba zimba ze...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACFF061A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30414</th>\n",
       "      <td>Zum Gali Gali</td>\n",
       "      <td>NaN</td>\n",
       "      <td>foreignlanguage campsong</td>\n",
       "      <td>Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACSF314Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30415</th>\n",
       "      <td>Zutula Dead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>death poison food</td>\n",
       "      <td>A nice girl gave Zutula bitter casava to eat a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Indies(Trinidad)</td>\n",
       "      <td>RcALZuDe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30416</th>\n",
       "      <td>Zwei Soldaten, Die</td>\n",
       "      <td>NaN</td>\n",
       "      <td>foreignlanguage soldier food homicide suicide ...</td>\n",
       "      <td>German. \"Es war einmal zwei Bauersohn, Die hat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(MW)</td>\n",
       "      <td>RDL056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30417 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  \\\n",
       "0                      10,000 Years Ago   \n",
       "1                10th MTB Flotilla Song   \n",
       "2                            13 Highway   \n",
       "3                              151 Days   \n",
       "4      1861 Anti Confederation Song, An   \n",
       "...                                 ...   \n",
       "30412                              Zula   \n",
       "30413                 Zulu Warrior, The   \n",
       "30414                     Zum Gali Gali   \n",
       "30415                       Zutula Dead   \n",
       "30416                Zwei Soldaten, Die   \n",
       "\n",
       "                                                key_name  \\\n",
       "0      I Was Born About Ten Thousand Years Ago (Bragg...   \n",
       "1                                      Fred Karno's Army   \n",
       "2                                                    NaN   \n",
       "3                             Hundred and Fifty-One Days   \n",
       "4                                Anti-Confederation Song   \n",
       "...                                                  ...   \n",
       "30412                                                NaN   \n",
       "30413                                                NaN   \n",
       "30414                                                NaN   \n",
       "30415                                                NaN   \n",
       "30416                                                NaN   \n",
       "\n",
       "                                                keywords  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2          grief love promise nonballad lover technology   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "30412                   love rejection separation travel   \n",
       "30413                        nonballad nonsense campsong   \n",
       "30414                           foreignlanguage campsong   \n",
       "30415                                  death poison food   \n",
       "30416  foreignlanguage soldier food homicide suicide ...   \n",
       "\n",
       "                                             description long_description  \\\n",
       "0                                                    NaN              NaN   \n",
       "1                                                    NaN              NaN   \n",
       "2      \"I went down 13 highway, Down in my baby's doo...              NaN   \n",
       "3                                                    NaN              NaN   \n",
       "4                                                    NaN              NaN   \n",
       "...                                                  ...              ...   \n",
       "30412  \"Thou lov'st another, Zula, Thou lovest him al...              NaN   \n",
       "30413  \"I-kama zimba zimba zayo I-kama zimba zimba ze...              NaN   \n",
       "30414  Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...              NaN   \n",
       "30415  A nice girl gave Zutula bitter casava to eat a...              NaN   \n",
       "30416  German. \"Es war einmal zwei Bauersohn, Die hat...              NaN   \n",
       "\n",
       "                    found_in   bi_file st_file dt_file   roud  \n",
       "0                        NaN      R410     NaN     NaN    NaN  \n",
       "1                        NaN  NeFrKaAr     NaN     NaN    NaN  \n",
       "2                     US(SE)   Rc13Hwy     NaN     NaN  29487  \n",
       "3                        NaN   Colq060     NaN     NaN    NaN  \n",
       "4                        NaN     FJ028     NaN     NaN    NaN  \n",
       "...                      ...       ...     ...     ...    ...  \n",
       "30412                 US(So)   Brne049     NaN     NaN  11330  \n",
       "30413                    NaN  ACFF061A     NaN     NaN    NaN  \n",
       "30414                    NaN  ACSF314Z     NaN     NaN    NaN  \n",
       "30415  West Indies(Trinidad)  RcALZuDe     NaN     NaN    NaN  \n",
       "30416                 US(MW)    RDL056     NaN     NaN    NaN  \n",
       "\n",
       "[30417 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BI extraction (MERGED)\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# get file data\n",
    "file_path = './Data/BalladIndex/txt/BDIDXTXT/balldidxedited.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# define record boundary pattern (handle both cases with optional newline)\n",
    "record_pattern = re.compile(r'(?<=\\n)?===\\n(.*?)(?=\\n===)', re.DOTALL)\n",
    "\n",
    "# feed pattern and data to a callable iterator to get `records` matches\n",
    "records = re.finditer(record_pattern, data)\n",
    "\n",
    "# define the field patterns, including the special pattern for stubs\n",
    "field_patterns = {\n",
    "    'name': r'NAME: (.*?)(?:: see |\\n|$)',\n",
    "    'key_name': r'(?<=: see )([^:(\\n]*)',  # extract main song for stubs\n",
    "    'description': r'DESCRIPTION: (.*?)(?:\\nAUTHOR: |\\nEARLIEST_DATE: |\\nKEYWORDS: |\\nFOUND_IN: |\\nREFERENCES: |\\nSAME_TUNE: |\\nST: |\\nDT: |\\nROUD: |\\nRECORDINGS: |\\nCROSS_REFERENCES: |\\nNOTES: |\\nLast updated|$)',\n",
    "    'author': r'AUTHOR: (.*?)(?:\\n|$)',\n",
    "    'earliest_date': r'EARLIEST_DATE: (.*?)(?:\\n|$)',\n",
    "    'long_description': r'LONG_DESCRIPTION: (.*?)(?:\\n[A-Z]+: |Last updated|$)',\n",
    "    'keywords': r'KEYWORDS: (.*?)(?:\\n|$)',\n",
    "    'found_in': r'FOUND_IN: (.*?)(?:\\n|$)',\n",
    "    'references': r'REFERENCES: (.*?)(?:\\n[A-Z]+: |Last updated|$)',\n",
    "    'same_tune': r'SAME_TUNE: (.*?)(?:\\n|$)',\n",
    "    'st_file': r\"(?<=\\nST: )(.*?)(?=\\n)\",\n",
    "    'dt_file': r\"(?<=\\nDT: )(.*?)(?=\\n)\",\n",
    "    'roud': r'ROUD: (.*?)(?:\\n|$)',\n",
    "    'recordings': r'RECORDINGS:\\n((?:(?!CROSS_REFERENCES|NOTES|Last updated).)*)(?:\\n|$)',\n",
    "    'cross_references': r'CROSS_REFERENCES:\\n((?:(?!NOTES|Last updated).)*)(?:\\n|$)',\n",
    "    'notes': r'NOTES: (.*?)(?:\\nLast updated|$)',\n",
    "    'last_updated': r'Last updated in version (.*?)(?:\\n|$)',\n",
    "    'bi_file': r\"File: (.*?)(?=\\nNAME:|\\n===|\\nLast updated|$)\"\n",
    "}\n",
    "\n",
    "# initialise a list to store dicts of extracted `record_data`\n",
    "records_data = []\n",
    "\n",
    "# loop over each of the `records` from the iterator\n",
    "for record in records:\n",
    "    # check if it is a stub record\n",
    "    is_stub = ': see ' in record.group(1)\n",
    "    if is_stub:\n",
    "        # handle stub records separately\n",
    "        stub_data = re.search(r'NAME: (.*?): see (.*?) \\(File: (.*?)\\)', record.group(1))\n",
    "        if stub_data:\n",
    "            name, key_name, bi_file = stub_data.groups()\n",
    "            record_data = {\n",
    "                'name': name.strip(),\n",
    "                'key_name': key_name.strip(),\n",
    "                'bi_file': bi_file.strip()\n",
    "            }\n",
    "    else:\n",
    "        # iterate over the patterns and `search` them, storing match group 1 with its field in `record_data`\n",
    "        record_data = {}\n",
    "        for field, pattern in field_patterns.items():\n",
    "            value = re.search(pattern, record.group(1))\n",
    "            if value:\n",
    "                value = value.group(1).strip()\n",
    "                # for dates: get earliest year and dump the rest\n",
    "                if field == 'earliest_date':\n",
    "                    years = re.findall(r'\\b\\d{4}\\b', value)\n",
    "                    if years:\n",
    "                        value = min(map(int, years))\n",
    "            else:\n",
    "                value = \"\"\n",
    "            record_data[field] = value\n",
    "\n",
    "    # add each finished record to the list\n",
    "    records_data.append(record_data)\n",
    "\n",
    "# make the data into a df, fill empty fields with `NaN`s\n",
    "df_extracted = pd.DataFrame(records_data)\n",
    "\n",
    "df_bi = df_extracted[['name', 'key_name', 'keywords', 'description', 'long_description', 'found_in', 'bi_file', 'st_file', 'dt_file', 'roud']].copy()\n",
    "df_bi.replace('', np.nan, inplace=True)\n",
    "df_bi.dropna(how='all', inplace=True)\n",
    "df_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stub inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to make stubs inherit Roud number and file references from their parent entries. I do this via a lookup table containing only those 'bi_file' entries that have the other data associated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rc13Hwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FSWB306A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASS1913*</td>\n",
       "      <td>17663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hopk112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hopk039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hopk046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30399</th>\n",
       "      <td>San449</td>\n",
       "      <td>San449 (Full)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30403</th>\n",
       "      <td>SuSm091B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30405</th>\n",
       "      <td>Dett196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30406</th>\n",
       "      <td>Fus214</td>\n",
       "      <td>Fus214 (Partial)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30412</th>\n",
       "      <td>Brne049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bi_file           st_file    dt_file   roud\n",
       "2       Rc13Hwy               NaN        NaN  29487\n",
       "5      FSWB306A               NaN  MASS1913*  17663\n",
       "8       Hopk112               NaN        NaN  29405\n",
       "11      Hopk039               NaN        NaN  29404\n",
       "12      Hopk046               NaN        NaN  29403\n",
       "...         ...               ...        ...    ...\n",
       "30399    San449     San449 (Full)        NaN  12174\n",
       "30403  SuSm091B               NaN        NaN  20694\n",
       "30405   Dett196               NaN        NaN  15233\n",
       "30406    Fus214  Fus214 (Partial)        NaN  16373\n",
       "30412   Brne049               NaN        NaN  11330\n",
       "\n",
       "[12420 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MERGED # make a lookup table where bi_file points to st_file, dt_file and roud\n",
    "df_file_lookup = df_bi[['bi_file', 'st_file', 'dt_file', 'roud']].dropna(subset=['roud', 'st_file', 'dt_file'], how='all')\n",
    "df_file_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGED fill missing values from lookup table\n",
    "df_bi['roud'] = df_bi['roud'].fillna(df_bi['bi_file'].map(df_file_lookup['roud']))\n",
    "df_bi['st_file'] = df_bi['st_file'].fillna(df_bi['bi_file'].map(df_file_lookup['st_file']))\n",
    "df_bi['dt_file'] = df_bi['dt_file'].fillna(df_bi['bi_file'].map(df_file_lookup['dt_file']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>key_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>description</th>\n",
       "      <th>long_description</th>\n",
       "      <th>found_in</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10,000 Years Ago</td>\n",
       "      <td>I Was Born About Ten Thousand Years Ago (Bragg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10th MTB Flotilla Song</td>\n",
       "      <td>Fred Karno's Army</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NeFrKaAr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13 Highway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grief love promise nonballad lover technology</td>\n",
       "      <td>\"I went down 13 highway, Down in my baby's doo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(SE)</td>\n",
       "      <td>Rc13Hwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151 Days</td>\n",
       "      <td>Hundred and Fifty-One Days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Colq060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1861 Anti Confederation Song, An</td>\n",
       "      <td>Anti-Confederation Song</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FJ028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30412</th>\n",
       "      <td>Zula</td>\n",
       "      <td>NaN</td>\n",
       "      <td>love rejection separation travel</td>\n",
       "      <td>\"Thou lov'st another, Zula, Thou lovest him al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(So)</td>\n",
       "      <td>Brne049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30413</th>\n",
       "      <td>Zulu Warrior, The</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonballad nonsense campsong</td>\n",
       "      <td>\"I-kama zimba zimba zayo I-kama zimba zimba ze...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACFF061A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30414</th>\n",
       "      <td>Zum Gali Gali</td>\n",
       "      <td>NaN</td>\n",
       "      <td>foreignlanguage campsong</td>\n",
       "      <td>Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACSF314Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30415</th>\n",
       "      <td>Zutula Dead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>death poison food</td>\n",
       "      <td>A nice girl gave Zutula bitter casava to eat a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>West Indies(Trinidad)</td>\n",
       "      <td>RcALZuDe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30416</th>\n",
       "      <td>Zwei Soldaten, Die</td>\n",
       "      <td>NaN</td>\n",
       "      <td>foreignlanguage soldier food homicide suicide ...</td>\n",
       "      <td>German. \"Es war einmal zwei Bauersohn, Die hat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US(MW)</td>\n",
       "      <td>RDL056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30417 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  \\\n",
       "0                      10,000 Years Ago   \n",
       "1                10th MTB Flotilla Song   \n",
       "2                            13 Highway   \n",
       "3                              151 Days   \n",
       "4      1861 Anti Confederation Song, An   \n",
       "...                                 ...   \n",
       "30412                              Zula   \n",
       "30413                 Zulu Warrior, The   \n",
       "30414                     Zum Gali Gali   \n",
       "30415                       Zutula Dead   \n",
       "30416                Zwei Soldaten, Die   \n",
       "\n",
       "                                                key_name  \\\n",
       "0      I Was Born About Ten Thousand Years Ago (Bragg...   \n",
       "1                                      Fred Karno's Army   \n",
       "2                                                    NaN   \n",
       "3                             Hundred and Fifty-One Days   \n",
       "4                                Anti-Confederation Song   \n",
       "...                                                  ...   \n",
       "30412                                                NaN   \n",
       "30413                                                NaN   \n",
       "30414                                                NaN   \n",
       "30415                                                NaN   \n",
       "30416                                                NaN   \n",
       "\n",
       "                                                keywords  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2          grief love promise nonballad lover technology   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "30412                   love rejection separation travel   \n",
       "30413                        nonballad nonsense campsong   \n",
       "30414                           foreignlanguage campsong   \n",
       "30415                                  death poison food   \n",
       "30416  foreignlanguage soldier food homicide suicide ...   \n",
       "\n",
       "                                             description long_description  \\\n",
       "0                                                    NaN              NaN   \n",
       "1                                                    NaN              NaN   \n",
       "2      \"I went down 13 highway, Down in my baby's doo...              NaN   \n",
       "3                                                    NaN              NaN   \n",
       "4                                                    NaN              NaN   \n",
       "...                                                  ...              ...   \n",
       "30412  \"Thou lov'st another, Zula, Thou lovest him al...              NaN   \n",
       "30413  \"I-kama zimba zimba zayo I-kama zimba zimba ze...              NaN   \n",
       "30414  Hebrew. \"Zum, gali-gali-gali, Zum gali-gali, Z...              NaN   \n",
       "30415  A nice girl gave Zutula bitter casava to eat a...              NaN   \n",
       "30416  German. \"Es war einmal zwei Bauersohn, Die hat...              NaN   \n",
       "\n",
       "                    found_in   bi_file st_file dt_file   roud  \n",
       "0                        NaN      R410     NaN     NaN    NaN  \n",
       "1                        NaN  NeFrKaAr     NaN     NaN    NaN  \n",
       "2                     US(SE)   Rc13Hwy     NaN     NaN  29487  \n",
       "3                        NaN   Colq060     NaN     NaN    NaN  \n",
       "4                        NaN     FJ028     NaN     NaN    NaN  \n",
       "...                      ...       ...     ...     ...    ...  \n",
       "30412                 US(So)   Brne049     NaN     NaN  11330  \n",
       "30413                    NaN  ACFF061A     NaN     NaN    NaN  \n",
       "30414                    NaN  ACSF314Z     NaN     NaN    NaN  \n",
       "30415  West Indies(Trinidad)  RcALZuDe     NaN     NaN    NaN  \n",
       "30416                 US(MW)    RDL056     NaN     NaN    NaN  \n",
       "\n",
       "[30417 rows x 10 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and split multiple 'dt_file' entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I need to handle cases where more than one DT filename is associated with each record, to allow for correct data merging later. I will assign the duplicates to new rows, first discarding DT numbers and other characters that do not constitute a valid DT filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGED clean up so only valid DT filenames remain, separated by a space\n",
    "def clean_filenames(df, column_name):\n",
    "    # reg pattern for valid filenames (5-8 characters, all in capitals)\n",
    "    pattern = r'\\b[A-Z0-9]{5,8}\\b'\n",
    "    \n",
    "    # function for cleaning each cell\n",
    "    def clean_cell(cell_value):\n",
    "        if pd.isna(cell_value):\n",
    "            return np.nan\n",
    "        else:\n",
    "            # find valid filenames in the cell\n",
    "            matches = re.findall(pattern, cell_value)\n",
    "            # return them as a space-separated string\n",
    "            cleaned_value = ' '.join(matches)\n",
    "            return cleaned_value\n",
    "    \n",
    "    # call cleaning function on the column provided in args\n",
    "    df[column_name] = df[column_name].apply(clean_cell)\n",
    "    return df\n",
    "\n",
    "df_bi_cleaned = clean_filenames(df_bi.copy(), 'dt_file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual check suggests the cleaning worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13779</th>\n",
       "      <td>Johnny Fill Up the Bowl (In Eighteen Hundred a...</td>\n",
       "      <td>R227</td>\n",
       "      <td>ABEWASH FORBALES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Admiral Benbow (I)</td>\n",
       "      <td>PBB076</td>\n",
       "      <td>ADBENBOW ADBENBW2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15110</th>\n",
       "      <td>Let Me In This Ae Nicht</td>\n",
       "      <td>DTaenich</td>\n",
       "      <td>AENICHT COLDRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>After the Ball</td>\n",
       "      <td>SRW169</td>\n",
       "      <td>AFTRBALL UNFORTU6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Aiken Drum</td>\n",
       "      <td>OO2007</td>\n",
       "      <td>AIKDRUM AIKDRUM3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28425</th>\n",
       "      <td>Weaver and the Factory Maid, The</td>\n",
       "      <td>DTwvfact</td>\n",
       "      <td>WVFACTGL WEAVFACT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>Brisk Young Butcher, The</td>\n",
       "      <td>DTxmasgo</td>\n",
       "      <td>XMASGOOS XMASGOO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22203</th>\n",
       "      <td>Rare Willie Drowned in Yarrow, or, The Water o...</td>\n",
       "      <td>C215</td>\n",
       "      <td>YARROW2 YARROW3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30179</th>\n",
       "      <td>Young Allan [Child 245]</td>\n",
       "      <td>C245</td>\n",
       "      <td>YNGALAN YNGALAN2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23899</th>\n",
       "      <td>Seventeen Come Sunday [Laws O17]</td>\n",
       "      <td>LO17</td>\n",
       "      <td>YONHIGH ROCKYMT TROOPRM2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name   bi_file  \\\n",
       "13779  Johnny Fill Up the Bowl (In Eighteen Hundred a...      R227   \n",
       "176                                   Admiral Benbow (I)    PBB076   \n",
       "15110                            Let Me In This Ae Nicht  DTaenich   \n",
       "198                                       After the Ball    SRW169   \n",
       "223                                           Aiken Drum    OO2007   \n",
       "...                                                  ...       ...   \n",
       "28425                   Weaver and the Factory Maid, The  DTwvfact   \n",
       "3176                            Brisk Young Butcher, The  DTxmasgo   \n",
       "22203  Rare Willie Drowned in Yarrow, or, The Water o...      C215   \n",
       "30179                            Young Allan [Child 245]      C245   \n",
       "23899                   Seventeen Come Sunday [Laws O17]      LO17   \n",
       "\n",
       "                        dt_file  \n",
       "13779          ABEWASH FORBALES  \n",
       "176           ADBENBOW ADBENBW2  \n",
       "15110          AENICHT COLDRAIN  \n",
       "198           AFTRBALL UNFORTU6  \n",
       "223            AIKDRUM AIKDRUM3  \n",
       "...                         ...  \n",
       "28425         WVFACTGL WEAVFACT  \n",
       "3176          XMASGOOS XMASGOO2  \n",
       "22203           YARROW2 YARROW3  \n",
       "30179          YNGALAN YNGALAN2  \n",
       "23899  YONHIGH ROCKYMT TROOPRM2  \n",
       "\n",
       "[450 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi_cleaned[['name', 'bi_file', 'dt_file']].loc[df_bi_cleaned.dt_file.str.contains(' ', na=False)].sort_values(by='dt_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to split the valid filenames into their own rows and examine the changed rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125147/3582655141.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_df = new_df.append(row, ignore_index=True)  # Include rows with NaN filenames\n",
      "/tmp/ipykernel_125147/3582655141.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_df = new_df.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# MERGED split rows\n",
    "def separate_filenames(df, column_name):\n",
    "    # empty DataFrame to store the results\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # function to split the multiple filenames into new rows\n",
    "    def split_multiple_filenames(row):\n",
    "        nonlocal new_df\n",
    "        if pd.notna(row[column_name]):\n",
    "            filenames = row[column_name].split()\n",
    "            for filename in filenames:\n",
    "                new_row = row.copy()\n",
    "                new_row[column_name] = filename\n",
    "                new_df = new_df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            new_df = new_df.append(row, ignore_index=True)  # Include rows with NaN filenames\n",
    "    \n",
    "    # apply the splitting function to the define column\n",
    "    df.apply(split_multiple_filenames, axis=1)\n",
    "    return new_df\n",
    "\n",
    "df_bi_separated = separate_filenames(df_bi_cleaned, 'dt_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Abdul the Bulbul Emir (II)</td>\n",
       "      <td>EM210</td>\n",
       "      <td>ABDULBL2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Abdul the Bulbul Emir (I)</td>\n",
       "      <td>LxA341</td>\n",
       "      <td>ABDULBUL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>A-Begging I Will Go</td>\n",
       "      <td>K217</td>\n",
       "      <td>ABEGGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14132</th>\n",
       "      <td>Johnny Fill Up the Bowl (In Eighteen Hundred a...</td>\n",
       "      <td>R227</td>\n",
       "      <td>ABEWASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>David's Lamentation</td>\n",
       "      <td>FSWB412B</td>\n",
       "      <td>ABSALON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>Drummer Boy of Waterloo, The [Laws J1]</td>\n",
       "      <td>LJ01</td>\n",
       "      <td>YOUNGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14832</th>\n",
       "      <td>Kingdom Coming (The Year of Jubilo)</td>\n",
       "      <td>R230</td>\n",
       "      <td>YRJUBILO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31061</th>\n",
       "      <td>Zack, the Mormon Engineer</td>\n",
       "      <td>BRaF444</td>\n",
       "      <td>ZACKMORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31067</th>\n",
       "      <td>Zebra Dun, The [Laws B16]</td>\n",
       "      <td>LB16</td>\n",
       "      <td>ZEBRADUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31064</th>\n",
       "      <td>Zeb Tourney's Girl [Laws E18]</td>\n",
       "      <td>LE18</td>\n",
       "      <td>ZEBTURNY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name   bi_file   dt_file\n",
       "85                            Abdul the Bulbul Emir (II)     EM210  ABDULBL2\n",
       "84                             Abdul the Bulbul Emir (I)    LxA341  ABDULBUL\n",
       "48                                   A-Begging I Will Go      K217   ABEGGIN\n",
       "14132  Johnny Fill Up the Bowl (In Eighteen Hundred a...      R227   ABEWASH\n",
       "5795                                 David's Lamentation  FSWB412B   ABSALON\n",
       "...                                                  ...       ...       ...\n",
       "6848              Drummer Boy of Waterloo, The [Laws J1]      LJ01   YOUNGED\n",
       "14832                Kingdom Coming (The Year of Jubilo)      R230  YRJUBILO\n",
       "31061                          Zack, the Mormon Engineer   BRaF444  ZACKMORM\n",
       "31067                          Zebra Dun, The [Laws B16]      LB16  ZEBRADUN\n",
       "31064                      Zeb Tourney's Girl [Laws E18]      LE18  ZEBTURNY\n",
       "\n",
       "[2489 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi_separated[df_bi_separated.duplicated(subset='bi_file', \n",
    "keep=False)].dropna(subset='dt_file').sort_values(by='dt_file')[['name', 'bi_file', 'dt_file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract DT filenames from BI filenames\n",
    "The next stage is to extract possible DT filenames from the bi_file entries. I will match these later on the DT data. Of the entries with BI filenames with this 'DTxxxxxx' pattern, 168 are missing DT filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Allan Water</td>\n",
       "      <td>DTalanwa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Altoona Freight Wreck, The</td>\n",
       "      <td>DTwrck12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>B'y' Sara Burned Down</td>\n",
       "      <td>DTBayous</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>Back and Side Go Bare, Go Bare!</td>\n",
       "      <td>DTbcksid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>Banks of Allen Water, The</td>\n",
       "      <td>DTalanwa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30282</th>\n",
       "      <td>Winter It Is Past, The</td>\n",
       "      <td>DTcurrki</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30290</th>\n",
       "      <td>Winter's Gone and Past</td>\n",
       "      <td>DTcurrki</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30307</th>\n",
       "      <td>Wise Willie</td>\n",
       "      <td>DTcutywr</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30455</th>\n",
       "      <td>Wreck at Latona, The</td>\n",
       "      <td>DTwrck12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31046</th>\n",
       "      <td>Your Grannie and Your Other Grannie</td>\n",
       "      <td>DTgranbu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name   bi_file dt_file\n",
       "432                            Allan Water  DTalanwa     NaN\n",
       "493             Altoona Freight Wreck, The  DTwrck12     NaN\n",
       "1133                 B'y' Sara Burned Down  DTBayous     NaN\n",
       "1188       Back and Side Go Bare, Go Bare!  DTbcksid     NaN\n",
       "1387             Banks of Allen Water, The  DTalanwa     NaN\n",
       "...                                    ...       ...     ...\n",
       "30282               Winter It Is Past, The  DTcurrki     NaN\n",
       "30290               Winter's Gone and Past  DTcurrki     NaN\n",
       "30307                          Wise Willie  DTcutywr     NaN\n",
       "30455                 Wreck at Latona, The  DTwrck12     NaN\n",
       "31046  Your Grannie and Your Other Grannie  DTgranbu     NaN\n",
       "\n",
       "[168 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter rows where 'bi_file' starts with 'DT' and 'dt_file' is NaN\n",
    "filtered_rows = df_bi_separated[df_bi_separated['bi_file'].str.startswith('DT', na=False) & df_bi_separated['dt_file'].isna()]\n",
    "filtered_rows[['name', 'bi_file', 'dt_file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will strip 'DT' from these filenames and insert them in the column DT_file for affected rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Allan Water</td>\n",
       "      <td>DTalanwa</td>\n",
       "      <td>alanwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Altoona Freight Wreck, The</td>\n",
       "      <td>DTwrck12</td>\n",
       "      <td>wrck12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>B'y' Sara Burned Down</td>\n",
       "      <td>DTBayous</td>\n",
       "      <td>Bayous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>Back and Side Go Bare, Go Bare!</td>\n",
       "      <td>DTbcksid</td>\n",
       "      <td>bcksid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>Banks of Allen Water, The</td>\n",
       "      <td>DTalanwa</td>\n",
       "      <td>alanwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30282</th>\n",
       "      <td>Winter It Is Past, The</td>\n",
       "      <td>DTcurrki</td>\n",
       "      <td>currki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30290</th>\n",
       "      <td>Winter's Gone and Past</td>\n",
       "      <td>DTcurrki</td>\n",
       "      <td>currki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30307</th>\n",
       "      <td>Wise Willie</td>\n",
       "      <td>DTcutywr</td>\n",
       "      <td>cutywr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30455</th>\n",
       "      <td>Wreck at Latona, The</td>\n",
       "      <td>DTwrck12</td>\n",
       "      <td>wrck12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31046</th>\n",
       "      <td>Your Grannie and Your Other Grannie</td>\n",
       "      <td>DTgranbu</td>\n",
       "      <td>granbu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name   bi_file dt_file\n",
       "432                            Allan Water  DTalanwa  alanwa\n",
       "493             Altoona Freight Wreck, The  DTwrck12  wrck12\n",
       "1133                 B'y' Sara Burned Down  DTBayous  Bayous\n",
       "1188       Back and Side Go Bare, Go Bare!  DTbcksid  bcksid\n",
       "1387             Banks of Allen Water, The  DTalanwa  alanwa\n",
       "...                                    ...       ...     ...\n",
       "30282               Winter It Is Past, The  DTcurrki  currki\n",
       "30290               Winter's Gone and Past  DTcurrki  currki\n",
       "30307                          Wise Willie  DTcutywr  cutywr\n",
       "30455                 Wreck at Latona, The  DTwrck12  wrck12\n",
       "31046  Your Grannie and Your Other Grannie  DTgranbu  granbu\n",
       "\n",
       "[168 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill 'dt_file' with the remainder of the string after 'DT'\n",
    "df_bi_separated.loc[filtered_rows.index, 'dt_file'] = filtered_rows['bi_file'].str[2:]\n",
    "\n",
    "df_bi_separated.loc[filtered_rows.index][['name', 'bi_file', 'dt_file']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target check:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT file references:\n",
    "\n",
    "Target: 2623 | Initial output: 2605 | Post-split: 3264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3432"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi_separated.query(\"dt_file.notna()\").dt_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ST file references:\n",
    "\n",
    "Target: 1180 | Initial output: 1166 | Post-split: 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi_separated.query(\"st_file.notna()\").st_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roud references:\n",
    "\n",
    "Target: 12126 | Initial output: 12004 | Post-split: 12656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bi_separated.query(\"roud.notna()\").roud.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique roud numbers: 11266 (note that this is inaccurate as multiple Roud numbers per field are sometimes still present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11266"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique Roud numbers amongst songs that now have lyrics matched:\n",
    "df_bi_separated.query(\"roud.notna()\").roud.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query shows I would have 3782 songs with Roud numbers and lyrics, if I were to now join up the data and all the referenced lyrics files could be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>roud</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GYPLADD3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GYPDAVY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GYPLADD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GYPLADD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>Gypsy Laddie, The [Child 200]</td>\n",
       "      <td>1</td>\n",
       "      <td>C200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GYPLADY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16274</th>\n",
       "      <td>Lord Cornwallis's Surrender</td>\n",
       "      <td>V50597</td>\n",
       "      <td>SBoA088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LRDCRNWL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17588</th>\n",
       "      <td>Memory of the Dead, The</td>\n",
       "      <td>V5143</td>\n",
       "      <td>PGa039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEMRYDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25860</th>\n",
       "      <td>Star-Spangled Banner, The</td>\n",
       "      <td>V5200</td>\n",
       "      <td>MKr015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STARSPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>Drive the Cold Winter Away (In Praise of Chris...</td>\n",
       "      <td>V9375</td>\n",
       "      <td>Log293</td>\n",
       "      <td>Log293 (Full)</td>\n",
       "      <td>ALLHAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796</th>\n",
       "      <td>Drive the Cold Winter Away (In Praise of Chris...</td>\n",
       "      <td>V9375</td>\n",
       "      <td>Log293</td>\n",
       "      <td>Log293 (Full)</td>\n",
       "      <td>DRIVCOLD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3784 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name    roud  bi_file  \\\n",
       "10003                      Gypsy Laddie, The [Child 200]       1     C200   \n",
       "10000                      Gypsy Laddie, The [Child 200]       1     C200   \n",
       "10001                      Gypsy Laddie, The [Child 200]       1     C200   \n",
       "10002                      Gypsy Laddie, The [Child 200]       1     C200   \n",
       "10011                      Gypsy Laddie, The [Child 200]       1     C200   \n",
       "...                                                  ...     ...      ...   \n",
       "16274                        Lord Cornwallis's Surrender  V50597  SBoA088   \n",
       "17588                            Memory of the Dead, The   V5143   PGa039   \n",
       "25860                          Star-Spangled Banner, The   V5200   MKr015   \n",
       "6797   Drive the Cold Winter Away (In Praise of Chris...   V9375   Log293   \n",
       "6796   Drive the Cold Winter Away (In Praise of Chris...   V9375   Log293   \n",
       "\n",
       "             st_file   dt_file  \n",
       "10003            NaN  GYPLADD3  \n",
       "10000            NaN   GYPDAVY  \n",
       "10001            NaN   GYPLADD  \n",
       "10002            NaN  GYPLADD2  \n",
       "10011            NaN   GYPLADY  \n",
       "...              ...       ...  \n",
       "16274            NaN  LRDCRNWL  \n",
       "17588            NaN  MEMRYDED  \n",
       "25860            NaN  STARSPAN  \n",
       "6797   Log293 (Full)   ALLHAIL  \n",
       "6796   Log293 (Full)  DRIVCOLD  \n",
       "\n",
       "[3784 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics_available = df_bi_separated.query(\"(st_file.notna() | dt_file.notna()) & roud.notna()\")\n",
    "df_lyrics_available[['name', 'roud', 'bi_file', 'st_file', 'dt_file']].sort_values('roud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number of songs could even increase if I could match variant lyrics also based on variant titles\n",
    "or if any new backwards file references to BI files are found in the two lyrics data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will copy these test modifications into `df_bi` and save to csv for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bi = df_bi_separated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bi.to_csv('./Data/df_bi.csv') #save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ST (Supplementary Tradition of BI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Supplementary Tradition is the lyrics index of the Ballad Index. Again, I must use regular expressions to extract the data, this time from `supptrad.txt`. This has a different format to the BI. \n",
    "\n",
    "The main song title is listed at the head of the records, followed by the type of lyrics [Complete text(s) or Partial text(s)] followed by different versions of the lyrics marked [*** A ***, *** B ***, *** C ***, ...] often preceded by an alternate title and notes about the story and/or provenance of the lyrics."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "===\n",
    "Version 6.5  February 26, 2022\n",
    "===\n",
    "\n",
    "A Robin, Jolly Robin\n",
    "  Complete text(s)\n",
    "\n",
    "          *** A ***\n",
    "\n",
    "A Robyn Jolly Robyn\n",
    "\n",
    "From Percy/Wheatley, I.ii.4, pp. 186-187\n",
    "\n",
    "\"[P]rinted from what appears to be the most ancient of Dr.\n",
    "Harrington's poetical MSS. and which has, therefore, been marked\n",
    "[...]\n",
    "\n",
    "A Robyn,\n",
    "  Jolly Robyn,\n",
    "Tell me how thy leman doeth\n",
    "  And thou shalt know of myn.\n",
    "\n",
    "'My lady is unkynde perde.'\n",
    "  Alack! why is she so?\n",
    "'She loveth an other better than me;\n",
    "  And yet she will say no.'\n",
    "[...]\n",
    "\n",
    "          *** B ***\n",
    "\n",
    "(No title)\n",
    "\n",
    "From Shakespeare, \"Twelfth Night\" Act IV, scene 2. In the scene,\n",
    "the Clown and Malvolio are talking past each other. The text\n",
    "below shows the reconstructed lines of the song, with Malvolio's\n",
    "answers in the margin. Line numbers are in the left margin.\n",
    "\n",
    "71 'Hey, Robin, jolly Robin,\n",
    "72    Tell me how thy lady does.'      Malv: Fool.\n",
    "74 'My lady is unkind, perdie!'        Malv: Fool.\n",
    "76 'Alas, why is she so?'              Malv: Fool, I say.\n",
    "78 'She loves another.'  Who calls, ha?\n",
    "\n",
    "File: Perc1185\n",
    "===\n",
    "\n",
    "A, U, Hinny Bird\n",
    "  Partial text(s)\n",
    "\n",
    "          *** A ***\n",
    "\n",
    "From Stokoe/Reay, Songs and Ballads of Northern England, pp. 160-161.\n",
    "\n",
    "Its O, but aw ken well --\n",
    "    A, U, hinny burd;\n",
    "The bonny lass o' Benwell,\n",
    "    A, U, A.\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the aforementioned song-based classification system of the BI, multiple alternate versions are often linked to one BI record file and key title. Later I may want to split the files into different versions, so I will treat the the main song record as a parent ('key_...') and treat the versions as children which will stand as individual records but inherit some values from their parents. Some of the alternate versions do not have their own names.\n",
    "\n",
    "I want to extract: `key_name`, `key_full_part`, `version_in_key`, `name`, `provenance` [detected to exclude from lyrics], `lyrics`, `bi_file` [this belongs to key/parent but I want to name consistently for later data combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, I.ii.4, pp. 186-187</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>B</td>\n",
       "      <td>From Shakespeare, \"Twelfth Night\" Act IV, scen...</td>\n",
       "      <td>(No title)</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>A</td>\n",
       "      <td>From Stokoe/Reay, Songs and Ballads of Norther...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>A</td>\n",
       "      <td>As found in Gale Huntington, Songs the Whaleme...</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>A</td>\n",
       "      <td>From the Bodleian Library (Cambridge), MS. Sel...</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>Young Strongbow</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>FlNG210</td>\n",
       "      <td>A</td>\n",
       "      <td>From Helen Hartness Flanders, Elizabeth Flande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In olden times there came, A likely youth who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>Young Waters [Child 94]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>C094</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, II.ii.18, pp. 229-231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one sheet 8vo.\", About Yule, quhen the wind bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>Zeb Tourney's Girl [Laws E18]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>LE18</td>\n",
       "      <td>A</td>\n",
       "      <td>As recorded by Vernon Dalhart, 1926. Transcrib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Down in the Tennessee mountains, Away from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>Zek'l Weep</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>San449</td>\n",
       "      <td>A</td>\n",
       "      <td>From Carl Sandburg, The American Songbag, pp. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Zek'l weep, Zek'l moan, Flesh come a-creepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>Zion's Sons and Daughters</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>Fus214</td>\n",
       "      <td>A</td>\n",
       "      <td>From Harvey H. Fuson, Ballads of the Kentucky ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>See the fountain opened wide, That from sinnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name     key_full_part   bi_file  \\\n",
       "0              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "1              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "2                  A, U, Hinny Bird   Partial text(s)   StoR160   \n",
       "3      Adieu to Erin (The Emigrant)  Complete text(s)   SWMS255   \n",
       "4              Agincourt Carol, The  Complete text(s)     MEL51   \n",
       "...                             ...               ...       ...   \n",
       "1224                Young Strongbow   Partial text(s)   FlNG210   \n",
       "1225        Young Waters [Child 94]  Complete text(s)      C094   \n",
       "1226  Zeb Tourney's Girl [Laws E18]  Complete text(s)      LE18   \n",
       "1227                     Zek'l Weep  Complete text(s)    San449   \n",
       "1228      Zion's Sons and Daughters   Partial text(s)    Fus214   \n",
       "\n",
       "     version_in_key                                         provenance  \\\n",
       "0                 A           From Percy/Wheatley, I.ii.4, pp. 186-187   \n",
       "1                 B  From Shakespeare, \"Twelfth Night\" Act IV, scen...   \n",
       "2                 A  From Stokoe/Reay, Songs and Ballads of Norther...   \n",
       "3                 A  As found in Gale Huntington, Songs the Whaleme...   \n",
       "4                 A  From the Bodleian Library (Cambridge), MS. Sel...   \n",
       "...             ...                                                ...   \n",
       "1224              A  From Helen Hartness Flanders, Elizabeth Flande...   \n",
       "1225              A         From Percy/Wheatley, II.ii.18, pp. 229-231   \n",
       "1226              A  As recorded by Vernon Dalhart, 1926. Transcrib...   \n",
       "1227              A  From Carl Sandburg, The American Songbag, pp. ...   \n",
       "1228              A  From Harvey H. Fuson, Ballads of the Kentucky ...   \n",
       "\n",
       "                       name                                             lyrics  \n",
       "0       A Robyn Jolly Robyn  \"[F]rom what appears to be the most ancient of...  \n",
       "1                (No title)  71 'Hey, Robin, jolly Robin, 72    Tell me how...  \n",
       "2                       NaN  A, U, hinny burd; The bonny lass o' Benwell, A...  \n",
       "3             Adieu to Erin  Oh, when I breathed a last adieu, To Erin's an...  \n",
       "4     The Song of Agincourt  Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "...                     ...                                                ...  \n",
       "1224                    NaN  In olden times there came, A likely youth who ...  \n",
       "1225                    NaN  one sheet 8vo.\", About Yule, quhen the wind bl...  \n",
       "1226                    NaN  Down in the Tennessee mountains, Away from the...  \n",
       "1227                    NaN  1 Zek'l weep, Zek'l moan, Flesh come a-creepin...  \n",
       "1228                    NaN  See the fountain opened wide, That from sinnin...  \n",
       "\n",
       "[1229 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/BalladIndex/txt/supptradedited.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "def parse_lyric_information(data):\n",
    "    outer_records = data.split(\"\\n===\\n\")  # split into outer records\n",
    "    records_list = []\n",
    "\n",
    "    for record in outer_records:\n",
    "        outer_lines = record.strip().split('\\n')\n",
    "        if len(outer_lines) < 2:\n",
    "            continue  # skip 'records' with insufficient lines\n",
    "\n",
    "        key_name = None\n",
    "        key_full_part = None\n",
    "        bi_file = None\n",
    "\n",
    "        inner_records = record.strip().split('          *** ')[1:]  # split into inner records\n",
    "\n",
    "        for i, line in enumerate(outer_lines):\n",
    "            if line.startswith(\"===\"):\n",
    "                if i > 0:\n",
    "                    break  # Stop looking for key_name and key_full_part after the first record\n",
    "            elif not key_name:\n",
    "                key_name = line.strip()\n",
    "            elif not key_full_part:\n",
    "                key_full_part = line.strip()\n",
    "            elif not bi_file:\n",
    "                bi_file_match = re.search(r\"File: (.+)\", line)\n",
    "                if bi_file_match:\n",
    "                    bi_file = bi_file_match.group(1).strip()\n",
    "\n",
    "        for inner_record in inner_records:\n",
    "            lines = inner_record.strip().split('\\n')\n",
    "            version_in_key = None\n",
    "            name = None\n",
    "            provenance = None\n",
    "            lyrics = None\n",
    "\n",
    "            is_in_provenance = False\n",
    "            provenance_lines = []\n",
    "            is_in_lyrics = False\n",
    "            lyrics_lines = []\n",
    "\n",
    "            for line in lines:\n",
    "                if not version_in_key and line.strip() and line.strip()[0].isupper():\n",
    "                    version_in_key = line.strip()[0]\n",
    "                elif not name and line.strip() and not line.strip().startswith(\"From \") and not line.strip().startswith(\"Text \") and \\\n",
    "                        not line.strip().startswith(\"Derived \") and not line.strip().startswith(\"As printed \") and \\\n",
    "                        not line.strip().startswith(\"Supplied \") and not line.strip().startswith(\"Lyrics \") and \\\n",
    "                        not line.strip().startswith(\"As found in \") and not line.strip().startswith(\"As recorded \") and \\\n",
    "                        not line.strip().startswith(\"Also from \") and not line.strip().startswith(\"Also supplied\") and \\\n",
    "                        not line.strip().startswith(\"Derived from \"):\n",
    "                    if name is None:\n",
    "                        name = line.strip()\n",
    "                elif not provenance and re.match(r\"^(From |Text |Derived |As printed |Supplied |Lyrics |As found in |As recorded |Also from |Also supplied|Derived from )\", line):\n",
    "                    is_in_provenance = True\n",
    "                elif not lyrics and not is_in_provenance and not is_in_lyrics and version_in_key:\n",
    "                    is_in_lyrics = True\n",
    "\n",
    "                if is_in_provenance:\n",
    "                    if line.strip():\n",
    "                        provenance_lines.append(line.strip())\n",
    "                    elif not line.strip() and provenance_lines:\n",
    "                        is_in_provenance = False\n",
    "                        provenance = \"\\n\".join(provenance_lines)\n",
    "                        provenance_lines = []\n",
    "                elif is_in_lyrics:\n",
    "                    if line.strip() and name is not None and name not in line and not line.startswith('File: '):\n",
    "                        if line.strip() == \"===\":\n",
    "                            is_in_lyrics = False  # Stop capturing lyrics at the demarcating line\n",
    "                        else:\n",
    "                            if lyrics_lines and not lyrics_lines[-1].endswith(('.', '?', '!', ',', ';', ':',)):\n",
    "                                lyrics_lines[-1] += ', ' + line.strip()\n",
    "                            else:\n",
    "                                lyrics_lines.append(line.strip())\n",
    "\n",
    "            if provenance_lines:\n",
    "                provenance = \"\\n\".join(provenance_lines)\n",
    "\n",
    "            if name is not None:\n",
    "                if name != \"\" and name in lines:\n",
    "                    name_index = lines.index(name)\n",
    "                    if name_index == 0 or lines[name_index - 1] == \"\" and (name_index == len(lines) - 1 or lines[name_index + 1] == \"\"):\n",
    "                        name = name.strip()\n",
    "                    else:\n",
    "                        name = \"\"\n",
    "\n",
    "            # join the collected lyrics lines from the list into a string\n",
    "            if lyrics_lines:\n",
    "                lyrics = \" \".join(lyrics_lines)\n",
    "\n",
    "            # append the extracted data to the records list\n",
    "            records_list.append([key_name, key_full_part, bi_file, version_in_key, provenance, name, lyrics])\n",
    "\n",
    "    # create a DataFrame from the records list \n",
    "    columns = [\"key_name\", \"key_full_part\", \"bi_file\", \"version_in_key\", \"provenance\", \"name\", \"lyrics\"]\n",
    "    df = pd.DataFrame(records_list, columns=columns)\n",
    "    return df\n",
    "\n",
    "df_st = parse_lyric_information(data)\n",
    "df_st = df_st.replace('', np.nan)\n",
    "df_st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: 1136 records | Output: 1229 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT (Mudcat's Digitrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only Mudcat Digitrad file available to download is an askSam 32-bit MS-DOS database which I was not able to open. I was able to access a database file in the ZIP where lyrics were visible in plan text. However, a lack of consistent record delimiters, field labels/delimiters, and the presence of many (often invisibe) unicode control characters made extraction challenging and unreliable. \n",
    "\n",
    "I extracted data using regular expressions, after using a text editor to add some line breaks and spaces in place of some errant unicode characters in the source. This resulted in a passable but very dirty dataset, especially on the detection of titles which then affected the rest of the detection for a field. I then turned to also using regex in the text file to clean it more.\n",
    "\n",
    "Although the data is now relatively clean, there are still some issues, notably that some lyrics still include notes on the text which were hard to separate from the lyrics themselves due to a lack of consistency. \n",
    "\n",
    "This data is stored in `df_dt`:\n",
    "\n",
    "File records:\n",
    "\n",
    "Target: 8932 | Inital output (minimal cleaning): 8249 | Output post-cleaning: 8726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_file</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARDTAC</td>\n",
       "      <td>'ARD TAC</td>\n",
       "      <td>1.I'm a shearer, yes I am, and I've shorn 'em ...</td>\n",
       "      <td>[Australia, sheep, shearing, drink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FISHFRY</td>\n",
       "      <td>(I'VE GOT) BIGGER FISH TO FRY</td>\n",
       "      <td>Sittin' on the bank of that muddy Mississippi,...</td>\n",
       "      <td>[fishing, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JULY12</td>\n",
       "      <td>THE 12TH OF JULY</td>\n",
       "      <td>Come pledge again your heart and your hand\\nOn...</td>\n",
       "      <td>[Irish, peace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVENUE16</td>\n",
       "      <td>16TH AVENUE</td>\n",
       "      <td>From the corners of the country, from the citi...</td>\n",
       "      <td>[country]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASS1913</td>\n",
       "      <td>THE 1913 MASSACRE</td>\n",
       "      <td>Take a trip with me in nineteen thirteen\\nTo C...</td>\n",
       "      <td>[union, work, death, Xmas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>ZEB TOURNEY'S GIRL</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "      <td>[feud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>ZEBRA DUN</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "      <td>[cowboy, animal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8722</th>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>ZEN GOSPEL SINGING</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "      <td>[religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>ZULEIKA</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "      <td>[marriage, infidelity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>THE ZULU KING</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\nFell ...</td>\n",
       "      <td>[camp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8725 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dt_file                           name  \\\n",
       "0      HARDTAC                       'ARD TAC   \n",
       "1      FISHFRY  (I'VE GOT) BIGGER FISH TO FRY   \n",
       "2       JULY12               THE 12TH OF JULY   \n",
       "3     AVENUE16                    16TH AVENUE   \n",
       "4     MASS1913              THE 1913 MASSACRE   \n",
       "...        ...                            ...   \n",
       "8720  ZEBTURNY             ZEB TOURNEY'S GIRL   \n",
       "8721  ZEBRADUN                      ZEBRA DUN   \n",
       "8722  ZENGOSPE             ZEN GOSPEL SINGING   \n",
       "8723    ZULIKA                        ZULEIKA   \n",
       "8724  ZULUKING                  THE ZULU KING   \n",
       "\n",
       "                                                 lyrics  \\\n",
       "0     1.I'm a shearer, yes I am, and I've shorn 'em ...   \n",
       "1     Sittin' on the bank of that muddy Mississippi,...   \n",
       "2     Come pledge again your heart and your hand\\nOn...   \n",
       "3     From the corners of the country, from the citi...   \n",
       "4     Take a trip with me in nineteen thirteen\\nTo C...   \n",
       "...                                                 ...   \n",
       "8720  Down in the Tennessee mountains,\\nFar from the...   \n",
       "8721  We was camped on the plains at the head of the...   \n",
       "8722  I once was a Baptist and on each Sunday morn\\n...   \n",
       "8723  Zuleika was fair to see,\\nA fair Persian maide...   \n",
       "8724  Oh the Zulu king with the big nose-ring\\nFell ...   \n",
       "\n",
       "                                 keywords  \n",
       "0     [Australia, sheep, shearing, drink]  \n",
       "1                         [fishing, food]  \n",
       "2                          [Irish, peace]  \n",
       "3                               [country]  \n",
       "4              [union, work, death, Xmas]  \n",
       "...                                   ...  \n",
       "8720                               [feud]  \n",
       "8721                     [cowboy, animal]  \n",
       "8722                           [religion]  \n",
       "8723               [marriage, infidelity]  \n",
       "8724                               [camp]  \n",
       "\n",
       "[8725 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MERGED\n",
    "import unicodedata\n",
    "\n",
    "def cleanse_unicode_control(text):\n",
    "    # remove Unicode control characters\n",
    "    cleaned_text = ''.join(c for c in text if unicodedata.category(c)[0] != 'C')\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_records_from_text(text):\n",
    "    # compile regular expressions\n",
    "    name_pattern = re.compile(r'^\\s*([A-Z0-9\\s\\'\\\"\\?\\!\\.\\,\\(\\)\\[\\]\\:\\;\\–\\—\\-]+[A-Z0-9][A-Z0-9\\s\\'\\\"\\?\\!.\\,\\(\\)\\:\\;\\–\\—\\-]{4,})\\s*$', flags=re.MULTILINE)\n",
    "    keywords_pattern = re.compile(r'@(.*?)\\n')\n",
    "    non_lyrics_prefix_pattern = re.compile(\n",
    "        r'^\\s*(?:@|filename:|________|-------|Child #|DT #|play.exe)'\n",
    "        r'|(?:-Author|SOURCE:|Source:|COMMENT:|Comment:|TEXT:|Note:|note:|tune:|Tune:|Tune|Music|music:)'\n",
    "        r'|(?:Printed|Collected|collected|As sung|Sung by|-recorded|Recorded|recorded|Transcribed|From The|From Folk|From Songs)'\n",
    "        r'|(?:Copyright|copyright|(c)|-traditional|-Traditional|-anonymous|-Anonymous|-From|-from|Alternate )')\n",
    "\n",
    "    # split records based on field boundary\n",
    "    boundary = '========'\n",
    "    records = re.split(boundary, text)\n",
    "    names = []\n",
    "    lyrics = []\n",
    "    keywords = []\n",
    "    filenames = []\n",
    "\n",
    "    # find and store the name\n",
    "    for record in records:\n",
    "        name_match = name_pattern.search(record)\n",
    "        if name_match and not re.match(r'^-+$', name_match.group(1)) and '\\n' not in name_match.group(1) and 'CHORUS' not in name_match.group(1):\n",
    "            name = name_match.group(1).strip()\n",
    "        else:\n",
    "            name = ''\n",
    "            continue\n",
    "\n",
    "        # find and mark lyrics, avoiding notes\n",
    "        lines = record.split('\\n')\n",
    "        lyrics_lines = []\n",
    "        lyrics_start = None\n",
    "        lyrics_end = None\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            cleaned_line = cleanse_unicode_control(line).strip() \n",
    "\n",
    "            if not lyrics_start and cleaned_line == name:\n",
    "                lyrics_start = i + 1\n",
    "                continue\n",
    "\n",
    "            elif (cleaned_line.startswith('(') and cleaned_line.endswith(')')) or non_lyrics_prefix_pattern.match(cleaned_line) or keywords_pattern.match(line) or re.search(r'filename:\\s*(.*)', line):\n",
    "                if lyrics_start:\n",
    "                    lyrics_start += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    lyrics_end = i -1\n",
    "                    break\n",
    "\n",
    "            elif lyrics_start and not lyrics_end and cleaned_line:\n",
    "                lyrics_lines.append(cleaned_line)\n",
    "                continue\n",
    "\n",
    "        lyrics_text = '\\n'.join(lyrics_lines)\n",
    "\n",
    "\n",
    "        # find and store keywords (each starting with '@' and all on the same line)\n",
    "        keywords_match = keywords_pattern.search(record)\n",
    "        if keywords_match:\n",
    "            keywords_line = keywords_match.group(1)\n",
    "            keywords_list = [keyword.strip('@') for keyword in keywords_line.split() if keyword.strip('@').isalnum()]\n",
    "        else:\n",
    "            keywords_list = []\n",
    "\n",
    "        # find and store filename\n",
    "        filename_match = re.search(r'filename:\\s*(.*)', record)\n",
    "        if filename_match:\n",
    "            filename = filename_match.group(1).strip()\n",
    "        else:\n",
    "            filename = ''\n",
    "\n",
    "        # append extracted data to lists\n",
    "        names.append(name)\n",
    "        lyrics.append(lyrics_text)\n",
    "        keywords.append(keywords_list)\n",
    "        filenames.append(filename)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'dt_file': filenames,\n",
    "        'name': names,\n",
    "        'lyrics': lyrics,\n",
    "        'keywords': keywords\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "with open('./Data/Mudcat/Z02cv4edited2.txt', 'r', encoding='latin-1') as file:\n",
    "    data = file.read()\n",
    "\n",
    "df_dt = extract_records_from_text(data)\n",
    "df_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI second pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now return to `df_bi` and replace the partial DT filenames (derived above from some of the BI filenames) with matching items from our newly-loaded DT data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the new partial dt_files from the filtered_rows in uppercase\n",
    "new_values = df_bi.loc[filtered_rows.index, 'dt_file'].str.upper()\n",
    "\n",
    "# filter out NaNs\n",
    "lookup_valid_values = df_dt[df_dt['dt_file'].notna()]\n",
    "\n",
    "# function to check for matches and handle NaN\n",
    "def find_match(x):\n",
    "    if pd.notna(x):\n",
    "        matches = lookup_valid_values[lookup_valid_values['dt_file'].str.upper().str.contains(x)]\n",
    "        return matches.iloc[0]['dt_file'] if not matches.empty else x\n",
    "    return x\n",
    "\n",
    "# Replace values in df_bi['dt_file'] with matches\n",
    "df_bi.loc[filtered_rows.index, 'dt_file'] = new_values.apply(find_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also update our lookup table for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rc13Hwy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FSWB306A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MASS1913</td>\n",
       "      <td>17663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hopk112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hopk039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hopk046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31069</th>\n",
       "      <td>San449</td>\n",
       "      <td>San449 Full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31073</th>\n",
       "      <td>SuSm091B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31075</th>\n",
       "      <td>Dett196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31076</th>\n",
       "      <td>Fus214</td>\n",
       "      <td>Fus214 Partial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31082</th>\n",
       "      <td>Brne049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13256 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bi_file         st_file   dt_file   roud\n",
       "2       Rc13Hwy             NaN       NaN  29487\n",
       "5      FSWB306A             NaN  MASS1913  17663\n",
       "8       Hopk112             NaN       NaN  29405\n",
       "11      Hopk039             NaN       NaN  29404\n",
       "12      Hopk046             NaN       NaN  29403\n",
       "...         ...             ...       ...    ...\n",
       "31069    San449     San449 Full       NaN  12174\n",
       "31073  SuSm091B             NaN       NaN  20694\n",
       "31075   Dett196             NaN       NaN  15233\n",
       "31076    Fus214  Fus214 Partial       NaN  16373\n",
       "31082   Brne049             NaN       NaN  11330\n",
       "\n",
       "[13256 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update lookup table\n",
    "df_file_lookup = df_bi.loc[:,'bi_file':'roud'].dropna(subset=['roud', 'st_file', 'dt_file'], how='all')\n",
    "# cleanup parenthesis\n",
    "df_file_lookup = df_file_lookup.applymap(lambda x: x if pd.isna(x) else x.replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "\n",
    "df_file_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Viewing the column names gives me an overview of what to match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_bi: (31087 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>key_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>description</th>\n",
       "      <th>long_description</th>\n",
       "      <th>found_in</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, key_name, keywords, description, long_description, found_in, bi_file, st_file, dt_file, roud]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_file_lookup: (13256 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [bi_file, st_file, dt_file, roud]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_st: (1229 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [key_name, key_full_part, bi_file, version_in_key, provenance, name, lyrics]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_dt: (8725 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_file</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dt_file, name, lyrics, keywords]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pretty print column names and row counts for current dfs using loop\n",
    "for var_name in ['df_bi', 'df_file_lookup', 'df_st', 'df_dt']:\n",
    "    df = locals()[var_name] #access local variables by string name\n",
    "    print(f\"Columns in {var_name}: ({df.shape[0]} rows)\")\n",
    "    display(df.iloc[:0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST lyrics: merge with additional data from BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, I.ii.4, pp. 186-187</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "      <td>Perc1185 Full</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>B</td>\n",
       "      <td>From Shakespeare, \"Twelfth Night\" Act IV, scen...</td>\n",
       "      <td>(No title)</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "      <td>Perc1185 Full</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>A</td>\n",
       "      <td>From Stokoe/Reay, Songs and Ballads of Norther...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "      <td>StoR160 Partial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>A</td>\n",
       "      <td>As found in Gale Huntington, Songs the Whaleme...</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "      <td>SWMS255 Full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>A</td>\n",
       "      <td>From the Bodleian Library (Cambridge), MS. Sel...</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "      <td>MEL51 Full</td>\n",
       "      <td>AGINCRT1</td>\n",
       "      <td>V29347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>Young Strongbow</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>FlNG210</td>\n",
       "      <td>A</td>\n",
       "      <td>From Helen Hartness Flanders, Elizabeth Flande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In olden times there came, A likely youth who ...</td>\n",
       "      <td>FlNG210 Partial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>Young Waters [Child 94]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>C094</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, II.ii.18, pp. 229-231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one sheet 8vo.\", About Yule, quhen the wind bl...</td>\n",
       "      <td>C094 Full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>Zeb Tourney's Girl [Laws E18]</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>LE18</td>\n",
       "      <td>A</td>\n",
       "      <td>As recorded by Vernon Dalhart, 1926. Transcrib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Down in the Tennessee mountains, Away from the...</td>\n",
       "      <td>LE18 Full</td>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>2249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>Zek'l Weep</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>San449</td>\n",
       "      <td>A</td>\n",
       "      <td>From Carl Sandburg, The American Songbag, pp. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Zek'l weep, Zek'l moan, Flesh come a-creepin...</td>\n",
       "      <td>San449 Full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>Zion's Sons and Daughters</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>Fus214</td>\n",
       "      <td>A</td>\n",
       "      <td>From Harvey H. Fuson, Ballads of the Kentucky ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>See the fountain opened wide, That from sinnin...</td>\n",
       "      <td>Fus214 Partial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1312 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name     key_full_part   bi_file  \\\n",
       "0              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "1              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "2                  A, U, Hinny Bird   Partial text(s)   StoR160   \n",
       "3      Adieu to Erin (The Emigrant)  Complete text(s)   SWMS255   \n",
       "4              Agincourt Carol, The  Complete text(s)     MEL51   \n",
       "...                             ...               ...       ...   \n",
       "1307                Young Strongbow   Partial text(s)   FlNG210   \n",
       "1308        Young Waters [Child 94]  Complete text(s)      C094   \n",
       "1309  Zeb Tourney's Girl [Laws E18]  Complete text(s)      LE18   \n",
       "1310                     Zek'l Weep  Complete text(s)    San449   \n",
       "1311      Zion's Sons and Daughters   Partial text(s)    Fus214   \n",
       "\n",
       "     version_in_key                                         provenance  \\\n",
       "0                 A           From Percy/Wheatley, I.ii.4, pp. 186-187   \n",
       "1                 B  From Shakespeare, \"Twelfth Night\" Act IV, scen...   \n",
       "2                 A  From Stokoe/Reay, Songs and Ballads of Norther...   \n",
       "3                 A  As found in Gale Huntington, Songs the Whaleme...   \n",
       "4                 A  From the Bodleian Library (Cambridge), MS. Sel...   \n",
       "...             ...                                                ...   \n",
       "1307              A  From Helen Hartness Flanders, Elizabeth Flande...   \n",
       "1308              A         From Percy/Wheatley, II.ii.18, pp. 229-231   \n",
       "1309              A  As recorded by Vernon Dalhart, 1926. Transcrib...   \n",
       "1310              A  From Carl Sandburg, The American Songbag, pp. ...   \n",
       "1311              A  From Harvey H. Fuson, Ballads of the Kentucky ...   \n",
       "\n",
       "                       name  \\\n",
       "0       A Robyn Jolly Robyn   \n",
       "1                (No title)   \n",
       "2                       NaN   \n",
       "3             Adieu to Erin   \n",
       "4     The Song of Agincourt   \n",
       "...                     ...   \n",
       "1307                    NaN   \n",
       "1308                    NaN   \n",
       "1309                    NaN   \n",
       "1310                    NaN   \n",
       "1311                    NaN   \n",
       "\n",
       "                                                 lyrics          st_file  \\\n",
       "0     \"[F]rom what appears to be the most ancient of...    Perc1185 Full   \n",
       "1     71 'Hey, Robin, jolly Robin, 72    Tell me how...    Perc1185 Full   \n",
       "2     A, U, hinny burd; The bonny lass o' Benwell, A...  StoR160 Partial   \n",
       "3     Oh, when I breathed a last adieu, To Erin's an...     SWMS255 Full   \n",
       "4     Deo gracias anglia, Redde pro victoria, 1 Owre...       MEL51 Full   \n",
       "...                                                 ...              ...   \n",
       "1307  In olden times there came, A likely youth who ...  FlNG210 Partial   \n",
       "1308  one sheet 8vo.\", About Yule, quhen the wind bl...        C094 Full   \n",
       "1309  Down in the Tennessee mountains, Away from the...        LE18 Full   \n",
       "1310  1 Zek'l weep, Zek'l moan, Flesh come a-creepin...      San449 Full   \n",
       "1311  See the fountain opened wide, That from sinnin...   Fus214 Partial   \n",
       "\n",
       "       dt_file    roud  \n",
       "0     HEYROBIN     NaN  \n",
       "1     HEYROBIN     NaN  \n",
       "2          NaN     235  \n",
       "3          NaN    2068  \n",
       "4     AGINCRT1  V29347  \n",
       "...        ...     ...  \n",
       "1307       NaN    4669  \n",
       "1308       NaN    2860  \n",
       "1309  ZEBTURNY    2249  \n",
       "1310       NaN   12174  \n",
       "1311       NaN   16373  \n",
       "\n",
       "[1312 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_plus_bi = df_st.merge(df_file_lookup.drop_duplicates(), how='left', on='bi_file', suffixes=('_st', '_bi'))\n",
    "df_st_plus_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will check for any backwards references from ST not found in BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [key_name, key_full_part, bi_file, version_in_key, provenance, name, lyrics, st_file, dt_file, roud]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st_plus_bi[(df_st_plus_bi.st_file.str.split().str.get(0) != df_st_plus_bi.bi_file) & df_st_plus_bi.st_file.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT lyrics: merge with additional data from BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_file</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>roud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARDTAC</td>\n",
       "      <td>'ARD TAC</td>\n",
       "      <td>1.I'm a shearer, yes I am, and I've shorn 'em ...</td>\n",
       "      <td>[Australia, sheep, shearing, drink]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FISHFRY</td>\n",
       "      <td>(I'VE GOT) BIGGER FISH TO FRY</td>\n",
       "      <td>Sittin' on the bank of that muddy Mississippi,...</td>\n",
       "      <td>[fishing, food]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JULY12</td>\n",
       "      <td>THE 12TH OF JULY</td>\n",
       "      <td>Come pledge again your heart and your hand\\nOn...</td>\n",
       "      <td>[Irish, peace]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVENUE16</td>\n",
       "      <td>16TH AVENUE</td>\n",
       "      <td>From the corners of the country, from the citi...</td>\n",
       "      <td>[country]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASS1913</td>\n",
       "      <td>THE 1913 MASSACRE</td>\n",
       "      <td>Take a trip with me in nineteen thirteen\\nTo C...</td>\n",
       "      <td>[union, work, death, Xmas]</td>\n",
       "      <td>FSWB306A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>ZEB TOURNEY'S GIRL</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "      <td>[feud]</td>\n",
       "      <td>LE18</td>\n",
       "      <td>LE18 Full</td>\n",
       "      <td>2249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8797</th>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>ZEBRA DUN</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "      <td>[cowboy, animal]</td>\n",
       "      <td>LB16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>ZEN GOSPEL SINGING</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "      <td>[religion]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>ZULEIKA</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "      <td>[marriage, infidelity]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>THE ZULU KING</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\nFell ...</td>\n",
       "      <td>[camp]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8801 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dt_file                           name  \\\n",
       "0      HARDTAC                       'ARD TAC   \n",
       "1      FISHFRY  (I'VE GOT) BIGGER FISH TO FRY   \n",
       "2       JULY12               THE 12TH OF JULY   \n",
       "3     AVENUE16                    16TH AVENUE   \n",
       "4     MASS1913              THE 1913 MASSACRE   \n",
       "...        ...                            ...   \n",
       "8796  ZEBTURNY             ZEB TOURNEY'S GIRL   \n",
       "8797  ZEBRADUN                      ZEBRA DUN   \n",
       "8798  ZENGOSPE             ZEN GOSPEL SINGING   \n",
       "8799    ZULIKA                        ZULEIKA   \n",
       "8800  ZULUKING                  THE ZULU KING   \n",
       "\n",
       "                                                 lyrics  \\\n",
       "0     1.I'm a shearer, yes I am, and I've shorn 'em ...   \n",
       "1     Sittin' on the bank of that muddy Mississippi,...   \n",
       "2     Come pledge again your heart and your hand\\nOn...   \n",
       "3     From the corners of the country, from the citi...   \n",
       "4     Take a trip with me in nineteen thirteen\\nTo C...   \n",
       "...                                                 ...   \n",
       "8796  Down in the Tennessee mountains,\\nFar from the...   \n",
       "8797  We was camped on the plains at the head of the...   \n",
       "8798  I once was a Baptist and on each Sunday morn\\n...   \n",
       "8799  Zuleika was fair to see,\\nA fair Persian maide...   \n",
       "8800  Oh the Zulu king with the big nose-ring\\nFell ...   \n",
       "\n",
       "                                 keywords   bi_file    st_file   roud  \n",
       "0     [Australia, sheep, shearing, drink]       NaN        NaN    NaN  \n",
       "1                         [fishing, food]       NaN        NaN    NaN  \n",
       "2                          [Irish, peace]       NaN        NaN    NaN  \n",
       "3                               [country]       NaN        NaN    NaN  \n",
       "4              [union, work, death, Xmas]  FSWB306A        NaN  17663  \n",
       "...                                   ...       ...        ...    ...  \n",
       "8796                               [feud]      LE18  LE18 Full   2249  \n",
       "8797                     [cowboy, animal]      LB16        NaN   3237  \n",
       "8798                           [religion]       NaN        NaN    NaN  \n",
       "8799               [marriage, infidelity]       NaN        NaN    NaN  \n",
       "8800                               [camp]       NaN        NaN    NaN  \n",
       "\n",
       "[8801 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_plus_bi = df_dt.merge(df_file_lookup.drop_duplicates(), how='left', on='dt_file', suffixes=('_dt', '_bi'))\n",
    "df_dt_plus_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I will check for any missed backward references to DT from the above ST/BI combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_file</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>keywords</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file_x</th>\n",
       "      <th>roud_x</th>\n",
       "      <th>st_file_y</th>\n",
       "      <th>roud_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dt_file, name, lyrics, keywords, bi_file, st_file_x, roud_x, st_file_y, roud_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dt_plus_bi.merge(df_st_plus_bi[['dt_file', 'st_file', 'roud']].dropna(), how='left', on='dt_file').dropna().query('roud_x != roud_y|st_file_x != st_file_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the initial merge of all files with lyrics. The data is still inconsistent but there is more that can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>key_full_part</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>provenance</th>\n",
       "      <th>name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>st_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>A</td>\n",
       "      <td>From Percy/Wheatley, I.ii.4, pp. 186-187</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "      <td>Perc1185 Full</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>B</td>\n",
       "      <td>From Shakespeare, \"Twelfth Night\" Act IV, scen...</td>\n",
       "      <td>(No title)</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "      <td>Perc1185 Full</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>Partial text(s)</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>A</td>\n",
       "      <td>From Stokoe/Reay, Songs and Ballads of Norther...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "      <td>StoR160 Partial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>A</td>\n",
       "      <td>As found in Gale Huntington, Songs the Whaleme...</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "      <td>SWMS255 Full</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2068</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>Complete text(s)</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>A</td>\n",
       "      <td>From the Bodleian Library (Cambridge), MS. Sel...</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "      <td>MEL51 Full</td>\n",
       "      <td>AGINCRT1</td>\n",
       "      <td>V29347</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LE18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEB TOURNEY'S GIRL</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "      <td>LE18 Full</td>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>2249</td>\n",
       "      <td>[feud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LB16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEBRA DUN</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>3237</td>\n",
       "      <td>[cowboy, animal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEN GOSPEL SINGING</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULEIKA</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[marriage, infidelity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THE ZULU KING</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\nFell ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[camp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10113 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name     key_full_part   bi_file  \\\n",
       "0              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "1              A Robin, Jolly Robin  Complete text(s)  Perc1185   \n",
       "2                  A, U, Hinny Bird   Partial text(s)   StoR160   \n",
       "3      Adieu to Erin (The Emigrant)  Complete text(s)   SWMS255   \n",
       "4              Agincourt Carol, The  Complete text(s)     MEL51   \n",
       "...                             ...               ...       ...   \n",
       "10108                           NaN               NaN      LE18   \n",
       "10109                           NaN               NaN      LB16   \n",
       "10110                           NaN               NaN       NaN   \n",
       "10111                           NaN               NaN       NaN   \n",
       "10112                           NaN               NaN       NaN   \n",
       "\n",
       "      version_in_key                                         provenance  \\\n",
       "0                  A           From Percy/Wheatley, I.ii.4, pp. 186-187   \n",
       "1                  B  From Shakespeare, \"Twelfth Night\" Act IV, scen...   \n",
       "2                  A  From Stokoe/Reay, Songs and Ballads of Norther...   \n",
       "3                  A  As found in Gale Huntington, Songs the Whaleme...   \n",
       "4                  A  From the Bodleian Library (Cambridge), MS. Sel...   \n",
       "...              ...                                                ...   \n",
       "10108            NaN                                                NaN   \n",
       "10109            NaN                                                NaN   \n",
       "10110            NaN                                                NaN   \n",
       "10111            NaN                                                NaN   \n",
       "10112            NaN                                                NaN   \n",
       "\n",
       "                        name  \\\n",
       "0        A Robyn Jolly Robyn   \n",
       "1                 (No title)   \n",
       "2                        NaN   \n",
       "3              Adieu to Erin   \n",
       "4      The Song of Agincourt   \n",
       "...                      ...   \n",
       "10108     ZEB TOURNEY'S GIRL   \n",
       "10109              ZEBRA DUN   \n",
       "10110     ZEN GOSPEL SINGING   \n",
       "10111                ZULEIKA   \n",
       "10112          THE ZULU KING   \n",
       "\n",
       "                                                  lyrics          st_file  \\\n",
       "0      \"[F]rom what appears to be the most ancient of...    Perc1185 Full   \n",
       "1      71 'Hey, Robin, jolly Robin, 72    Tell me how...    Perc1185 Full   \n",
       "2      A, U, hinny burd; The bonny lass o' Benwell, A...  StoR160 Partial   \n",
       "3      Oh, when I breathed a last adieu, To Erin's an...     SWMS255 Full   \n",
       "4      Deo gracias anglia, Redde pro victoria, 1 Owre...       MEL51 Full   \n",
       "...                                                  ...              ...   \n",
       "10108  Down in the Tennessee mountains,\\nFar from the...        LE18 Full   \n",
       "10109  We was camped on the plains at the head of the...              NaN   \n",
       "10110  I once was a Baptist and on each Sunday morn\\n...              NaN   \n",
       "10111  Zuleika was fair to see,\\nA fair Persian maide...              NaN   \n",
       "10112  Oh the Zulu king with the big nose-ring\\nFell ...              NaN   \n",
       "\n",
       "        dt_file    roud                keywords  \n",
       "0      HEYROBIN     NaN                     NaN  \n",
       "1      HEYROBIN     NaN                     NaN  \n",
       "2           NaN     235                     NaN  \n",
       "3           NaN    2068                     NaN  \n",
       "4      AGINCRT1  V29347                     NaN  \n",
       "...         ...     ...                     ...  \n",
       "10108  ZEBTURNY    2249                  [feud]  \n",
       "10109  ZEBRADUN    3237        [cowboy, animal]  \n",
       "10110  ZENGOSPE     NaN              [religion]  \n",
       "10111    ZULIKA     NaN  [marriage, infidelity]  \n",
       "10112  ZULUKING     NaN                  [camp]  \n",
       "\n",
       "[10113 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics = pd.concat([df_st_plus_bi,df_dt_plus_bi], ignore_index=True)\n",
    "df_lyrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3914 lyrics have Roud numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>name</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>st_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>StoR160 Partial</td>\n",
       "      <td>235</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>A</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>SWMS255 Full</td>\n",
       "      <td>2068</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>A</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>MEL51 Full</td>\n",
       "      <td>V29347</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All Is Well</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>FlBr078</td>\n",
       "      <td>FlBr078 Partial</td>\n",
       "      <td>5455</td>\n",
       "      <td>Oh, what is this that steals upon my frame? Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All Night Long (I)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>San448</td>\n",
       "      <td>San448 Full</td>\n",
       "      <td>6703</td>\n",
       "      <td>Paul and Silas, bound in jail, All night long....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10095</th>\n",
       "      <td>NaN</td>\n",
       "      <td>YOUNG REDIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>Young Redin's til the hunting gane\\nWi' therty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>NaN</td>\n",
       "      <td>YOUNG SAILOR CUT DOWN IN HIS PRIME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LoF201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>One day as I strolled down by the Royal Albion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ZACK, THE MORMON ENGINEER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRaF444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4761</td>\n",
       "      <td>Old Zack, he came to Utah, way back in seventy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEB TOURNEY'S GIRL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LE18</td>\n",
       "      <td>LE18 Full</td>\n",
       "      <td>2249</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ZEBRA DUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LB16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3237</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3913 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name                                name  \\\n",
       "2                  A, U, Hinny Bird                                 NaN   \n",
       "3      Adieu to Erin (The Emigrant)                       Adieu to Erin   \n",
       "4              Agincourt Carol, The               The Song of Agincourt   \n",
       "5                       All Is Well                                 NaN   \n",
       "6                All Night Long (I)                                 NaN   \n",
       "...                             ...                                 ...   \n",
       "10095                           NaN                         YOUNG REDIN   \n",
       "10097                           NaN  YOUNG SAILOR CUT DOWN IN HIS PRIME   \n",
       "10106                           NaN           ZACK, THE MORMON ENGINEER   \n",
       "10108                           NaN                  ZEB TOURNEY'S GIRL   \n",
       "10109                           NaN                           ZEBRA DUN   \n",
       "\n",
       "      version_in_key  bi_file          st_file    roud  \\\n",
       "2                  A  StoR160  StoR160 Partial     235   \n",
       "3                  A  SWMS255     SWMS255 Full    2068   \n",
       "4                  A    MEL51       MEL51 Full  V29347   \n",
       "5                  A  FlBr078  FlBr078 Partial    5455   \n",
       "6                  A   San448      San448 Full    6703   \n",
       "...              ...      ...              ...     ...   \n",
       "10095            NaN     C068              NaN      47   \n",
       "10097            NaN   LoF201              NaN       2   \n",
       "10106            NaN  BRaF444              NaN    4761   \n",
       "10108            NaN     LE18        LE18 Full    2249   \n",
       "10109            NaN     LB16              NaN    3237   \n",
       "\n",
       "                                                  lyrics  \n",
       "2      A, U, hinny burd; The bonny lass o' Benwell, A...  \n",
       "3      Oh, when I breathed a last adieu, To Erin's an...  \n",
       "4      Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "5      Oh, what is this that steals upon my frame? Is...  \n",
       "6      Paul and Silas, bound in jail, All night long....  \n",
       "...                                                  ...  \n",
       "10095  Young Redin's til the hunting gane\\nWi' therty...  \n",
       "10097  One day as I strolled down by the Royal Albion...  \n",
       "10106  Old Zack, he came to Utah, way back in seventy...  \n",
       "10108  Down in the Tennessee mountains,\\nFar from the...  \n",
       "10109  We was camped on the plains at the head of the...  \n",
       "\n",
       "[3913 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics[['key_name', 'name', 'version_in_key', 'bi_file', 'st_file', 'roud', 'lyrics']].query('roud.notna()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inherit 'name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all songs have both a 'key_name' and a 'name', so to standardise the data I will inherit missing 'name' data from 'key_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics['name'] = df_lyrics['name'].fillna(df_lyrics['key_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify 'name' case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song names from DT are in all capitals, so I also unify the 'name' case using a special [`titlecase` module](https://pypi.org/project/titlecase/). Rather than the simple initial capitals provided by the inbuilt `title` string method, `titlecase` attempts a smart transformation into title case.\n",
    "\n",
    "#### Examples: Comparing `title` and `titlecase`:\n",
    "The main problem with `title` was the mishandling of a letter after an apostrophe (**0, 2-6**), followed by the stylistically-dubious capitalisation of all \"small words\" (**1, 4**). `titlecase` also has further benefits for other special string sequences, for example proper names beginning \"Mc\" (**2**).\n",
    "\n",
    "*Note: a small tweak to the source code was needed to properly handle \"O'...\" names (**5**), and I added a regex for Roman numerals (**6**) as acronyms. It's also possible to speficy a `wordlist` file for other acronyms but I've not done this (**7**).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example name</th>\n",
       "      <th>Using title Method</th>\n",
       "      <th>Using titlecase Module</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AULD MAN'S MARE'S DEID</td>\n",
       "      <td>Auld Man'S Mare'S Deid</td>\n",
       "      <td>Auld Man's Mare's Deid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALL THROUGH THE ALE</td>\n",
       "      <td>All Through The Ale</td>\n",
       "      <td>All Through the Ale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCKINLEY'S RAG</td>\n",
       "      <td>Mckinley'S Rag</td>\n",
       "      <td>McKinley's Rag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MACNAMARA'S BAND</td>\n",
       "      <td>Macnamara'S Band</td>\n",
       "      <td>Macnamara's Band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WHA'LL BE KING BUT CHARLIE?</td>\n",
       "      <td>Wha'Ll Be King But Charlie?</td>\n",
       "      <td>Wha'll Be King but Charlie?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O'DOOLEY'S FIRST FIVE O'CLOCK TEA</td>\n",
       "      <td>O'Dooley'S First Five O'Clock Tea</td>\n",
       "      <td>O'Dooley's First Five O'Clock Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KEEL ROW III</td>\n",
       "      <td>Keel Row Iii</td>\n",
       "      <td>Keel Row III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ALL AROUND MY HEART (IRA)</td>\n",
       "      <td>All Around My Heart (Ira)</td>\n",
       "      <td>All Around My Heart (Ira)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Example name                 Using title Method  \\\n",
       "0             AULD MAN'S MARE'S DEID             Auld Man'S Mare'S Deid   \n",
       "1                ALL THROUGH THE ALE                All Through The Ale   \n",
       "2                     MCKINLEY'S RAG                     Mckinley'S Rag   \n",
       "3                   MACNAMARA'S BAND                   Macnamara'S Band   \n",
       "4        WHA'LL BE KING BUT CHARLIE?        Wha'Ll Be King But Charlie?   \n",
       "5  O'DOOLEY'S FIRST FIVE O'CLOCK TEA  O'Dooley'S First Five O'Clock Tea   \n",
       "6                       KEEL ROW III                       Keel Row Iii   \n",
       "7          ALL AROUND MY HEART (IRA)          All Around My Heart (Ira)   \n",
       "\n",
       "              Using titlecase Module  \n",
       "0             Auld Man's Mare's Deid  \n",
       "1                All Through the Ale  \n",
       "2                     McKinley's Rag  \n",
       "3                   Macnamara's Band  \n",
       "4        Wha'll Be King but Charlie?  \n",
       "5  O'Dooley's First Five O'Clock Tea  \n",
       "6                       Keel Row III  \n",
       "7          All Around My Heart (Ira)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from titlecase import titlecase\n",
    "# note: edit required to titlecase.__init__.py:\n",
    "#             if all_caps:\n",
    "#                 if UC_INITIALS.match(word):\n",
    "#                     tc_line.append(word)\n",
    "#                     continue\n",
    "#                 word = word.lower() #moved from below to fix o' names in all caps\n",
    "# also added (IX|IV|V?I{0,3}) regex for low roman numerals\n",
    "\n",
    "test_titles = [\"AULD MAN'S MARE'S DEID\", \"ALL THROUGH THE ALE\", \"MCKINLEY'S RAG\", \"MACNAMARA'S BAND\", \"WHA'LL BE KING BUT CHARLIE?\",\"O'DOOLEY'S FIRST FIVE O'CLOCK TEA\",\"KEEL ROW III\",\n",
    "    \"ALL AROUND MY HEART (IRA)\"]\n",
    "df_title_test = pd.DataFrame({\n",
    "    \"Example name\": test_titles,\n",
    "    \"Using title Method\": [s.title() for s in test_titles],\n",
    "    \"Using titlecase Module\": [titlecase(s) for s in test_titles]\n",
    "})\n",
    "df_title_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "def make_titlecase(value):\n",
    "    if pd.notna(value):  # if value is not NaN (it breaks the function)\n",
    "        return titlecase(value)\n",
    "    else:\n",
    "        return value  # keep any NaNs as they are\n",
    "\n",
    "df_lyrics = df_lyrics[['key_name', 'name', 'version_in_key',  'bi_file','dt_file', 'roud', 'lyrics']].copy()\n",
    "df_lyrics['name'] = df_lyrics['name'].apply(make_titlecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_name</th>\n",
       "      <th>name</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>A</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>(No Title)</td>\n",
       "      <td>B</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>A</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>A</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2068</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>A</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>AGINCRT1</td>\n",
       "      <td>V29347</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zeb Tourney's Girl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LE18</td>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>2249</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zebra Dun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LB16</td>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>3237</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zen Gospel Singing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuleika</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Zulu King</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\nFell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10113 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key_name                   name version_in_key  \\\n",
       "0              A Robin, Jolly Robin    A Robyn Jolly Robyn              A   \n",
       "1              A Robin, Jolly Robin             (No Title)              B   \n",
       "2                  A, U, Hinny Bird       A, U, Hinny Bird              A   \n",
       "3      Adieu to Erin (The Emigrant)          Adieu to Erin              A   \n",
       "4              Agincourt Carol, The  The Song of Agincourt              A   \n",
       "...                             ...                    ...            ...   \n",
       "10108                           NaN     Zeb Tourney's Girl            NaN   \n",
       "10109                           NaN              Zebra Dun            NaN   \n",
       "10110                           NaN     Zen Gospel Singing            NaN   \n",
       "10111                           NaN                Zuleika            NaN   \n",
       "10112                           NaN          The Zulu King            NaN   \n",
       "\n",
       "        bi_file   dt_file    roud  \\\n",
       "0      Perc1185  HEYROBIN     NaN   \n",
       "1      Perc1185  HEYROBIN     NaN   \n",
       "2       StoR160       NaN     235   \n",
       "3       SWMS255       NaN    2068   \n",
       "4         MEL51  AGINCRT1  V29347   \n",
       "...         ...       ...     ...   \n",
       "10108      LE18  ZEBTURNY    2249   \n",
       "10109      LB16  ZEBRADUN    3237   \n",
       "10110       NaN  ZENGOSPE     NaN   \n",
       "10111       NaN    ZULIKA     NaN   \n",
       "10112       NaN  ZULUKING     NaN   \n",
       "\n",
       "                                                  lyrics  \n",
       "0      \"[F]rom what appears to be the most ancient of...  \n",
       "1      71 'Hey, Robin, jolly Robin, 72    Tell me how...  \n",
       "2      A, U, hinny burd; The bonny lass o' Benwell, A...  \n",
       "3      Oh, when I breathed a last adieu, To Erin's an...  \n",
       "4      Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "...                                                  ...  \n",
       "10108  Down in the Tennessee mountains,\\nFar from the...  \n",
       "10109  We was camped on the plains at the head of the...  \n",
       "10110  I once was a Baptist and on each Sunday morn\\n...  \n",
       "10111  Zuleika was fair to see,\\nA fair Persian maide...  \n",
       "10112  Oh the Zulu king with the big nose-ring\\nFell ...  \n",
       "\n",
       "[10113 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inherit Roud number from other versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Roud index groups similar songs, we can safely inherit Roud numbers from songs with the same `dt_file` (matched minus single trailing digit because of variations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics2 = df_lyrics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_file BAYOUSAR was assigned roud 10010 and 4139\n",
      "dt_file COWDENK2 was assigned roud 8209\n",
      "dt_file KATEHRN3 was assigned roud 555\n",
      "dt_file SOLDMAI2 was assigned roud 226\n",
      "dt_file ABDULBL3 was assigned roud 4321\n",
      "dt_file AGINCRT2 was assigned roud V29347\n",
      "dt_file GRTCRAZY was assigned roud 15691\n",
      "dt_file GOODNEW2 was assigned roud 11891\n",
      "dt_file STARVDT2 was assigned roud 799\n",
      "dt_file RONDHAT2 was assigned roud 803 plus 3729, 1034\n",
      "dt_file RONDHAT3 was assigned roud 803 plus 3729, 1034\n",
      "dt_file DOWNOUTB was assigned roud 18521\n",
      "dt_file RONDHAT4 was assigned roud 803 plus 3729, 1034\n",
      "dt_file HEREGRG2 was assigned roud 475\n",
      "dt_file SVNVIRG2 was assigned roud 127\n",
      "dt_file AMAZGRA3 was assigned roud 5430\n",
      "dt_file UNFORTU3 was assigned roud 4859\n",
      "dt_file AMPHITR2 was assigned roud 301\n",
      "dt_file HENRMRT2 was assigned roud 104\n",
      "dt_file ANGLBAND was assigned roud 4268\n",
      "dt_file AULDLNG5 was assigned roud 13892\n",
      "dt_file AULDLNG3 was assigned roud 13892\n",
      "dt_file AULDLNG4 was assigned roud 13892\n",
      "dt_file DEAFWOM2 was assigned roud 467\n",
      "dt_file AVONDALE was assigned roud 3250\n",
      "dt_file BABWOOD4 was assigned roud 288\n",
      "dt_file BABWOOD5 was assigned roud 288\n",
      "dt_file LAREDS11 was assigned roud 2\n",
      "dt_file LAREDST6 was assigned roud 2\n",
      "dt_file ALANWATR was assigned roud 4260\n",
      "dt_file GREWILL2 was assigned roud 172\n",
      "dt_file SWTDUND2 was assigned roud 148\n",
      "dt_file BNKSBAN3 was assigned roud 889\n",
      "dt_file BNKSDEE was assigned roud 3847\n",
      "dt_file BNKSDEE2 was assigned roud 3847\n",
      "dt_file BNKSLEE was assigned roud 6857\n",
      "dt_file BRNBRKL3 was assigned roud 4017\n",
      "dt_file HGHTALM2 was assigned roud 830\n",
      "dt_file BAYOUSAR was assigned roud 10010 and 4139\n",
      "dt_file UNFORTU5 was assigned roud 4859\n",
      "dt_file ABEGGIN2 was assigned roud 286\n",
      "dt_file BOGIEBL2 was assigned roud 2155\n",
      "dt_file BESSBANK was assigned roud 566\n",
      "dt_file BETSYBEL was assigned roud 5211\n",
      "dt_file BIGROCK was assigned roud 6696\n",
      "dt_file GYPLAD6 was assigned roud 1\n",
      "dt_file ROLLCTT3 was assigned roud 2627\n",
      "dt_file BLKVEL3 was assigned roud 2146 and 3764\n",
      "dt_file BLKWTR2 was assigned roud 564\n",
      "dt_file BLRNSTO2 was assigned roud 4800\n",
      "dt_file BLINDFI2 was assigned roud 7833\n",
      "dt_file BLOWYE3 was assigned roud 2012\n",
      "dt_file BLOWYE2 was assigned roud 2012\n",
      "dt_file BLUEYES was assigned roud 4308 and 18831\n",
      "dt_file BLUEVEL3 was assigned roud 2146 and 3764\n",
      "dt_file BOLDARC3 was assigned roud 83\n",
      "dt_file VANTYGL8 was assigned roud 122\n",
      "dt_file NEVSAYNO was assigned roud 2903\n",
      "dt_file BLACKHR2 was assigned roud 1656\n",
      "dt_file DAILYGR2 was assigned roud 31\n",
      "dt_file GLENSHE was assigned roud 292\n",
      "dt_file BOTBAY3 was assigned roud 3267\n",
      "dt_file BALQUID2 was assigned roud 541\n",
      "dt_file LAREDS13 was assigned roud 2\n",
      "dt_file BONLGHT2 was assigned roud 1185\n",
      "dt_file BRKLYNST was assigned roud 3258\n",
      "dt_file COWDENK2 was assigned roud 8209\n",
      "dt_file BRNGIRL2 was assigned roud 180\n",
      "dt_file MILLDEE4 was assigned roud 310\n",
      "dt_file BULLYTW2 was assigned roud 4182\n",
      "dt_file BROOMBES was assigned roud 1623\n",
      "dt_file BYHUSH was assigned roud 2314\n",
      "dt_file BYKERHIL was assigned roud 3488\n",
      "dt_file EASYRID2 was assigned roud 10056\n",
      "dt_file CALEWES was assigned roud 857\n",
      "dt_file CALEWE2 was assigned roud 857\n",
      "dt_file SOUNDOF3 was assigned roud 10398\n",
      "dt_file CAMFRAN2 was assigned roud 5814\n",
      "dt_file CNAANLND was assigned roud 5722\n",
      "dt_file CHARLTT2 was assigned roud 4839\n",
      "dt_file CASEJON2 was assigned roud 3247\n",
      "dt_file CHERTRE3 was assigned roud 453\n",
      "dt_file CHERTRE2 was assigned roud 453\n",
      "dt_file CHISHLM2 was assigned roud 3438\n",
      "dt_file CLAUDALL was assigned roud 2245\n",
      "dt_file CLEMENT2 was assigned roud 9611\n",
      "dt_file CLERKSA was assigned roud 3855\n",
      "dt_file CLYDWAT2 was assigned roud 91\n",
      "dt_file CLYDWAT3 was assigned roud 91\n",
      "dt_file CLYDWAT2 was assigned roud 91\n",
      "dt_file CSTPERU2 was assigned roud 1997\n",
      "dt_file LILSADI2 was assigned roud 780\n",
      "dt_file CODLIVR2 was assigned roud 4221\n",
      "dt_file COLDRAW was assigned roud 135\n",
      "dt_file LOWHOLL9 was assigned roud 484\n",
      "dt_file COLUMBIA was assigned roud 4843\n",
      "dt_file COMWRIT2 was assigned roud 381\n",
      "dt_file COMTHRY2 was assigned roud 5512\n",
      "dt_file AMAZGRA4 was assigned roud 5430\n",
      "dt_file COTTNFLD was assigned roud 11662\n",
      "dt_file SWTJOAN3 was assigned roud 592\n",
      "dt_file CROCKRY2 was assigned roud 1490\n",
      "dt_file CROPPIE1 was assigned roud 1030\n",
      "dt_file CRUELMO4 was assigned roud 263\n",
      "dt_file ANDRROS3 was assigned roud 623\n",
      "dt_file CALABAR2 was assigned roud 1079\n",
      "dt_file CURRKILD was assigned roud 583\n",
      "dt_file CUTYWRE2 was assigned roud 236\n",
      "dt_file JOHNPEL2 was assigned roud 1239\n",
      "dt_file DAILYGR6 was assigned roud 31\n",
      "dt_file DNTDAVE was assigned roud 2387\n",
      "dt_file FYVIOLS3 was assigned roud 545\n",
      "dt_file SUNSCHOL2 was assigned roud 766\n",
      "dt_file JNGLBLL3 was assigned roud 25804\n",
      "dt_file DAWNDAY2 was assigned roud 370\n",
      "dt_file DELIAGO4 was assigned roud 3264\n",
      "dt_file DELIAGO5 was assigned roud 3264\n",
      "dt_file DELIAGO6 was assigned roud 3264\n",
      "dt_file DERBYRM6 was assigned roud 126\n",
      "dt_file DEVLWIF6 was assigned roud 160\n",
      "dt_file DEVLWIDW was assigned roud 160\n",
      "dt_file DEVLMAR2 was assigned roud 1017\n",
      "dt_file DIAMONJ2 was assigned roud 3585\n",
      "dt_file THOLDMN2 was assigned roud 3550\n",
      "dt_file DOMISS2 was assigned roud 4366\n",
      "dt_file DONTSEL2 was assigned roud 7796\n",
      "dt_file HTHRMOR2 was assigned roud 375\n",
      "dt_file RIVTEX2 was assigned roud 4764\n",
      "dt_file CLEMENT5 was assigned roud 9611\n",
      "dt_file CANEBREK was assigned roud 10063\n",
      "dt_file DRUNKDR2 was assigned roud 722\n",
      "dt_file DUMYLINE was assigned roud 15359\n",
      "dt_file DUNCBRDY was assigned roud 4177\n",
      "dt_file LAREDST2 was assigned roud 2\n",
      "dt_file DOUGTRD3 was assigned roud 321\n",
      "dt_file EARLY1A2 was assigned roud 12682\n",
      "dt_file EATWORMS was assigned roud 12764\n",
      "dt_file JLSLOVR3 was assigned roud 500\n",
      "dt_file BASKETEG) was assigned roud 377\n",
      "dt_file MARBON4 was assigned roud 183\n",
      "dt_file PLNWLOO6 was assigned roud 1922\n",
      "dt_file ELFKNGT was assigned roud 21\n",
      "dt_file AUTUMN was assigned roud 1706\n",
      "dt_file COMRND3 was assigned roud 7052\n",
      "dt_file FACTRSG2 was assigned roud 572\n",
      "dt_file JLSLOVR4 was assigned roud 500\n",
      "dt_file FOXOUT3 was assigned roud 131\n",
      "dt_file FALSKNT3 was assigned roud 20\n",
      "dt_file FALSKNT4 was assigned roud 20\n",
      "dt_file BOLAMKN4 was assigned roud 6\n",
      "dt_file BONLOVE2 was assigned roud 201\n",
      "dt_file FLSESIR2 was assigned roud 21\n",
      "dt_file PIGINEB4 was assigned roud 7322\n",
      "dt_file FAREWELL was assigned roud 803 plus 3729, 1034\n",
      "dt_file FAREWELS was assigned roud 803 plus 3729, 1034\n",
      "dt_file FARWELSY was assigned roud 384\n",
      "dt_file TARWATH2 was assigned roud 2562\n",
      "dt_file FRMRDELL was assigned roud 6306\n",
      "dt_file SOLDMAI2 was assigned roud 226\n",
      "dt_file FINNWATR was assigned roud 1009\n",
      "dt_file FINNWAK2 was assigned roud 1009\n",
      "dt_file FIREBEL2 was assigned roud 813\n",
      "dt_file FIRELOVE was assigned roud 1780\n",
      "dt_file GOODMAN was assigned roud 114\n",
      "dt_file GOODMAN4 was assigned roud 114\n",
      "dt_file GOLDWED2 was assigned roud 5491\n",
      "dt_file FOGGDEW6 was assigned roud 558\n",
      "dt_file FOGGDEW6 was assigned roud 558\n",
      "dt_file FTPRINTS was assigned roud 2660\n",
      "dt_file FORSAKLV was assigned roud 466\n",
      "dt_file FOURLOOM was assigned roud 937\n",
      "dt_file FOURSTWL was assigned roud 36099\n",
      "dt_file FOXOUT2 was assigned roud 131\n",
      "dt_file FOXOUT4 was assigned roud 131\n",
      "dt_file FOXOUT5 was assigned roud 131\n",
      "dt_file FRGCORT4 was assigned roud 16\n",
      "dt_file FRGCORT5 was assigned roud 16\n",
      "dt_file GAMBLR was assigned roud 3416\n",
      "dt_file GAMBLR3 was assigned roud 3416\n",
      "dt_file DARLCOR2 was assigned roud 5723\n",
      "dt_file KERIMUR2 was assigned roud 4828\n",
      "dt_file GENTLAN2 was assigned roud 2656\n",
      "dt_file GEORDI3 was assigned roud 90\n",
      "dt_file GEORDI5 was assigned roud 90\n",
      "dt_file GILMORE was assigned roud 53\n",
      "dt_file GINNYGON was assigned roud 481\n",
      "dt_file GIRLLFT3 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file GIRLLF11 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file GIRLLFT2 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file GIRLLFT8 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file GIRLLF12 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file LAREDS14 was assigned roud 2\n",
      "dt_file GLASGPG2 was assigned roud 95\n",
      "dt_file GODREST2 was assigned roud 394\n",
      "dt_file HEARTDIX was assigned roud 18324\n",
      "dt_file DOWNRIVE was assigned roud 7677\n",
      "dt_file VANTYGL3 was assigned roud 122\n",
      "dt_file VANTYGL6 was assigned roud 122\n",
      "dt_file GOLDWEDD was assigned roud 5491\n",
      "dt_file GOODBOY2 was assigned roud 13612\n",
      "dt_file IRENGDN2 was assigned roud 11681\n",
      "dt_file ADAMEV2 was assigned roud V37609\n",
      "dt_file IRENGDN3 was assigned roud 11681\n",
      "dt_file GRNWALE.NOT was assigned roud 2817 and 15026\n",
      "dt_file GRTGRNDD was assigned roud 4543\n",
      "dt_file GRNBROM3 was assigned roud 379\n",
      "dt_file GRNFLDA2 was assigned roud 2290\n",
      "dt_file GRENGRAS was assigned roud 279\n",
      "dt_file GRENGREN was assigned roud 279\n",
      "dt_file GRRASH2 was assigned roud 2772\n",
      "dt_file GRNRUSH4 was assigned roud 133\n",
      "dt_file GREENLDY was assigned roud 347\n",
      "dt_file GRNSLVS2 was assigned roud V19581\n",
      "dt_file VANTYGL2 was assigned roud 122\n",
      "dt_file OLDSHOE2 was assigned roud 362\n",
      "dt_file GREYCOC2 was assigned roud 179\n",
      "dt_file GRNRUSH3 was assigned roud 133\n",
      "dt_file GYPLADD4 was assigned roud 1\n",
      "dt_file NCNTRYM3 was assigned roud 1367\n",
      "dt_file HRDTIME2 was assigned roud 2659\n",
      "dt_file OVRHILL7 was assigned roud 8460\n",
      "dt_file FAIRFLR3 was assigned roud 25\n",
      "dt_file HEXHMLS2 was assigned roud 3182\n",
      "dt_file GOODLOKN was assigned roud 3340\n",
      "dt_file GLASGPG3 was assigned roud 95\n",
      "dt_file HIELND2 was assigned roud 4691\n",
      "dt_file HITLERB2 was assigned roud 10493\n",
      "dt_file SUFFMRC4 was assigned roud 246\n",
      "dt_file BITWITH2 was assigned roud 452\n",
      "dt_file HOMESTEA was assigned roud 7744\n",
      "dt_file HOUSCARN was assigned roud 14\n",
      "dt_file HUSHLIL2 was assigned roud 470\n",
      "dt_file HUSHLIL2 was assigned roud 470\n",
      "dt_file IMAROVR2 was assigned roud 3135\n",
      "dt_file FLSEBRD9 was assigned roud 154\n",
      "dt_file KNOWHER2 was assigned roud 1645 and 5701\n",
      "dt_file SHEEPSNG was assigned roud 879\n",
      "dt_file TAVTOWN2 was assigned roud 60\n",
      "dt_file LAREDS17 was assigned roud 2\n",
      "dt_file JUSTFAC2 was assigned roud 3127\n",
      "dt_file LUMBERJK was assigned roud 591 and 7088\n",
      "dt_file MOONSHI3 was assigned roud 414\n",
      "dt_file IMAROVER was assigned roud 3135\n",
      "dt_file IRSHLAB2 was assigned roud 1137\n",
      "dt_file LADAMER was assigned roud 18316\n",
      "dt_file RYEWHIS2 was assigned roud 941\n",
      "dt_file INPINE2 was assigned roud 3421\n",
      "dt_file INTOAIR was assigned roud 15440\n",
      "dt_file BIGROCK4 was assigned roud 6696\n",
      "dt_file CURRKIL2 was assigned roud 583\n",
      "dt_file SIRHUGH4 was assigned roud 73\n",
      "dt_file SIRHUGH5 was assigned roud 73\n",
      "dt_file TIPRARY was assigned roud 11235\n",
      "dt_file SYMEOVR2 was assigned roud 9621\n",
      "dt_file SYMEOVR3 was assigned roud 9621\n",
      "dt_file FLATRVR2 was assigned roud 642\n",
      "dt_file FLATRVR3 was assigned roud 642\n",
      "dt_file GLENKIN2 was assigned roud 145\n",
      "dt_file JACKROWL was assigned roud 268\n",
      "dt_file JACOBLAD was assigned roud 2286\n",
      "dt_file JMCONNL2 was assigned roud 12495\n",
      "dt_file JAMFOYE3 was assigned roud 1941\n",
      "dt_file KATEHRN3 was assigned roud 555\n",
      "dt_file JESSJAM1 was assigned roud 2240\n",
      "dt_file BLUETAI2 was assigned roud 1274\n",
      "dt_file JNGLBLAU was assigned roud 25804\n",
      "dt_file JNGLBLL2 was assigned roud 25804\n",
      "dt_file JOHNAND1 was assigned roud 16967\n",
      "dt_file JOHNAND5 was assigned roud 16967\n",
      "dt_file JBARLEY2 was assigned roud 164\n",
      "dt_file JOHNAND6 was assigned roud 16967\n",
      "dt_file JOHNAND7 was assigned roud 16967\n",
      "dt_file JOHNHIEL was assigned roud 650\n",
      "dt_file DUNDER2 was assigned roud 4461\n",
      "dt_file MARBONE6 was assigned roud 183\n",
      "dt_file JOLLPLO2 was assigned roud 186\n",
      "dt_file KAFOOZL2 was assigned roud 10135\n",
      "dt_file PEGGORD2 was assigned roud 2280\n",
      "dt_file KATYCRU2 was assigned roud 1645 and 5701\n",
      "dt_file KEELROW3 was assigned roud 3059\n",
      "dt_file PRDMARG2 was assigned roud 37\n",
      "dt_file SPRINGH3 was assigned roud 2713\n",
      "dt_file LADYFRN4 was assigned roud 487\n",
      "dt_file GRNSLVS3 was assigned roud V19581\n",
      "dt_file SILKIE3 was assigned roud 197\n",
      "dt_file LAIDLEY2 was assigned roud 3968\n",
      "dt_file ELFKNGT2 was assigned roud 21\n",
      "dt_file JOHNAND2 was assigned roud 16967\n",
      "dt_file CURRKIL3 was assigned roud 583\n",
      "dt_file LNDLDYDT was assigned roud V33309\n",
      "dt_file LASTROSE was assigned roud 13861\n",
      "dt_file LAVNDER3 was assigned roud 3483\n",
      "dt_file FRANJON3 was assigned roud 254\n",
      "dt_file LEAVLIV2 was assigned roud 9435\n",
      "dt_file AENICHT was assigned roud 135\n",
      "dt_file PATGAME2 was assigned roud 18464\n",
      "dt_file HOLEBCK2 was assigned roud 17845\n",
      "dt_file LIFERAIL was assigned roud 13933\n",
      "dt_file BRWNEYED was assigned roud 17030\n",
      "dt_file LYDIAPN2 was assigned roud 8368\n",
      "dt_file LYDIAPN3 was assigned roud 8368\n",
      "dt_file LIMERAKE was assigned roud 3018\n",
      "dt_file LAREDS18 was assigned roud 2\n",
      "dt_file LTLBLSS2 was assigned roud 7788\n",
      "dt_file LILMOHE1 was assigned roud 275\n",
      "dt_file LITMOSE2 was assigned roud 3546\n",
      "dt_file LITTLEPD was assigned roud 1930\n",
      "dt_file LTTLSTCH was assigned roud 1937\n",
      "dt_file KEACHCR3 was assigned roud 120\n",
      "dt_file ASHGROV4 was assigned roud 24988\n",
      "dt_file ASHGROV3 was assigned roud 24988\n",
      "dt_file LOCHLMD3 was assigned roud 9598\n",
      "dt_file LAREDST3 was assigned roud 2\n",
      "dt_file TURTDOV3 was assigned roud 49\n",
      "dt_file LNGTRAIL was assigned roud 23525\n",
      "dt_file LONGTIME was assigned roud 5732\n",
      "dt_file LORDBAT5 was assigned roud 40\n",
      "dt_file LRDBEIC2 was assigned roud 40\n",
      "dt_file LORDGRG3 was assigned roud 49\n",
      "dt_file BRWNGRL3 was assigned roud 4\n",
      "dt_file LVNGNAN2 was assigned roud 563\n",
      "dt_file VANTYGL4 was assigned roud 122\n",
      "dt_file VANTYGL7 was assigned roud 122\n",
      "dt_file LOWHOL10 was assigned roud 484\n",
      "dt_file BROKEBN2 was assigned roud 24846\n",
      "dt_file CALABAR3 was assigned roud 1079\n",
      "dt_file MARRYNO was assigned roud 1403\n",
      "dt_file MARTINMA was assigned roud 2173\n",
      "dt_file MARYANN2 was assigned roud 4438\n",
      "dt_file MARYLAM3 was assigned roud 7622\n",
      "dt_file MARYLAMB was assigned roud 7622\n",
      "dt_file MARTINDL was assigned roud 2173\n",
      "dt_file MARYSOMR was assigned roud 2496\n",
      "dt_file MARYLAND was assigned roud 7622\n",
      "dt_file MATTHYL was assigned roud 2880\n",
      "dt_file MATTHYL2 was assigned roud 2880\n",
      "dt_file MATTIE was assigned roud 52\n",
      "dt_file MAYMRNHM was assigned roud 5405\n",
      "dt_file HARLCH2 was assigned roud 24790\n",
      "dt_file MERMAID4 was assigned roud 124\n",
      "dt_file MERMAID2 was assigned roud 124\n",
      "dt_file MICHAELR was assigned roud 11975\n",
      "dt_file MILWAUKE was assigned roud 3255\n",
      "dt_file MRSHDRK2 was assigned roud 9753\n",
      "dt_file MOLLYMA2 was assigned roud 16932\n",
      "dt_file FATALSN2 was assigned roud 175\n",
      "dt_file MOONSHIN was assigned roud 414\n",
      "dt_file MOTHR was assigned roud 16113\n",
      "dt_file BTTLOVE2 was assigned roud 5462\n",
      "dt_file MTDEW was assigned roud 938\n",
      "dt_file MTMDOW2 was assigned roud 3240\n",
      "dt_file MYBONNI2 was assigned roud 1422\n",
      "dt_file LAMECRN was assigned roud 13622\n",
      "dt_file DEARCOM2 was assigned roud 411 and 459\n",
      "dt_file GDOLDMN2 was assigned roud 240\n",
      "dt_file JOHNLAD2 was assigned roud 6131\n",
      "dt_file BUCKBRN2 was assigned roud 934\n",
      "dt_file MYSWEETH was assigned roud 4756\n",
      "dt_file GIRLLFT9 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file CALTONW2 was assigned roud 883\n",
      "dt_file MORNDEW4 was assigned roud 11\n",
      "dt_file GUNDAGRD was assigned roud 10221 AND 9121\n",
      "dt_file ONOJOHN2 was assigned roud 146\n",
      "dt_file NOAHARK was assigned roud 318\n",
      "dt_file SAMHALL3 was assigned roud 369\n",
      "dt_file NBDYKNEW was assigned roud 5438\n",
      "dt_file LAREDS10 was assigned roud 2\n",
      "dt_file REILLY2 was assigned roud 1161\n",
      "dt_file SHEARNA3 was assigned roud 4845\n",
      "dt_file OATSBEAN was assigned roud 1380\n",
      "dt_file LONEPRA2 was assigned roud 631\n",
      "dt_file OHDEATH was assigned roud 4933\n",
      "dt_file HEREGRO2 was assigned roud 475\n",
      "dt_file FENIANGN was assigned roud 4531\n",
      "dt_file GIRLLF10 was assigned roud 4497 and 7680 and 23929\n",
      "dt_file CLOKWIN2 was assigned roud 241\n",
      "dt_file KNGCOLE3 was assigned roud 1164\n",
      "dt_file LOGCABI2 was assigned roud 7376\n",
      "dt_file OLDMAID4 was assigned roud 802\n",
      "dt_file OLDSHOE3 was assigned roud 362\n",
      "dt_file OLDSMOK3 was assigned roud 414\n",
      "dt_file SOWMEAS2 was assigned roud 17759\n",
      "dt_file STEPSTONE was assigned roud 7453\n",
      "dt_file OVRHILL8 was assigned roud 8460\n",
      "dt_file OLDSMOK4 was assigned roud 414\n",
      "dt_file OLDSMOK5 was assigned roud 414\n",
      "dt_file ONEMRDY2 was assigned roud 704\n",
      "dt_file LAREDS12 was assigned roud 2\n",
      "dt_file ONCHRS2 was assigned roud V26738\n",
      "dt_file LOWHOL11 was assigned roud 484\n",
      "dt_file OVRHILL2 was assigned roud 8460\n",
      "dt_file OVRHILL3 was assigned roud 8460\n",
      "dt_file SKYEBOT2 was assigned roud 3772\n",
      "dt_file CHARLOVR was assigned roud 729\n",
      "dt_file OVRHILL6 was assigned roud 8460\n",
      "dt_file PADDO was assigned roud 4695\n",
      "dt_file PADRAIL2 was assigned roud 208\n",
      "dt_file PADRAIL4 was assigned roud 208\n",
      "dt_file PAPERPI2 was assigned roud 573\n",
      "dt_file PATSPENS2 was assigned roud 41\n",
      "dt_file PADRAIL3 was assigned roud 208\n",
      "dt_file RDDLSNG4 was assigned roud 330 and 36\n",
      "dt_file ROOTHOG4 was assigned roud 4292\n",
      "dt_file PIGINEB3 was assigned roud 7322\n",
      "dt_file LAREDST9 was assigned roud 2\n",
      "dt_file PLNWLOO5 was assigned roud 1922\n",
      "dt_file PLNWLOO4 was assigned roud 1922\n",
      "dt_file PLESDELT was assigned roud 660\n",
      "dt_file PLESDEL2 was assigned roud 660\n",
      "dt_file PLOOLAD2 was assigned roud 5138\n",
      "dt_file PLOUGHB2 was assigned roud 2538\n",
      "dt_file PLOUGHMN was assigned roud 2538\n",
      "dt_file PLOUGHM2 was assigned roud 2538\n",
      "dt_file PLOUGHM3 was assigned roud 2538\n",
      "dt_file POLLVON3 was assigned roud 166\n",
      "dt_file ELLNSMT3 was assigned roud 448\n",
      "dt_file POORLIL2 was assigned roud 10310\n",
      "dt_file DEADHOR2 was assigned roud 513\n",
      "dt_file OLDMAID3 was assigned roud 802\n",
      "dt_file JOLLHANG was assigned roud 1048\n",
      "dt_file PRETBABY was assigned roud 288\n",
      "dt_file LILBIRD was assigned roud 5742\n",
      "dt_file KEACHCR4 was assigned roud 120\n",
      "dt_file PRETSAR5 was assigned roud 417\n",
      "dt_file PRETSAR4 was assigned roud 417\n",
      "dt_file PRETSAR2 was assigned roud 417\n",
      "dt_file PRETSAR3 was assigned roud 417\n",
      "dt_file HANGMAN3 was assigned roud 896\n",
      "dt_file PUDDYWL3 was assigned roud 16\n",
      "dt_file PUDDYWEL was assigned roud 16\n",
      "dt_file PUSHBOYS was assigned roud 8088\n",
      "dt_file RAGECANO was assigned roud 735\n",
      "dt_file RRBILLKT was assigned roud 4181\n",
      "dt_file REDRIVA2 was assigned roud 756\n",
      "dt_file ELFKNGT3 was assigned roud 21\n",
      "dt_file REDRIVPL was assigned roud 756\n",
      "dt_file REYNRDFX was assigned roud 2349\n",
      "dt_file RCHMRCH2 was assigned roud 536\n",
      "dt_file RDDLSNG2 was assigned roud 330 and 36\n",
      "dt_file BONBROQ2 was assigned roud 161\n",
      "dt_file BONBROQ3 was assigned roud 161\n",
      "dt_file KEACHCR2 was assigned roud 120\n",
      "dt_file RISESHEP was assigned roud 11968\n",
      "dt_file RHPEDLRS was assigned roud 333\n",
      "dt_file ROBHDTH2 was assigned roud 3299\n",
      "dt_file ROCKBABY was assigned roud 3024\n",
      "dt_file VIRGIBN4 was assigned roud 27\n",
      "dt_file ROCKYMNT was assigned roud 277\n",
      "dt_file RMCORLY3 was assigned roud 5279\n",
      "dt_file RMCORLY2 was assigned roud 5279\n",
      "dt_file ROLLCTT2 was assigned roud 2627\n",
      "dt_file ROLLCHR2 was assigned roud 3632\n",
      "dt_file ROOTHOG5 was assigned roud 4292\n",
      "dt_file ROSEBRIR was assigned roud 1796\n",
      "dt_file ROSEBUDD was assigned roud 812\n",
      "dt_file RYEWHISx was assigned roud 941\n",
      "dt_file GOLDRIVR was assigned roud 7405\n",
      "dt_file PLESDEL3 was assigned roud 660\n",
      "dt_file SAILTAI2 was assigned roud 917\n",
      "dt_file SAILBORD was assigned roud 314\n",
      "dt_file SAMBAS2 was assigned roud 2244\n",
      "dt_file SAMHALL2 was assigned roud 369\n",
      "dt_file SNTYANN3 was assigned roud 207\n",
      "dt_file ELFKNGT4 was assigned roud 21\n",
      "dt_file STWBLHR5 was assigned roud 456\n",
      "dt_file SEVENOL2 was assigned roud 10227\n",
      "dt_file COLDRAI2 was assigned roud 135\n",
      "dt_file SHULARN5 was assigned roud 911\n",
      "dt_file UNFORTU4 was assigned roud 4859\n",
      "dt_file SILVDAG was assigned roud 22620 and 22621\n",
      "dt_file SNGLGRL4 was assigned roud 436\n",
      "dt_file TITANIC9 was assigned roud 4173\n",
      "dt_file SINERMN3 was assigned roud 3408\n",
      "dt_file STWBLHR4 was assigned roud 456\n",
      "dt_file SKYEBOT3 was assigned roud 3772\n",
      "dt_file FALSKNT5 was assigned roud 20\n",
      "dt_file SOLONGI2 was assigned roud 15161\n",
      "dt_file GRTWHEE2 was assigned roud 10237\n",
      "dt_file SOLDMARN was assigned roud 226\n",
      "dt_file SOLDBOY3 was assigned roud 1917\n",
      "dt_file COCKADE2 was assigned roud 191\n",
      "dt_file BRITGRE2 was assigned roud 11231?\n",
      "dt_file BACKWODS was assigned roud 641\n",
      "dt_file SONSLIB was assigned roud 596\n",
      "dt_file SPRINGHI was assigned roud 2713\n",
      "dt_file LAREDS16 was assigned roud 2\n",
      "dt_file STWBLHR3 was assigned roud 456\n",
      "dt_file STILILO2 was assigned roud 654\n",
      "dt_file STRFORB2 was assigned roud 20764\n",
      "dt_file LAREDS15 was assigned roud 2\n",
      "dt_file LAREDS20 was assigned roud 2\n",
      "dt_file SWEETBYE was assigned roud 3234\n",
      "dt_file VANTYGL5 was assigned roud 122\n",
      "dt_file SWTROSIE was assigned roud 9560\n",
      "dt_file SWTVILT was assigned roud 10232 and 10404\n",
      "dt_file CARCROW3 was assigned roud 891\n",
      "dt_file TARYTRO2 was assigned roud 427\n",
      "dt_file TARYTRO3 was assigned roud 427\n",
      "dt_file TEDONEIL was assigned roud 5207\n",
      "dt_file POORLOU was assigned roud 4643\n",
      "dt_file THISLAN2 was assigned roud 16378\n",
      "dt_file THREEBL2 was assigned roud 3753\n",
      "dt_file THREEBRO was assigned roud 3753\n",
      "dt_file TIMEHARD was assigned roud 16072\n",
      "dt_file TITANIC7 was assigned roud 4173\n",
      "dt_file TITANIC9 was assigned roud 4173\n",
      "dt_file TITANIC6 was assigned roud 4173\n",
      "dt_file MORROW1 was assigned roud 9554\n",
      "dt_file CANONBL2 was assigned roud 4759\n",
      "dt_file TOMMYHL2 was assigned roud 481\n",
      "dt_file GLORYPE2 was assigned roud 19921\n",
      "dt_file CLOSEWND was assigned roud 15986\n",
      "dt_file GYPLAD5 was assigned roud 1\n",
      "dt_file LAREDST7 was assigned roud 2\n",
      "dt_file JESSJAM2 was assigned roud 2240\n",
      "dt_file TURKSTR2 was assigned roud 4247\n",
      "dt_file TURNYEM2 was assigned roud 23557\n",
      "dt_file TUTRLDOV was assigned roud 49\n",
      "dt_file TURTDOV2 was assigned roud 49\n",
      "dt_file THRERAV7 was assigned roud 747?\n",
      "dt_file THRERAV7 was assigned roud 747?\n",
      "dt_file TWOSIS6 was assigned roud 8\n",
      "dt_file TWOBROCW was assigned roud 38\n",
      "dt_file TWOSIS12 was assigned roud 8\n",
      "dt_file TWOSIS13 was assigned roud 8\n",
      "dt_file TWOSIS7 was assigned roud 8\n",
      "dt_file UNDRAPRN was assigned roud 899\n",
      "dt_file LAREDST5 was assigned roud 2\n",
      "dt_file DIXIELN2 was assigned roud 8231\n",
      "dt_file GOTOSEA2 was assigned roud 644\n",
      "dt_file CRUELMO5 was assigned roud 263\n",
      "dt_file PLNWLOO4 was assigned roud 1922\n",
      "dt_file LAREDST4 was assigned roud 2\n",
      "dt_file SWTJOAN2 was assigned roud 592\n",
      "dt_file COCKADE3 was assigned roud 191\n",
      "dt_file DOITNOW2 was assigned roud 1401\n",
      "dt_file LAREDS19 was assigned roud 2\n",
      "dt_file BARNBINN was assigned roud 4704\n",
      "dt_file DEVLWIF5 was assigned roud 160\n",
      "dt_file REYNFOX2 was assigned roud 1868 and 190\n",
      "dt_file JACBITE2 was assigned roud 5517\n",
      "dt_file SALGARD3 was assigned roud 3819\n",
      "dt_file DAILYGR4 was assigned roud 31\n",
      "dt_file DAILYGR5 was assigned roud 31\n",
      "dt_file LAREDST8 was assigned roud 2\n",
      "dt_file WRCK1262 was assigned roud 7128\n",
      "dt_file WILLIWI4 was assigned roud 64\n",
      "dt_file YARROW4 was assigned roud 13\n",
      "dt_file XMASGOO2 was assigned roud 167\n",
      "dt_file WASSCORN was assigned roud 209\n",
      "dt_file YARROW5 was assigned roud 13\n",
      "dt_file WHENOVR2 was assigned roud 3446\n",
      "dt_file WALLABBY was assigned roud 7483\n",
      "dt_file WILDMTH2 was assigned roud 541\n",
      "dt_file WILDBIKE was assigned roud 2246\n",
      "dt_file WARGRMN3 was assigned roud 904\n",
      "dt_file WARLIKES was assigned roud 690\n",
      "dt_file WE3KING2 was assigned roud 24751\n",
      "dt_file WEARGRE2 was assigned roud 3278\n",
      "dt_file WHAKING was assigned roud 729\n",
      "dt_file WHNCORT was assigned roud 4275 and 2977\n",
      "dt_file WHCHSID2 was assigned roud 15159\n",
      "dt_file WHITFIS2 was assigned roud 3888\n",
      "dt_file WIDWSTMO was assigned roud 228\n",
      "dt_file WILDBIL2 was assigned roud 2246\n",
      "dt_file WILILAD2 was assigned roud 220\n",
      "dt_file WINNIP2 was assigned roud 8348\n",
      "dt_file WINTERI was assigned roud 1942\n",
      "dt_file WINTER was assigned roud 1942\n",
      "dt_file WRECK972 was assigned roud 777\n",
      "dt_file YLLOWTX3 was assigned roud 10405\n",
      "dt_file YLLOWTX2 was assigned roud 10405\n",
      "dt_file YNGPEG2 was assigned roud 3875\n"
     ]
    }
   ],
   "source": [
    "#match and assign roud on (partial) dt_file \n",
    "#(chatgpt collaboration)\n",
    "def match_rows_and_update(df):\n",
    "    updated_rows = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notnull(row['roud']):\n",
    "            continue  # Skip rows with valid `roud`\n",
    "\n",
    "        dt_file = row['dt_file']\n",
    "        if pd.notnull(dt_file):\n",
    "            dt_file = str(dt_file)\n",
    "\n",
    "            # Prepare dt_file for matching based on the rules\n",
    "            if dt_file[-1].isdigit() and dt_file[-2].isalpha() and not dt_file[-2].isdigit():\n",
    "                dt_file = dt_file[:6]\n",
    "            else:\n",
    "                dt_file = dt_file.strip()\n",
    "\n",
    "            # Find potential matches in the DataFrame\n",
    "            potential_matches = df[(df['dt_file'].apply(lambda x: pd.notnull(x) and str(x).strip().startswith(dt_file[:6])))\n",
    "                                   & (pd.notnull(df['roud']))]\n",
    "\n",
    "            for _, match_row in potential_matches.iterrows():\n",
    "                if pd.notnull(match_row['roud']):\n",
    "                    df.at[idx, 'roud'] = match_row['roud']\n",
    "                    updated_rows.append((row['dt_file'], match_row['roud']))\n",
    "                    break\n",
    "\n",
    "    return updated_rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    updated_rows = match_rows_and_update(df_lyrics2)\n",
    "\n",
    "    for dt_file, roud in updated_rows:\n",
    "        print(f\"dt_file {dt_file} was assigned roud {roud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final merged dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will drop exact duplicates and reindex the data. For now, lyrics without Roud numbers will stay in as they can also eventually be clustered. This will be the full dataset that I will use to evaluate the results of the clustering, retaining the current index numbers as a reference in order to join the data back up after clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics2.drop_duplicates(subset=['name','lyrics','roud'], inplace=True)\n",
    "df_lyrics2.reset_index(inplace=True)\n",
    "#df_lyrics2.to_csv('./Data/df_lyrics2.csv') #save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>key_name</th>\n",
       "      <th>name</th>\n",
       "      <th>version_in_key</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>A Robyn Jolly Robyn</td>\n",
       "      <td>A</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"[F]rom what appears to be the most ancient of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A Robin, Jolly Robin</td>\n",
       "      <td>(No Title)</td>\n",
       "      <td>B</td>\n",
       "      <td>Perc1185</td>\n",
       "      <td>HEYROBIN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71 'Hey, Robin, jolly Robin, 72    Tell me how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>A, U, Hinny Bird</td>\n",
       "      <td>A</td>\n",
       "      <td>StoR160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235</td>\n",
       "      <td>A, U, hinny burd; The bonny lass o' Benwell, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Adieu to Erin (The Emigrant)</td>\n",
       "      <td>Adieu to Erin</td>\n",
       "      <td>A</td>\n",
       "      <td>SWMS255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2068</td>\n",
       "      <td>Oh, when I breathed a last adieu, To Erin's an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Agincourt Carol, The</td>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>A</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>AGINCRT1</td>\n",
       "      <td>V29347</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9968</th>\n",
       "      <td>10108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zeb Tourney's Girl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LE18</td>\n",
       "      <td>ZEBTURNY</td>\n",
       "      <td>2249</td>\n",
       "      <td>Down in the Tennessee mountains,\\nFar from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>10109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zebra Dun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LB16</td>\n",
       "      <td>ZEBRADUN</td>\n",
       "      <td>3237</td>\n",
       "      <td>We was camped on the plains at the head of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>10110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zen Gospel Singing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZENGOSPE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I once was a Baptist and on each Sunday morn\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>10111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuleika</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULIKA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuleika was fair to see,\\nA fair Persian maide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>10112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Zulu King</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZULUKING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh the Zulu king with the big nose-ring\\nFell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9973 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                      key_name                   name  \\\n",
       "0         0          A Robin, Jolly Robin    A Robyn Jolly Robyn   \n",
       "1         1          A Robin, Jolly Robin             (No Title)   \n",
       "2         2              A, U, Hinny Bird       A, U, Hinny Bird   \n",
       "3         3  Adieu to Erin (The Emigrant)          Adieu to Erin   \n",
       "4         4          Agincourt Carol, The  The Song of Agincourt   \n",
       "...     ...                           ...                    ...   \n",
       "9968  10108                           NaN     Zeb Tourney's Girl   \n",
       "9969  10109                           NaN              Zebra Dun   \n",
       "9970  10110                           NaN     Zen Gospel Singing   \n",
       "9971  10111                           NaN                Zuleika   \n",
       "9972  10112                           NaN          The Zulu King   \n",
       "\n",
       "     version_in_key   bi_file   dt_file    roud  \\\n",
       "0                 A  Perc1185  HEYROBIN     NaN   \n",
       "1                 B  Perc1185  HEYROBIN     NaN   \n",
       "2                 A   StoR160       NaN     235   \n",
       "3                 A   SWMS255       NaN    2068   \n",
       "4                 A     MEL51  AGINCRT1  V29347   \n",
       "...             ...       ...       ...     ...   \n",
       "9968            NaN      LE18  ZEBTURNY    2249   \n",
       "9969            NaN      LB16  ZEBRADUN    3237   \n",
       "9970            NaN       NaN  ZENGOSPE     NaN   \n",
       "9971            NaN       NaN    ZULIKA     NaN   \n",
       "9972            NaN       NaN  ZULUKING     NaN   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     \"[F]rom what appears to be the most ancient of...  \n",
       "1     71 'Hey, Robin, jolly Robin, 72    Tell me how...  \n",
       "2     A, U, hinny burd; The bonny lass o' Benwell, A...  \n",
       "3     Oh, when I breathed a last adieu, To Erin's an...  \n",
       "4     Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "...                                                 ...  \n",
       "9968  Down in the Tennessee mountains,\\nFar from the...  \n",
       "9969  We was camped on the plains at the head of the...  \n",
       "9970  I once was a Baptist and on each Sunday morn\\n...  \n",
       "9971  Zuleika was fair to see,\\nA fair Persian maide...  \n",
       "9972  Oh the Zulu king with the big nose-ring\\nFell ...  \n",
       "\n",
       "[9973 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finished lyrics dataset has:\n",
    "- 9974 songs (with lyrics)\n",
    "- 4352 song lyrics with Roud numbers\n",
    "- 2893 unique roud numbers*\n",
    "- 1262 songs with roud numbers that have three or more songs attached*\n",
    "\n",
    "\\* not accounting for multiple Roud numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9973"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_lyrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4350"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_lyrics2.query('roud.notna()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics2.roud.dropna().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics2['roud_count'] = df_lyrics2.groupby('roud')['roud'].transform('count')\n",
    "df_roud_multiples = df_lyrics2[df_lyrics2['roud_count'] >= 3]\n",
    "len(df_roud_multiples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some 'roud' fields contain multiple numbers which still need to be split. I will decide later how to do this when labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only lyrics whose roud number appears 3 or more times in the data; remove columns for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Song of Agincourt</td>\n",
       "      <td>MEL51</td>\n",
       "      <td>AGINCRT1</td>\n",
       "      <td>V29347</td>\n",
       "      <td>Deo gracias anglia, Redde pro victoria, 1 Owre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O Falmouth Is a Fine Town</td>\n",
       "      <td>LK43A</td>\n",
       "      <td>AMBLTOWN</td>\n",
       "      <td>269</td>\n",
       "      <td>Text supplied by Don Duncan. Reportedly writte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Atisket, Atasket (I Sent a Letter to My Love)</td>\n",
       "      <td>BAF806A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13188</td>\n",
       "      <td>I wrote a letter to my love; I carried water i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Atisket, Atasket (I Sent a Letter to My Love)</td>\n",
       "      <td>BAF806A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13188</td>\n",
       "      <td>And the night before; if he does again to-nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Atisket, Atasket (I Sent a Letter to My Love)</td>\n",
       "      <td>BAF806A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13188</td>\n",
       "      <td>A green leather basket; I wrote a letter to my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>Young Barbour</td>\n",
       "      <td>C100</td>\n",
       "      <td>WILLIWI2</td>\n",
       "      <td>64</td>\n",
       "      <td>'Twas of a lady in the west counteree,\\nShe wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>Young Hunting</td>\n",
       "      <td>C068</td>\n",
       "      <td>YNGHUNT</td>\n",
       "      <td>47</td>\n",
       "      <td>It happened on one evening late,\\nAs the maid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>Young Hunting 2</td>\n",
       "      <td>C068</td>\n",
       "      <td>YNGHUNT2</td>\n",
       "      <td>47</td>\n",
       "      <td>A lady stood in her bower door,\\nIn her bower ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>Young Redin</td>\n",
       "      <td>C068</td>\n",
       "      <td>YNGHUNT5</td>\n",
       "      <td>47</td>\n",
       "      <td>Young Redin's til the hunting gane\\nWi' therty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9957</th>\n",
       "      <td>Young Sailor Cut Down in His Prime</td>\n",
       "      <td>LoF201</td>\n",
       "      <td>YNGMNPRM</td>\n",
       "      <td>2</td>\n",
       "      <td>One day as I strolled down by the Royal Albion...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1259 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  bi_file   dt_file  \\\n",
       "4                             The Song of Agincourt    MEL51  AGINCRT1   \n",
       "10                        O Falmouth Is a Fine Town    LK43A  AMBLTOWN   \n",
       "28    Atisket, Atasket (I Sent a Letter to My Love)  BAF806A       NaN   \n",
       "29    Atisket, Atasket (I Sent a Letter to My Love)  BAF806A       NaN   \n",
       "30    Atisket, Atasket (I Sent a Letter to My Love)  BAF806A       NaN   \n",
       "...                                             ...      ...       ...   \n",
       "9942                                  Young Barbour     C100  WILLIWI2   \n",
       "9948                                  Young Hunting     C068   YNGHUNT   \n",
       "9949                                Young Hunting 2     C068  YNGHUNT2   \n",
       "9955                                    Young Redin     C068  YNGHUNT5   \n",
       "9957             Young Sailor Cut Down in His Prime   LoF201  YNGMNPRM   \n",
       "\n",
       "        roud                                             lyrics  \n",
       "4     V29347  Deo gracias anglia, Redde pro victoria, 1 Owre...  \n",
       "10       269  Text supplied by Don Duncan. Reportedly writte...  \n",
       "28     13188  I wrote a letter to my love; I carried water i...  \n",
       "29     13188  And the night before; if he does again to-nigh...  \n",
       "30     13188  A green leather basket; I wrote a letter to my...  \n",
       "...      ...                                                ...  \n",
       "9942      64  'Twas of a lady in the west counteree,\\nShe wa...  \n",
       "9948      47  It happened on one evening late,\\nAs the maid ...  \n",
       "9949      47  A lady stood in her bower door,\\nIn her bower ...  \n",
       "9955      47  Young Redin's til the hunting gane\\nWi' therty...  \n",
       "9957       2  One day as I strolled down by the Royal Albion...  \n",
       "\n",
       "[1259 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only name, bi_file, dt_file, roud, and lyrics\n",
    "df_roud_multiples = df_roud_multiples[['name', 'bi_file', 'dt_file', 'roud', 'lyrics']]\n",
    "df_roud_multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bi_file</th>\n",
       "      <th>dt_file</th>\n",
       "      <th>roud</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, bi_file, dt_file, roud, lyrics]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roud_multiples[df_roud_multiples.duplicated(subset=['lyrics','roud'], keep=False)].sort_values('lyrics').head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean lyrics data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lyric in df_lyrics2['lyrics']:\n",
    "    lyric = cleanse_unicode_control(lyric).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a dataset of lyrics, these must be transformed into machine-readable data so that they can be clustered. There are various ways of doing this transformation, and the process is referred to as text vectorisation or embedding.\n",
    "\n",
    "Although often used interchangeably, simple vectorisation by index and vector space embedding have differences that are relevant to the task at hand. Vectorisation focuses on the discrete indexing and counting of tokens, whereas embeddings represent tokens in a continuous vector space which captures their interrelationships.\n",
    "* Vectorisation gives each token an index and a vector indicating its frequency. Frequency can be measured relative to different contexts (eg a simple count, or relative to its frequency in the corpus) by different models. Examples: n-gram, Bag-of-Words models, TF-IDF, or Count Vectorization\n",
    "* Embeddings, on the other hand, use context information to place words in a multidimensional vector space representing the entire input data corpus. Proximity in this space indicates semantic and contextual closeness. Examples: Word2Vec, GloVe, transformer-based models (eg BERT, GPT, T5) or RNN-based models (eg LSTM, GRU, Hierarchical Attention Networks)\n",
    "\n",
    "I will use true embeddings for my data, given that the structure of a song is important to its similarity, not only word frequency or semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different embeddings to choose from, with new models being released regularly. To select a model I considered the input data (lyrics) and the task (clustering) along with model performance benchmarks. The [Massive Text Embeddings Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard) at huggingface evaluates various embedding models on various datasets and tasks, including clustering. Overall, language models are not yet as effective at clustering as they are at other tasks like classification.\n",
    "\n",
    "The best embeddings tested for clustering are currently:\n",
    "* **gte** (large and base) from BAAI - a BERT-based model\n",
    "* **bge** (large and base) from Alibaba DAMO Academy - a BERT-based model\n",
    "* **text-embedding-ada-002** from OpenAI \n",
    "* **instructor** (large) from HKU NLP - a T5-based model\n",
    " \n",
    "The authors of [the original MTEB paper](https://arxiv.org/pdf/2210.07316.pdf) suggest that the front-runner at the time of publishing, MPNet, may have performed better due to its diversity of training data sets and resulting ability to create well-spaced embeddings in types of text it had not previously encountered. If true, I imagine this factor to be particularly relevant for lyrics because their structure does not match normal human-generated language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with `instructor-large` and `SentenceTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"hkunlp/instructor-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing the embeddings on only the subset of songs sharing Roud numbers, I give embeddings to the whole corpus of lyrics in `df_lyrics2`. These are fed one-at-a-time and so the corpus is not actually considered as a whole (as with TF-IDF), but the high performance of large language embedding models in 2023 means this may not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_lyrics2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initally used SentenceTransformer to create the embeddings but I wanted to get access to the custom embeddings for clustering, so I installed and used the InstructorEmbedding module. The model also attempts to adapt to the domain, and accepts instructions in plain text, hence I pass the instruction \"Represent the Lyrics for clustering:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another attempt using custom embeddings for clustering directly from InstructorEmbedding module\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "instruction = \"Represent the Lyrics for clustering:\"\n",
    "lyrics = df['lyrics'].tolist()\n",
    "\n",
    "lyrics_embed_for_cluster_task = []\n",
    "i = 0\n",
    "for lyric in lyrics:\n",
    "    embedding = embeddings = model.encode([[instruction,lyric]])\n",
    "    lyrics_embed_for_cluster_task.append(embedding)\n",
    "    i +=1\n",
    "\n",
    "import pickle\n",
    "#pickle:\n",
    "pickle.dump( lyrics_embed_for_cluster_task, open( \"embed_custom.p\", \"wb\" ) )\n",
    "\n",
    "#unpickle:\n",
    "#lyrics_embeddings = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = df['lyrics'].tolist()\n",
    "\n",
    "#Took hours, so commented out for re-runs: data pickled\n",
    "# lyrics_embeddings = []\n",
    "# i = 0\n",
    "# for lyric in lyrics:\n",
    "#     embedding = model.encode(lyric)\n",
    "#     lyrics_embeddings.append(embedding)\n",
    "#     # process taking a long time so print stored values to assure they are being saved and stored\n",
    "#     #print(i, lyrics_embeddings[i][0:3], '...')\n",
    "#     i +=1 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle:\n",
    "#pickle.dump( lyrics_embeddings, open( \"save.p\", \"wb\" ) )\n",
    "\n",
    "#unpickle:\n",
    "lyrics_embeddings = pickle.load( open( \"save.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (9974) does not match length of index (9973)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mlyric_embed_instructor\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m lyrics_embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4172\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4174\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4176\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4177\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4178\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4179\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4180\u001b[0m     ):\n\u001b[1;32m   4181\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4914\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4915\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4916\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.9/site-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (9974) does not match length of index (9973)"
     ]
    }
   ],
   "source": [
    "df['lyric_embed_instructor'] = lyrics_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics_w_embeddings = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each embedding has 768 dimensions, meaning that it cannot be graphically visualised without some transformation, eg feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lyrics_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to view my embeddings in a 2D or 3D space. There are specialised tools for this which combine the steps of feature reduction and visualisation. First I'll try out `atlas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nomic\n",
    "from nomic import atlas\n",
    "\n",
    "atlas_embeddings = np.asarray(lyrics_embeddings)\n",
    "\n",
    "project = atlas.map_embeddings(embeddings=atlas_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try other viz tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: TF-IDF vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import texthero as hero\n",
    "# df = df_cluster_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lyrics'] = hero.clean(df['lyrics'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tfidf'] = (hero.tfidf(df['lyrics'], max_features=300))\n",
    "# df[['tfidf', 'roud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pca'] = hero.pca(df['tfidf'])\n",
    "# fig = hero.scatterplot(\n",
    "#     df, \n",
    "#     col='pca', \n",
    "#     color='roud', \n",
    "#     title=\"Roud PCA\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: below I had to edit the source code to comment out an out of date input for sklearn kmeans (in representation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# #import plotly\n",
    "# # fig.update_layout(height=800)\n",
    "# # fig.canvas.layout.width = '7in'\n",
    "# # fig.update_yaxes(\n",
    "# #     scaleanchor=\"x\",\n",
    "# #     scaleratio=1,\n",
    "# #   )\n",
    "\n",
    "# hero.scatterplot(\n",
    "#     df.dropna(subset='roud'), \n",
    "#     col='pca', \n",
    "#     color='kmeans_labels_dt', \n",
    "#     title=\"K-means (PCA)\",\n",
    "\n",
    "#     hover_data=['roud']\n",
    "# )\n",
    "# hero.scatterplot(use_container_width=True,height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2=df[['roud', 'lyrics_st']].dropna('any').concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to cluster my lyrics into non-discrete clusters based on the high-dimensional emebeddings data. This led me to consider the following approaches and potential models:\n",
    "\n",
    "**Soft Clustering:** assigns data points to multiple clusters, with varying degrees of membership\n",
    "- **Centroid-based: Fuzzy C-Means**: like K-Means, but with flexible cluster membership\n",
    "     - **Disadvantages:**\n",
    "        - Requires specifying the number of clusters\n",
    "        - Assumes clusters are uniform blobs\n",
    "        - Sensitive to initial cluster placement\n",
    "- **Gaussian Mixture Models (GMM)**: assigns probabilities to data points' cluster memberships using Expectation-Maximisation\n",
    "    - **Disadvantages:**\n",
    "        - Assumes data points in a cluster will have a Gaussian distribution, and my clusters might have too few members to measure this. It is also unclear to me if this would be true for lyrics data\n",
    "        - Sensitive to initial cluster placement ([is initialised similarly to, and often using, K-Means](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html)) \n",
    "\n",
    "**Hierarchical Clustering:** puts data points into a hierarchical tree of clusters\n",
    "- **Hierarchical Agglomerative Clustering (HAC)** starts from initial points and agglomorates other points to them (compare: divisive clustering)\n",
    "    - **Advantages:**\n",
    "        - Could provide insights into how lyrics versions are connected and descended/derived from others\n",
    "    - **Disadvantages:**\n",
    "        - Computationally intensive\n",
    "        - Produces discrete hierarchies, whereas lyrics may be more like a web\n",
    "\n",
    "**Density-based Clustering:** creates clusters by detecting areas of increased density in the feature space\n",
    "- **[HDBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN):** based on DBSCAN, but creates a hierarchical tree of possible clusters arranged by density, starting out from having the only the most densely arranged points clustered, and splitting the tree where points have ambiguous membership. Selecting a cutoff level in the tree allows for a mix of cluster densities in the final configuration.\n",
    "    - **Advantages:**\n",
    "        - Soft cluster assignment is possible https://hdbscan.readthedocs.io/en/latest/soft_clustering.html \n",
    "        - Automatically determines the number of clusters\n",
    "        - Handles outliers and unevenly distributed data\n",
    "        - Can return a medoid which could, if it is a real data point(?), serve as an exemplary point for that cluster\n",
    "\n",
    "**Affinity Propagation Clustering:** clusters data points by passing messages between pairs of points about group membership preferences\n",
    "- **[AffinityPropagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)**\n",
    "    - **Advantages:**\n",
    "        - Captures non-linear relationships between data, potentially good for finding less-obvious connections between texts\n",
    "        - Automatically determines the number of clusters\n",
    "        - Returns an examplar of each cluster\n",
    "    - **Disadvantages:**\n",
    "        - Requires careful parameter tuning for optimal results.\n",
    "        - Hard clustering only (a fuzzy version [SCAP] exists but not as a Python library)\n",
    "\n",
    "These are summarised in the table below. HDBSCAN was the model that fulfilled the most requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Model**            | **method**  | **predicts number<br>of clusters** | **soft/fuzzy cluster<br>membership** | **multi- dimensional<br>data** | **nonspherical or<br>uneven density clusters** |\n",
    "| -------------------- | :--------------------: | :--------------------: | :--------------------: | :--------------------: | :--------------------: |\n",
    "| Fuzzy<br>C-Means        |Centroid     |                ❌                |                 ✔️                |              ❌              |                      ❌                      |\n",
    "| GMM                  | Probability;<br>Expectation Maximisation |                ❌                |                 ✔️                |              ❌              |                      ✔️                     |\n",
    "| HDBSCAN              |Density  |                ✔️               |          ✔️<br>experimental          |              ✔️             |                      ✔️                     |\n",
    "| Affinity<br>propagation |\"message passing\"|                ✔️               |                 ❌                 |             ✔️?             |                      ✔️                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with the `HDBSCAN` model from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HDBSCAN` is a new addition to `sklearn` in version 1.3, released June 2023, so I have to update before I can import the modules needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "import ipympl\n",
    "#%config InlineBackend.figure_format='retina'\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will run the clustering initially on the songs with Roud numbers having at least 3 songs attached, and also set the `min_cluster_size` size to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_roud_w_embed = df_lyrics_w_embeddings[df_lyrics_w_embeddings.index.isin(df_roud_multiples.index)]\n",
    "df_multi_roud_w_embed[['lyrics', 'lyric_embed_instructor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = HDBSCAN(min_cluster_size=3, store_centers='medoid')\n",
    "X = df_multi_roud_w_embed['lyric_embed_instructor'].tolist()\n",
    "hdb.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each data point has been assigned a cluster label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels = pd.DataFrame(hdb.labels_)\n",
    "df_cluster_labels['probabilities'] = hdb.probabilities_\n",
    "df_cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, over half of the data points were simply assigned -1, meaning the data was considered 'noisy' by HDBSCAN. (None were assigned -2 or -3 for invalid data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels[df_cluster_labels[0] ==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities of cluster membership among items assigned to a cluster are mainly 1, but some are lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels['probabilities'][df_cluster_labels['probabilities'].between(0,1, \n",
    "                                                inclusive=False)].agg(['count','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I try to plot the result, there are too many dimensions to see anything useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, hdb.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction for cluster visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although no dimension reduction was done for calculating the clusters, we can still use it to visualise the data points in 2 dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE is a dimensionality reduction technique that is better at preserving clusters, whereas PCA is better at preserving distances and therefor the size of differences. Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components = 2).fit_transform(np.asarray(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_pca = PCA(n_components = 2).fit_transform(np.asarray(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting function from https://scikit-learn.org/stable/auto_examples/cluster/plot_hdbscan.html\n",
    "def clusterplot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = labels if labels is not None else np.ones(X.shape[0])\n",
    "    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    # The probability of a point belonging to its labeled cluster determines\n",
    "    # the size of its marker\n",
    "    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_index = np.where(labels == k)[0]\n",
    "        for ci in class_index:\n",
    "            ax.plot(\n",
    "                X[ci, 0],\n",
    "                X[ci, 1],\n",
    "                \"x\" if k == -1 else \"o\",\n",
    "                markerfacecolor=tuple(col),\n",
    "                markeredgecolor=\"k\",\n",
    "                markersize=4 if k == -1 else 1 + 5 * proba_map[ci],\n",
    "            )\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    preamble = \"True\" if ground_truth else \"Estimated\"\n",
    "    title = f\"{preamble} number of clusters: {n_clusters_}\"\n",
    "    if parameters is not None:\n",
    "        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n",
    "        title += f\" | {parameters_str}\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterplot(X_tsne, hdb.labels_, hdb.probabilities_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterplot(X_pca, hdb.labels_, hdb.probabilities_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE seems to produce the best results, but I need to inspect the data to be sure that the clusters reflect the lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare clusters to Roud numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the counts of unique song names and Roud numbers per cluster label. Some clusters have multiple Roud numbers in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_roud_w_embed_label = df_multi_roud_w_embed.copy()\n",
    "df_multi_roud_w_embed_label = df_multi_roud_w_embed_label.reset_index().join(df_cluster_labels)\n",
    "df_multi_roud_w_embed_label.sort_values(by=[0])[['name', 'roud', 'probabilities', 0]].groupby([0]).nunique().sort_values(by='roud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's graph only the songs that got a valid label (not -1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_v_roud = df_multi_roud_w_embed_label.sort_values(by=[0])[['name', 'roud', 'probabilities', 0]][df_multi_roud_w_embed_label[0] >=0]\n",
    "clusters_v_roud = clusters_v_roud.rename(columns={0: 'cluster_label'}).sort_values(by='roud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add roud label fixed and song label hover\n",
    "%matplotlib ipympl\n",
    "fig, ax = plt.subplots()\n",
    "sns.despine(fig)\n",
    "\n",
    "hist_plot = sns.histplot(\n",
    "    clusters_v_roud,\n",
    "    x='cluster_label', binwidth=1,\n",
    "    hue='roud',\n",
    "    discrete=True,\n",
    "    multiple=\"stack\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=.5\n",
    ")\n",
    "ax.set_xticks(clusters_v_roud.cluster_label)\n",
    "ax.set_yticks(np.arange(1, 13, step=1))\n",
    "ax.margins(x=0)\n",
    "plt.xticks(fontsize = 6, rotation=90)\n",
    "\n",
    "# TODO: Add labels to the 'hue' patches insead of bars \n",
    "for bar in ax.containers[0]:  \n",
    "    height = bar.get_height()\n",
    "    width = bar.get_width()\n",
    "    x_pos = bar.get_x() + width / 2\n",
    "    y_pos = height\n",
    "    label = clusters_v_roud.loc[clusters_v_roud['cluster_label'] == x_pos, 'roud'].values[0]\n",
    "    \n",
    "    ax.text(x_pos, y_pos, label, ha='center', va='bottom', fontsize=6, color='white', rotation='vertical')\n",
    "\n",
    "ax.get_legend().set_visible(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most cluster labels, however, were assigned to exactly three songs (our minimum set for both cluster size and minimum Roud sample size) all of the same Roud number. That suggests at least a partially successful clustering. \n",
    "\n",
    "Let's look at clusters containing songs with differing Roud numbers and see if it's clear why this happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roud_per_cluster = clusters_v_roud.groupby('cluster_label').nunique()\n",
    "multi_roud_per_cluster = roud_per_cluster[roud_per_cluster.roud >1]\n",
    "clusters_v_roud_multi = clusters_v_roud[clusters_v_roud['cluster_label'].isin(multi_roud_per_cluster.reset_index()['cluster_label'])]\n",
    "clusters_v_roud_multi #.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8)) \n",
    "cross_tab = pd.crosstab(clusters_v_roud_multi['cluster_label'], clusters_v_roud_multi['roud'])\n",
    "sns.heatmap(cross_tab, fmt=\"d\", cmap='PuOr', center=0, cbar=False, linewidths=0.5, linecolor='white')\n",
    "\n",
    "# annotate only non-zero values\n",
    "for i in range(cross_tab.shape[0]):\n",
    "    for j in range(cross_tab.shape[1]):\n",
    "        value = cross_tab.iloc[i, j]\n",
    "        if value != 0:\n",
    "            plt.text(j + 0.5, i + 0.5, str(value), va='center', ha='center', color='white', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tick_params(which='both', bottom=False, top=False)\n",
    "\n",
    "plt.title('Cluster vs. Roud Heatmap')\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 111 is particularly diffuse. Upon an initial inspection of these songs, they are all immediately recognisable as Scottish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_111 = df_multi_roud_w_embed_label[df_multi_roud_w_embed_label[0] == 111]\n",
    "df_111[['name', 'roud', 'lyrics', 0, 'probabilities']].sort_values(by='roud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the probabilities align with the Rouds (Note: *Jock O'The Side* [Roud #82] is mislabeled in one record, creating a duplicate). All the lyrics with 100% probability or close are variations on *Clyde's Water*, Roud #111. A smaller leaf size might give better results, and visualising the clustering tree could give more insights, but the `sklearn` version of HDBSCAN doesn't support this feature, so I will continue with the original version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of HDBSCAN currently officially implemented by `sklearn` also does not have the `prediction_data=True` parameter which allows us to do soft clustering. Instead I'll load the standalone 'contributions' version from which it is derived (from [Github](https://github.com/scikit-learn-contrib/hdbscan.git) - there is a [bug](https://github.com/scikit-learn-contrib/hdbscan/releases/tag/0.8.33) in the current main one). This time I will fit the model with the extra parameters available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "hdb2 = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size=3, min_samples=2, gen_min_span_tree=True)\n",
    "data = np.array(X)\n",
    "hdb2.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `min_samples=2` I could achieve a similar number of clusters to the `sklearn` version, at 129. Using any higher value resulted three or fewer clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb2.labels_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb2.probabilities_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "118 samples have probabilities of less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels_2 = pd.DataFrame()\n",
    "df_cluster_labels_2['cluster_labels_2'] = hdb2.labels_\n",
    "df_cluster_labels_2['probabilities_2'] = hdb2.probabilities_\n",
    "df_cluster_labels_2[df_cluster_labels_2.probabilities_2.between(0,1, inclusive=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two representations of the clusters. The first one, taken from standalone HDBSCAN, calculates the T-SNE on just the sample dataset (n roud >= 3) whereas the second one, taken from sklearn as above, is mapped onto 2D space using a T-SNE based on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterplot(X_tsne, hdb2.labels_, hdb2.probabilities_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the tree of splits from the new clusterer. It has a potentially unusual left-branching shape, which may indicate that there could be a more efficient way to split the data, but it's not yet clear how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.figure(figsize=(12,4))\n",
    "hdb2.condensed_tree_.plot(cmap=\"viridis\", log_size=True)\n",
    "plt.yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metrics affect the 'closeness' of points, and the choice of metric might therefore depend on what kind of closeness should be measured in a given task. Let's see if different distance metrics make a difference to how splits are discovered by the clustering model (`min_cluster_size=3, min_samples=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(X)\n",
    "distance_metrics = ('braycurtis',\n",
    " 'canberra',\n",
    " 'chebyshev',\n",
    " 'cityblock',\n",
    " 'euclidean',\n",
    " 'infinity',\n",
    " 'l1',\n",
    " 'l2',\n",
    " 'manhattan',\n",
    " 'p'\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 22))\n",
    "fig.suptitle('HDBSCAN with various distance metrics', fontsize=14)\n",
    "\n",
    "for ax, metric in zip(axes.ravel(), distance_metrics):\n",
    "    # clustering\n",
    "    clusterer = hdbscan.HDBSCAN(metric=metric, min_cluster_size=2, min_samples=2) #, min_samples=2\n",
    "    clusterer.fit(data)\n",
    "    # plot\n",
    "    clusterer.condensed_tree_.plot(cmap=\"viridis\", colorbar=False, log_size=True, axis=ax) #log_size=True, \n",
    "    num_clusters = len(np.unique(clusterer.labels_))\n",
    "    ax.set_title(f\"Distance Metric: {metric} ({num_clusters} clusters)\")\n",
    "    #ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store membership probabilities for multiple clusters per point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [docs](https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan.prediction.membership_vector), the probability that point `i` is a member of cluster `j` is in `membership_vectors[i, j]. I extracted the first, second and third choices of clusters for each data point and combined them with the hard clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_vectors = hdbscan.all_points_membership_vectors(hdb2)\n",
    "\n",
    "mem = [] #temp results storage\n",
    "for i, j in enumerate(membership_vectors):\n",
    "    # Get the original cluster label and probability\n",
    "    soft_cluster0 = j.argmax()\n",
    "    soft_probability = j[soft_cluster0]\n",
    "    \n",
    "    # Sort the probabilities in descending order and get the indices\n",
    "    sorted_indices = j.argsort()[::-1]\n",
    "    \n",
    "    # Get the top two next highest probabilities\n",
    "    soft_cluster1 = sorted_indices[1]\n",
    "    soft_probability1 = j[soft_cluster1]\n",
    "    \n",
    "    soft_cluster2 = sorted_indices[2]\n",
    "    soft_probability2 = j[soft_cluster2]\n",
    "    \n",
    "    # Append the data for the current datapoint\n",
    "    mem.append([i, soft_cluster0, soft_probability, soft_cluster1, soft_probability1, soft_cluster2, soft_probability2])\n",
    "\n",
    "# Create a dataframe from the collected data\n",
    "columns = ['datapoint', 'soft_cluster_0', 'soft_probability_0', 'soft_cluster_1', 'soft_probability_1', 'soft_cluster_2', 'soft_probability_2']\n",
    "df_soft_cluster = pd.DataFrame(mem, columns=columns)\n",
    "df_soft_merged = df_cluster_labels_2.join(df_soft_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_cluster_data = df_multi_roud_w_embed_label[['name', 'roud', 'roud_count', 'lyrics']].join(df_soft_merged).drop(['datapoint'], axis=1)\n",
    "df_soft_cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to cluster 111. I now also include points that would have been considered 111 if they had been clustered, but were considered too noisy. The cluster membership probabilities appear to work differently in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft_cluster_data[df_soft_cluster_data.soft_cluster_0==111].sort_values(by='probabilities_2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from matplotlib import style\n",
    "\n",
    "# #%matplotlib inline #added because ipympl was breaking seaborn\n",
    "\n",
    "# from sklearn import preprocessing, metrics\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_samples, silhouette_score, pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
